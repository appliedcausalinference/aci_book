{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a python function to load a CSV file with the following columns into a Pandas dataframe:\n",
    "\t\n",
    "- instant: record index\n",
    "- dteday : date\n",
    "- season : season (1:winter, 2:spring, 3:summer, 4:fall)\n",
    "- yr : year (0: 2011, 1:2012)\n",
    "- mnth : month ( 1 to 12)\n",
    "- hr : hour (0 to 23)\n",
    "- holiday : weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\n",
    "- weekday : day of the week\n",
    "- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "- weathersit : \n",
    "    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "- temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\n",
    "- atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\n",
    "- hum: Normalized humidity. The values are divided to 100 (max)\n",
    "- windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
    "- casual: count of casual users\n",
    "- registered: count of registered users\n",
    "- cnt: count of total rental bikes including both casual and registered\n",
    "\n",
    "Convert the following column names as follows:\n",
    "\n",
    "- dteday -> date\n",
    "- yr -> year\n",
    "- mnth -> month\n",
    "- hr -> hour\n",
    "- holiday -> is_holiday\n",
    "- weekday -> day_of_week\n",
    "- workingday -> is_workingday\n",
    "- weathersit -> weather\n",
    "- hum -> humidity\n",
    "- causal -> num_casual_users\n",
    "- registered -> num_registered_users\n",
    "- cnt -> num_total_users\n",
    "\n",
    "Convert the values in `season` column into categorical values using this map: (1:winter, 2:spring, 3:summer, 4:fall)\n",
    "Convert values in year using this map: (0: 2011, 1:2012)\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "def load_hour_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df.rename(columns={'dteday': 'date', 'yr': 'year', 'mnth': 'month', 'hr': 'hour', 'holiday': 'is_holiday', 'weekday': 'day_of_week', 'workingday': 'is_workingday', 'weathersit': 'weather', 'hum': 'humidity', 'casual': 'num_casual_users', 'registered': 'num_registered_users', 'cnt': 'num_total_users'}, inplace=True)\n",
    "    df['season'] = df['season'].map({1: 'winter', 2: 'spring', 3: 'summer', 4: 'fall'}).astype('category')\n",
    "    df['year'] = df['year'].map({0: 2011, 1: 2012})\n",
    "\n",
    "    df['timestamp'] = pd.to_datetime(df['date'] + ' ' + df['hour'].astype(str) + ':00:00')\n",
    "    df.drop(['date', 'year'], axis=1, inplace=True)    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_day_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df.rename(columns={'dteday': 'date', 'yr': 'year', 'mnth': 'month', 'holiday': 'is_holiday', 'weekday': 'day_of_week', 'workingday': 'is_workingday', 'weathersit': 'weather', 'hum': 'humidity', 'casual': 'num_casual_users', 'registered': 'num_registered_users', 'cnt': 'num_total_users'}, inplace=True)\n",
    "    df['season'] = df['season'].map({1: 'winter', 2: 'spring', 3: 'summer', 4: 'fall'}).astype('category')\n",
    "    df['year'] = df['year'].map({0: 2011, 1: 2012})\n",
    "\n",
    "    df['timestamp'] = pd.to_datetime(df['date'])\n",
    "    df.drop(['date', 'year'], axis=1, inplace=True)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_file = '/Users/klogram/Downloads/Bike-Sharing-Dataset/hour.csv'\n",
    "day_file = '/Users/klogram/Downloads/Bike-Sharing-Dataset/day.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these datasets, the instant (or the index) acts as a time step variable that we can use to compute time differences. In the hour dataset, the time step is one hour. In the day dataset, the time step in one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour = load_hour_data(hour_file)\n",
    "print(df_hour.shape)\n",
    "df_hour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day = load_day_data(day_file)\n",
    "print(df_day.shape)\n",
    "df_day.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hour-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# histogram of total users, with axis labels\n",
    "df_hour['num_total_users'].hist()\n",
    "plt.xlabel('Total Users')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of casual users, with axis labels\n",
    "df_hour['num_casual_users'].hist()\n",
    "plt.xlabel('Casual Users')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of registered users, with axis labels\n",
    "df_hour['num_registered_users'].hist()\n",
    "plt.xlabel('Registered Users')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-dependent Causal Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the description of how the day-level bike sharing dataset was analyzed in Zheng and Kleinberg, 2017. Like them, we divide the continuous variables into three bins of equal size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a function that takes a dataframe and column name as input and \n",
    "adds a new column to the dataframe that labels each row as 'low', 'medium', or 'high',\n",
    "based on whether the value in the original column is above or below one \n",
    "standard deviation from the mean.\n",
    "\"\"\"\n",
    "def label_by_std(df, col_name):\n",
    "    std = df[col_name].std()\n",
    "    mean = df[col_name].mean()\n",
    "    df[f'{col_name}_bin'] = df[col_name].apply(lambda x: 'low' if x < mean - std else ('high' if x > mean + 2*std else 'medium'))\n",
    "    return df\n",
    "\n",
    "df_hour = label_by_std(df_hour, 'num_total_users')\n",
    "df_hour = label_by_std(df_hour, 'num_casual_users')\n",
    "df_hour = label_by_std(df_hour, 'num_registered_users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour[\"num_total_users_bin\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "write a function that takes a dataframe column and converts the values into \n",
    "three bins of equal size, based on the values in the column.\n",
    "\"\"\"\n",
    "def convert_to_bins(df, column):\n",
    "    return pd.qcut(df[column], 3, labels=['low', 'medium', 'high'])\n",
    "\n",
    "# convert the temp column into three bins\n",
    "df_hour['temp_bin'] = convert_to_bins(df_hour, 'temp')\n",
    "\n",
    "# convert the atemp column into three bins\n",
    "df_hour['atemp_bin'] = convert_to_bins(df_hour, 'atemp')\n",
    "\n",
    "# convert the humidity column into three bins\n",
    "df_hour['humidity_bin'] = convert_to_bins(df_hour, 'humidity')\n",
    "\n",
    "# convert the windspeed column into three bins\n",
    "df_hour['windspeed_bin'] = convert_to_bins(df_hour, 'windspeed')\n",
    "\n",
    "# convert the num_total_users column into three bins\n",
    "# df_hour['num_total_users_bin'] = convert_to_bins(df_hour, 'num_total_users')\n",
    "\n",
    "# convert the num_casual_users column into three bins\n",
    "# df_hour['num_casual_users_bin'] = convert_to_bins(df_hour, 'num_casual_users')\n",
    "\n",
    "# convert the num_registered_users column into three bins\n",
    "# df_hour['num_registered_users_bin'] = convert_to_bins(df_hour, 'num_registered_users')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Variables\n",
    "\n",
    "The variables represent the things that are potential causes. Here are the variables:\n",
    "\n",
    "- is_holiday\n",
    "- is_workingday\n",
    "- is_weekend\n",
    "- is_raining\n",
    "- is_bad_weather\n",
    "- is_mild_precipitation\n",
    "- is_daytime\n",
    "- is_nighttime\n",
    "- is_`<season>`\n",
    "- is_high_temp\n",
    "- is_high_atemp\n",
    "- is_low_temp\n",
    "- is_low_atemp\n",
    "- is_high_humidity\n",
    "- is_low_humidity\n",
    "- is_windy\n",
    "- not_windy\n",
    "\n",
    "### Functions for determing the state \n",
    "\n",
    "For each variable, we define a boolean function that tells us what is true or false at any point in time in the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_holiday(df, instant):\n",
    "    return 1 if df.loc[instant, 'is_holiday'] == 1 else 0\n",
    "\n",
    "def is_workingday(df, instant):\n",
    "    return 1 if df.loc[instant, 'is_workingday'] == 1 else 0\n",
    "\n",
    "def is_weekend(df, instant):\n",
    "    return 1 if df.loc[instant, 'day_of_week'] in [0, 6] else 0\n",
    "    \n",
    "def is_bad_weather(df, instant):\n",
    "    return 1 if df.loc[instant, 'weather'] in [3, 4] else 0\n",
    "\n",
    "def is_mild_precipitation(df, instant):\n",
    "    return 1 if df.loc[instant, 'weather'] == 3 else 0\n",
    "\n",
    "def is_rush_hour(df, instant):\n",
    "    return 1 if df.loc[instant, 'hour'] in [7, 8, 9, 17, 18, 19] else 0\n",
    "\n",
    "def is_daytime(df, instant):\n",
    "    return 1 if df.loc[instant, 'hour'] in [6, 7, 8, 9, 10, 11, 12, 13, 14, 15] else 0\n",
    "\n",
    "def is_nighttime(df, instant):\n",
    "    return 1 if df.loc[instant, 'hour'] in [0, 1, 2, 3, 4, 5, 20, 21, 22, 23] else 0\n",
    "\n",
    "def is_spring(df, instant):\n",
    "    return 1 if df.loc[instant, 'season'] == 'spring' else 0\n",
    "\n",
    "def is_summer(df, instant):\n",
    "    return 1 if df.loc[instant, 'season'] == 'summer' else 0\n",
    "\n",
    "def is_fall(df, instant):\n",
    "    return 1 if df.loc[instant, 'season'] == 'fall' else 0\n",
    "\n",
    "def is_winter(df, instant):\n",
    "    return 1 if df.loc[instant, 'season'] == 'winter' else 0\n",
    "\n",
    "def is_high_temp(df, instant):\n",
    "    return 1 if df.loc[instant, 'temp_bin'] == 'high' else 0\n",
    "\n",
    "def is_low_temp(df, instant):\n",
    "    return 1 if df.loc[instant, 'temp_bin'] == 'low' else 0\n",
    "\n",
    "def is_high_atemp(df, instant):\n",
    "    return 1 if df.loc[instant, 'atemp_bin'] == 'high' else 0\n",
    "\n",
    "def is_low_atemp(df, instant):\n",
    "    return 1 if df.loc[instant, 'atemp_bin'] == 'low' else 0\n",
    "\n",
    "def is_high_humidity(df, instant):\n",
    "    return 1 if df.loc[instant, 'humidity_bin'] == 'high' else 0\n",
    "\n",
    "def is_low_humidity(df, instant):\n",
    "    return 1 if df.loc[instant, 'humidity_bin'] == 'low' else 0\n",
    "\n",
    "def is_windy(df, instant):\n",
    "    return 1 if df.loc[instant, 'windspeed_bin'] == 'high' else 0\n",
    "\n",
    "def not_windy(df, instant):\n",
    "    return 1 if df.loc[instant, 'windspeed_bin'] == 'low' else 0\n",
    "\n",
    "labelers = {\n",
    "    'is_holiday': is_holiday,\n",
    "    'is_workingday': is_workingday,\n",
    "    'is_weekend': is_weekend,\n",
    "    'is_bad_weather': is_bad_weather,\n",
    "    'is_mild_precipitation': is_mild_precipitation,\n",
    "    'is_rush_hour': is_rush_hour,\n",
    "    'is_daytime': is_daytime,\n",
    "    'is_nighttime': is_nighttime,\n",
    "    'is_spring': is_spring,\n",
    "    'is_summer': is_summer,\n",
    "    'is_fall': is_fall,\n",
    "    'is_winter': is_winter,\n",
    "    'is_high_temp': is_high_temp,\n",
    "    'is_low_temp': is_low_temp,\n",
    "    'is_high_atemp': is_high_atemp,\n",
    "    'is_low_atemp': is_low_atemp,\n",
    "    'is_high_humidity': is_high_humidity,\n",
    "    'is_low_humidity': is_low_humidity,\n",
    "    'is_windy': is_windy,\n",
    "    'not_windy': not_windy\n",
    "}\n",
    "\n",
    "idx2labeler = {i: labeler for i, labeler in enumerate(labelers.keys())}\n",
    "\n",
    "label_fns = [labelers[idx2labeler[i]] for i in range(len(labelers))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want some boolean functions for the effect values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_total_users(df, instant):\n",
    "    return 1 if df.loc[instant, 'num_total_users_bin'] == 'high' else 0\n",
    "\n",
    "def low_total_users(df, instant):\n",
    "    return 1 if df.loc[instant, 'num_total_users_bin'] == 'low' else 0\n",
    "\n",
    "def high_casual_users(df, instant):\n",
    "    return 1 if df.loc[instant, 'num_casual_users_bin'] == 'high' else 0\n",
    "\n",
    "def low_casual_users(df, instant):\n",
    "    return 1 if df.loc[instant, 'num_casual_users_bin'] == 'low' else 0\n",
    "\n",
    "def high_registered_users(df, instant):\n",
    "    return 1 if df.loc[instant, 'num_registered_users_bin'] == 'high' else 0\n",
    "\n",
    "def low_registered_users(df, instant):\n",
    "    return 1 if df.loc[instant, 'num_registered_users_bin'] == 'low' else 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using type-level relationships to explain observed events\n",
    "\n",
    "Here, we identify type-level relationships that explain why bike rentals are high or low. \n",
    "\n",
    "Here are the steps in the process:\n",
    "\n",
    "1. Identify the token-level relationships in the dataset that are between the variables listed above\n",
    "2. compute the significance of the type-level relationships, $\\epsilon_{avg}(c_{r-s},e)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to evaluate the significance of each type-level cause for each instance of high and low bike rentals. For each instance of high and low bike rentals, we then use a significance threshold or only keep the top $k$ relationships to partition the relationships/events into those for which sufficiently significant type-level causes have been found and those for which sufficiently significant type-level causes have *not* been found. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify token-level relationships\n",
    "\n",
    "We will pass through the dataset, and at each point, we will use the values of the various variables to select *prima facie* causes that can be given a significance score in the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "V = len(labelers)  # number of variables\n",
    "T = df_hour.shape[0]  # number of time steps\n",
    "D = np.zeros((T, V), dtype=np.int64)  # value of each variable at each time step\n",
    "\n",
    "print(\"Number of potential type-level causes: %d\" % V)\n",
    "print(\"Number of time steps: %d\" % T)\n",
    "\n",
    "for t in range(T):\n",
    "    for i in range(V):\n",
    "        D[t, i] = label_fns[i](df_hour, t)\n",
    "\n",
    "variable_names = idx2labeler.values()\n",
    "df_D = pd.DataFrame(D, columns=variable_names)\n",
    "\n",
    "# add effect columns to the dataframe\n",
    "# this makes it easy to filter to compute proabilities\n",
    "df_D[\"high_total_users\"] = [high_total_users(df_hour, t) for t in range(T)]\n",
    "df_D[\"low_total_users\"] = [low_total_users(df_hour, t) for t in range(T)]\n",
    "df_D[\"high_casual_users\"] = [high_casual_users(df_hour, t) for t in range(T)]\n",
    "df_D[\"low_casual_users\"] = [low_casual_users(df_hour, t) for t in range(T)]\n",
    "df_D[\"high_registered_users\"] = [high_registered_users(df_hour, t) for t in range(T)]\n",
    "df_D[\"low_registered_users\"] = [low_registered_users(df_hour, t) for t in range(T)]\n",
    "\n",
    "# df_hour = pd.concat([df_hour, df_D], axis=1)\n",
    "\n",
    "# remove duplicate columns\n",
    "# df_hour = df_hour.loc[:,~df_hour.columns.duplicated()]\n",
    "\n",
    "# df_hour.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures for Causal Relationships\n",
    "\n",
    "Here we introduce several data structures that we will use to work with type-level and token-level relationships.\n",
    "First, we have two class that let us store the names of causal variables, so that we don't have to keep passing around strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "VariableIndex = int\n",
    "\n",
    "\n",
    "class BidirectionalDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._backward = {v: k for k, v in self.items()}\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        super().__setitem__(key, value)\n",
    "        self._backward[value] = key\n",
    "\n",
    "    def forward_lookup(self, key):\n",
    "        return self[key]\n",
    "\n",
    "    def backward_lookup(self, value):\n",
    "        return self._backward[value]\n",
    "\n",
    "    def keys(self):\n",
    "        return list(super().keys())\n",
    "\n",
    "    def values(self):\n",
    "        return list(self._backward.keys())\n",
    "\n",
    "\n",
    "class VariableStore:\n",
    "    def __init__(self):\n",
    "        self.storage = BidirectionalDict()\n",
    "\n",
    "    def add(self, variable_name: str):\n",
    "        if variable_name not in self.storage:\n",
    "            self.storage[variable_name] = len(self.storage)\n",
    "\n",
    "    def lookup_by_name(self, name: str) -> VariableIndex:\n",
    "        return self.storage.forward_lookup(name)\n",
    "\n",
    "    def lookup_by_index(self, index: VariableIndex) -> str:\n",
    "        return self.storage.backward_lookup(index)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.storage)\n",
    "\n",
    "    def __contains__(self, name) -> bool:\n",
    "        return name in self.storage\n",
    "\n",
    "    @property\n",
    "    def names(self) -> List[str]:\n",
    "        return sorted(self.storage.keys())\n",
    "\n",
    "    @property\n",
    "    def ids(self) -> List[VariableIndex]:\n",
    "        return sorted(self.storage.values())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have data structures for time window, type-level and token-level causal relationships, and significance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Set, Union\n",
    "\n",
    "class Window:\n",
    "    \"\"\"\n",
    "    A window of time, of the closed interval [start, end]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, start: int, end: int):\n",
    "        if start > end:\n",
    "            raise ValueError(\"Window start must be <= than end\")\n",
    "        if start < 0 or end < 0:\n",
    "            raise ValueError(\"Window start and end must be >= 0\")\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Window({self.start}, {self.end})\"\n",
    "\n",
    "    def __eq__(self, __value: object) -> bool:\n",
    "        if isinstance(__value, Window):\n",
    "            return self.start == __value.start and self.end == __value.end\n",
    "        return False\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return hash((self.start, self.end))\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CausalRelation:\n",
    "    \"\"\"\n",
    "    A cause and effect pair\n",
    "    \"\"\"\n",
    "\n",
    "    cause: VariableIndex\n",
    "    effect: VariableIndex\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TokenCause:\n",
    "    \"\"\"\n",
    "    A token event that supports a potential cause\n",
    "    \"\"\"\n",
    "\n",
    "    relation: CausalRelation\n",
    "    t_cause: int  # time step where cause is true\n",
    "    t_effect: int  # time step where effect is true\n",
    "\n",
    "    @property\n",
    "    def lag(self) -> int:\n",
    "        \"\"\"The lag between the cause and effect\"\"\"\n",
    "        return self.t_effect - self.t_cause\n",
    "\n",
    "\n",
    "class TypeLevelCause:\n",
    "    r\"\"\"\n",
    "    A potential cause of an effect, with a time window, a probability of\n",
    "    occurrence, and a list of token events that support this type-level\n",
    "    cause. In PCTL language, `c \\leadsto {}^{\\geq r, \\leq s}_{\\geq p} e`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, relation: CausalRelation, window: Window, prob: float, token_events: List[TokenCause]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a type-level cause\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        relation : CausalRelation, the cause and effect\n",
    "        window : Window, the time window in which the effect occurs after the cause\n",
    "        prob : float, the probability of the effect occurring in the window\n",
    "        token_events : List[TokenCause], the token events that support this type-level cause\n",
    "        \"\"\"\n",
    "        self.relation = relation\n",
    "        self.window = window\n",
    "        self.prob = prob\n",
    "        self.token_events = token_events\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"TypeLevelCause({self.relation}, {self.window}, {self.prob})\"\n",
    "\n",
    "    # define equality and hashing based on the relation, window, prob, and token events\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, TypeLevelCause):\n",
    "            return (\n",
    "                self.relation == other.relation\n",
    "                and self.window == other.window\n",
    "                and self.prob == other.prob\n",
    "                and self.token_events == other.token_events\n",
    "            )\n",
    "        return False\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.relation, self.window, self.prob, tuple(self.token_events)))\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CompositeScore:\n",
    "    lag_scores: np.array\n",
    "\n",
    "    @property\n",
    "    def score(self):\n",
    "        return np.sum(self.lag_scores)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add a class that identifies the causal relationships and computes their relative significance. This makes it easy to compute things with a few simple method calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalTree:\n",
    "    def __init__(self, df: pd.DataFrame, cause_names: List[str], effect_name: str):\n",
    "        CausalTree.validate_causal_variables(df, cause_names, effect_name)\n",
    "        self.df = df\n",
    "        self.store = VariableStore()\n",
    "\n",
    "        for name in cause_names + [effect_name]:\n",
    "            self.store.add(name)\n",
    "\n",
    "        self.type_level_causes: List[TypeLevelCause] = []\n",
    "        self.type_level_significance_scores: List[CompositeScore] = []\n",
    "        self.max_lag = -1\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_causal_variables(df: pd.DataFrame, cause_names: List[str], effect_name: str):\n",
    "        # check that all variables are in the dataframe\n",
    "        missing_names = set(cause_names) - set(df.columns)\n",
    "        if effect_name not in df.columns:\n",
    "            missing_names.add(effect_name)\n",
    "        if len(missing_names) > 0:\n",
    "            raise ValueError(f\"Variables not in df: {missing_names}\")\n",
    "\n",
    "        # check that all variables are binary-valued\n",
    "        non_binary_cols = []\n",
    "        for col in df.columns:\n",
    "            if not df[col].isin([0, 1]).all():\n",
    "                non_binary_cols.append(col)\n",
    "        if len(non_binary_cols) > 0:\n",
    "            raise ValueError(f\"Non-binary columns: {non_binary_cols}\")\n",
    "\n",
    "    def get_variable_name(self, i: VariableIndex) -> str:\n",
    "        return self.store.lookup_by_index(i)\n",
    "\n",
    "    def get_variable_index(self, name: str) -> VariableIndex:\n",
    "        return self.store.lookup_by_name(name)\n",
    "\n",
    "    def get_relation(self, cause: str, effect: str) -> CausalRelation:\n",
    "        cause_idx = self.store.lookup_by_name(cause)\n",
    "        effect_idx = self.store.lookup_by_name(effect)\n",
    "        return CausalRelation(cause_idx, effect_idx)\n",
    "\n",
    "    def cause_holds_at(self, t: int, cause: Union[str, VariableIndex]) -> bool:\n",
    "        # Returns true if the given cause is true at the given time step\n",
    "        cause_name = cause if isinstance(cause, str) else self.get_variable_name(cause)\n",
    "        return self.df.at[t, cause_name] == 1\n",
    "\n",
    "    def effect_holds_in(self, window: Window, effect: Union[str, VariableIndex]) -> bool:\n",
    "        # Returns true if the given effect happened in the given window\n",
    "        e = effect if isinstance(effect, str) else self.get_variable_name(effect)\n",
    "        return self.df.loc[window.start : window.end, e].sum() > 0\n",
    "\n",
    "    def get_token_effect_times(self, relation: CausalRelation, window: Window, t: int) -> List[int]:\n",
    "        # Returns time step(s) where causal effect is true in the given window\n",
    "        w = Window(t + window.start, t + window.end)\n",
    "        cause_holds = self.cause_holds_at(t, relation.cause)\n",
    "        effect_holds_in_window = self.effect_holds_in(w, relation.effect)\n",
    "\n",
    "        if cause_holds and effect_holds_in_window:\n",
    "            effect_name = self.get_variable_name(relation.effect)\n",
    "            # find each time step where effect is true\n",
    "            subset = self.df.loc[w.start : w.end, effect_name]\n",
    "            indices = subset[subset == 1].index\n",
    "            effect_times = [t for t in indices]\n",
    "            return effect_times\n",
    "        return []\n",
    "\n",
    "    def c_leadsto_e_in(self, window: Window, relation: CausalRelation, t: int) -> int:\n",
    "        # Returns 1 if cause leads to effect inside window, 0 otherwise.\n",
    "        w = Window(t + window.start, t + window.end)\n",
    "        cause_holds = self.cause_holds_at(t, relation.cause)\n",
    "        effect_holds_in_window = self.effect_holds_in(w, relation.effect)\n",
    "\n",
    "        if cause_holds and effect_holds_in_window:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def num_time_steps(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def identify_potential_cause(\n",
    "        self, relation: CausalRelation, window: Window\n",
    "    ) -> Optional[TypeLevelCause]:\n",
    "        # Finds type-level cause that precedes effect inside the window, if it exists.\n",
    "        dt = window.end - window.start\n",
    "        token_causes = []\n",
    "        num_c_leadsto_e = 0\n",
    "\n",
    "        for t in range(self.num_time_steps - dt):\n",
    "            num_c_leadsto_e += self.c_leadsto_e_in(window, relation, t)\n",
    "            effect_times = self.get_token_effect_times(relation, window, t)\n",
    "            for t_effect in effect_times:\n",
    "                token_causes.append(TokenCause(relation, t, t_effect))\n",
    "\n",
    "        if num_c_leadsto_e > 0:\n",
    "            cause_column = self.get_variable_name(relation.cause)\n",
    "            prob = num_c_leadsto_e / self.df[cause_column].sum()\n",
    "            return TypeLevelCause(relation, window, prob, token_causes)\n",
    "        return None\n",
    "\n",
    "    def build(self, effect: str, max_lag: int = 5, verbose: bool = False):\n",
    "        # Build the tree\n",
    "        if effect not in self.store:\n",
    "            raise ValueError(f\"Effect variable {effect} not in store\")\n",
    "\n",
    "        self.max_lag = max_lag  # maximum lag to consider\n",
    "        # create all possible windows\n",
    "        windows = []\n",
    "        for start in range(1, max_lag):\n",
    "            for end in range(start + 1, max_lag + 1):\n",
    "                windows.append(Window(start, end))\n",
    "        print(f\"Created {len(windows)} windows.\")\n",
    "        if verbose:\n",
    "            for window in windows:\n",
    "                print(window)\n",
    "\n",
    "        # identify all potential type-level causes\n",
    "        for cause in self.store.names:\n",
    "            if cause == effect:\n",
    "                continue\n",
    "            if verbose:\n",
    "                print(f\"Finding valid windows for {cause} => {effect}\")\n",
    "            relation = self.get_relation(cause, effect)\n",
    "            num_causes = 0\n",
    "            num_token_events = 0\n",
    "\n",
    "            for window in windows:\n",
    "                type_level_cause = self.identify_potential_cause(relation, window)\n",
    "                if type_level_cause is not None:\n",
    "                    self.type_level_causes.append(type_level_cause)\n",
    "                    num_causes += 1\n",
    "                    num_token_events += len(type_level_cause.token_events)\n",
    "            if verbose:\n",
    "                print(f\"Found {num_causes} type-level relations for {cause} => {effect}\")\n",
    "                print(f\"Found {num_token_events} token events for {cause} => {effect}\")\n",
    "\n",
    "    def _prob_given_c_and_x(self, c: TypeLevelCause, x: TypeLevelCause) -> np.array:\n",
    "        # Computes $P(e\\vert c \\land x)$ for all lags        \n",
    "        window = c.window\n",
    "        cause = self.get_variable_name(c.relation.cause)\n",
    "        effect = self.get_variable_name(c.relation.effect)\n",
    "        x_name = self.get_variable_name(x.relation.cause)\n",
    "        dt = window.end - window.start\n",
    "        num_e_after_cx = np.zeros(self.max_lag + 1)\n",
    "\n",
    "        for t in range(self.num_time_steps - dt):\n",
    "            # if cause or x did not happen, skip\n",
    "            if self.df.at[t, cause] == 0 or self.df.at[t, x_name] == 0:\n",
    "                continue\n",
    "            for t1 in range(t + window.start, t + window.end + 1):\n",
    "                if t1 >= self.num_time_steps:\n",
    "                    break\n",
    "                lag = t1 - t\n",
    "                num_e_after_cx[lag] += self.df.at[t1, effect]\n",
    "\n",
    "        if num_e_after_cx.sum() == 0:\n",
    "            return np.zeros(self.max_lag + 1)\n",
    "\n",
    "        num_c_and_x = self.df[(self.df[cause] == 1) & (self.df[x_name] == 1)].shape[0]\n",
    "        if num_c_and_x == 0:\n",
    "            return np.zeros(self.max_lag + 1)\n",
    "        prob = num_e_after_cx / num_c_and_x\n",
    "        return prob\n",
    "\n",
    "    def _prob_given_notc_and_x(self, c: TypeLevelCause, x: TypeLevelCause) -> np.array:\n",
    "        # Computes $P(e\\vert \\neg{c} \\land x)$, for each lag\n",
    "        window = c.window\n",
    "        cause = self.get_variable_name(c.relation.cause)\n",
    "        effect = self.get_variable_name(c.relation.effect)\n",
    "        x_name = self.get_variable_name(x.relation.cause)\n",
    "        dt = window.end - window.start\n",
    "        num_e_after_notcx = np.zeros(self.max_lag + 1)\n",
    "\n",
    "        for t in range(self.num_time_steps - dt):\n",
    "            # if c happened or x did not happen, skip\n",
    "            if self.df.at[t, cause] == 1 or self.df.at[t, x_name] == 0:\n",
    "                continue\n",
    "            for t1 in range(t + window.start, t + window.end + 1):\n",
    "                if t1 >= self.num_time_steps:\n",
    "                    break\n",
    "                lag = t1 - t\n",
    "                num_e_after_notcx[lag] += self.df.at[t1, effect]\n",
    "\n",
    "        if num_e_after_notcx.sum() == 0:\n",
    "            return np.zeros(self.max_lag + 1)\n",
    "\n",
    "        num_notc_and_x = self.df[(self.df[cause] == 0) & (self.df[x_name] == 1)].shape[0]\n",
    "        if num_notc_and_x == 0:\n",
    "            return np.zeros(self.max_lag + 1)\n",
    "        prob = num_e_after_notcx / num_notc_and_x\n",
    "        return prob\n",
    "\n",
    "    def filter_type_level_causes_by_window(self, window: Window) -> List[TypeLevelCause]:\n",
    "        return [c for c in self.type_level_causes if c.window == window]\n",
    "\n",
    "    def get_other_causes_in_window(self, c: TypeLevelCause) -> Set[TypeLevelCause]:\n",
    "        return set(self.filter_type_level_causes_by_window(c.window)) - {c}\n",
    "\n",
    "    def _compute_type_level_significance(self, cause: TypeLevelCause) -> np.array:\n",
    "        e_avg = np.zeros(self.max_lag + 1)\n",
    "        X = self.get_other_causes_in_window(cause)\n",
    "        for x in X:\n",
    "            e_avg += self._prob_given_c_and_x(cause, x) - self._prob_given_notc_and_x(cause, x)\n",
    "        e_avg /= len(X)\n",
    "        return e_avg\n",
    "\n",
    "    def compute_significance(\n",
    "        self,\n",
    "        verbose: bool = False,\n",
    "    ) -> List[CompositeScore]:\n",
    "        #  Compute significance of each type-level cause\n",
    "        scores = []\n",
    "        for cause in self.type_level_causes:\n",
    "            if verbose:\n",
    "                print(\"Computing significance of type-level cause: %s\" % cause)\n",
    "            e_avg = self._compute_type_level_significance(cause)\n",
    "            scores.append(CompositeScore(e_avg))\n",
    "\n",
    "        self.type_level_significance_scores = scores\n",
    "        return scores\n",
    "\n",
    "    def prune(self):\n",
    "        # Remove type-level causes with negative significance score from tree\n",
    "        pruned_causes = []\n",
    "        pruned_scores = []\n",
    "        for cause, score in zip(self.type_level_causes, self.type_level_significance_scores):\n",
    "            if score.score.sum() > 0:\n",
    "                pruned_causes.append(cause)\n",
    "                pruned_scores.append(score)\n",
    "        self.type_level_causes = pruned_causes\n",
    "        self.type_level_significance_scores = pruned_scores\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can identify the type-level relationships and find the most significanct ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = CausalTree(df_D, list(variable_names), \"high_total_users\")\n",
    "tree.build(\"high_total_users\", max_lag=3)\n",
    "tree.compute_significance()\n",
    "tree.prune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass \n",
    "class ScoredTypeLevelCause:\n",
    "    cause: TypeLevelCause\n",
    "    score: float\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.cause}: {self.score:.2f}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "scored_causes = []\n",
    "for cause, composite_score in zip(tree.type_level_causes, tree.type_level_significance_scores):\n",
    "    scored_causes.append(ScoredTypeLevelCause(cause, composite_score.score))\n",
    "\n",
    "scored_causes.sort(key=lambda x: x.score, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in scored_causes[:10]:\n",
    "    cause = tree.get_variable_name(c.cause.relation.cause)\n",
    "    effect = tree.get_variable_name(c.cause.relation.effect)\n",
    "    print(f\"{cause} => {effect}: {c.score:.2f}, {c.cause.prob:.2f} p-value, {c.cause.window} \")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zkcausality-NQsw8E1t-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
