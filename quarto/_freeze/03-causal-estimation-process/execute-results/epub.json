{
  "hash": "811b1ebcc5010e355d283f194664c07b",
  "result": {
    "markdown": "# Causal Inference: A Practical Approach {#sec-causalestimation}\n\nHaving laid the foundation with the potential outcome framework and fundamental causal concepts, we now delve into the world of causal modeling. In this chapter, we explore the power of causal graphs as a comprehensive approach for inferring causal relationships. Firstly, we introduce causal graphs, explaining the basics of graph modeling in the context of causal inference. Next, we offer an overview of the high-level process involved in causal inference using causal graphs. Subsequently, we discuss each stage of the causal inference process, providing a more detailed examination of the methods and techniques commonly employed. Finally, we present a comprehensive case study utilizing the Lalonde dataset. This study compares and contrasts the various techniques for causal inference discussed, accompanied by a thorough analysis.\n\n## Causal Inference: Logical Flow\n\nIn the last chapter, we introduced the causality flowchart for estimating the causal effect, as shown in @fig-causalestimandtoestimate1.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Causality flowchart: From a target causal estimand to an estimate](img/PotentialOutcomeEstimation.png){#fig-causalestimandtoestimate1 fig-align='center' width=90%}\n:::\n:::\n\n\n\n\nThe two pertinent questions will be answered in this chapter, as shown in @fig-causalestimandtoestimate2.\n\n\n\n1. How do we perform identification and convert causal estimands into statistical estimands? What are the tools available for this process?\n\n>_The identification and conversion of causal estimands to statistical estimands are achieved through causal modeling._\n\n2. Once we have obtained statistical estimands, how do we proceed with estimation? What tools can be utilized for this purpose?\n\n>_Estimation is carried out by leveraging the statistical estimands derived from the causal modeling process and utilizing observational data in conjunction with appropriate statistical techniques._\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Causality Flowchart: How to go from a target causal estimand to an estimate using causal modeling](img/PotentialOutcomeEstimationHow.png){#fig-causalestimandtoestimate2 fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n::: {.callout-tip}\nCausal models can be constructed either by experts with domain knowledge or through automated procedures known as causal discovery (discussed in @sec-causaldiscovery). Building causal models requires domain knowledge, identifying relevant variables, and determining the relationships between them.\n:::\n\n\n## Causal Inference: Practical Flow\n\nIn the previous section, we examined the logical flowchart that illustrates transitioning from a target causal estimand to an estimate. Now, we will delve into the practical aspect of this process, providing a high-level overview of the steps involved in practice.\n\nThis section aims to bridge the gap between theory and application, offering insights into how the causal inference process unfolds in real-world scenarios.\n\n\nNotably, unlike the machine learning process, the causal inference process involves distinct steps, as Dow et al. underscored in their work [@dowhypaper] and given by @fig-inferencingprocess.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Causal inference process in practice](img/InferencingProcess.png){#fig-inferencingprocess fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n1. To initiate the causal modeling process, one of the methodologies employed is constructing a causal graph with appropriate structural assumptions. Causal graphs depict the causal structure by utilizing nodes to represent variables and edges to represent the causal relationships between them.\n\n2. After establishing the causal assumptions within a causal model, the subsequent stage of causal analysis is identification. During this stage, the objective is to examine the causal model, encompassing the relationships between variables and the observed variables, to ascertain if there is sufficient information to address a particular causal inference question. Different techniques are utilized with the aim of converting the causal estimands into statistical estimands.\n\n3. After confirming the estimability of the causal effect and transforming the causal estimands into statistical estimands, the third step revolves around estimating the effect using suitable statistical estimators. Different estimation techniques, such as regression models or propensity score matching, can be utilized to estimate the causal effect.\n\n4. Finally, the fourth step involves validating and assessing the robustness of the obtained estimate through rigorous checks and sensitivity analyses. This step includes examining the sensitivity of the estimated effect to different model specifications, testing the robustness of the results against potential sources of bias or unobserved confounding, and assessing the generalizability of the findings.\n\n## Causal Modeling\n\nThe initial phase of any causal inference effort entails constructing causal models, graphical models in our case, which encode domain understanding and assumptions. A well-designed causal model should capture most of the relationships between the outcome and variables and inter-relationships between the variables.\n\nCausal graphs are a common way of representing the relationships between variables when modeling the joint distribution.\n\n::: {.callout-tip}\nA cause in a directed acyclic graph (DAG), similar to Bayesian networks, is defined as if any changes are made to a node, a response, or corresponding modifications are observed in the connected node(s). In the context of graphical models, a causal graph is a type of Bayesian network in which each node's direct causes are represented by its parents.\n:::\n\nFor a comprehensive and in-depth exploration of causal graphs, we highly recommend referring to Judea Pearl's work [@pearl2009overview; @pearl2000].\n\n### Assumptions in Causal Modeling\n\n@fig-causalgraph shows the entire flow of assumptions that help in causal modeling.\n\n* __Local Markov Assumption__:\nGiven the parents in the DAG,  a node is independent of all its non-descendants.\n* __Minimality assumption__:\nThe adjacent nodes in the DAG are dependent and have to be considered in the factorization in addition to the local Markov assumption. This assumption removes independent assumptions when the edges are present in the DAG.\n* __Causal Edge assumption__:\nIn causal relationships, every parent is the direct cause of their children.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Causal Graph Assumptions](img/CausalGraphs.png){#fig-causalgraph fig-align='center' width=1546}\n:::\n:::\n\n\n\n\nThus for a given distribution represented by DAGs, the local Markov assumption helps to measure statistical independence, the minimality assumption helps to focus on statistical dependencies (at least between the adjacent nodes), and finally, layering the causal edge assumption gives us the causal dependencies.\n\nCausal graphs possess an essential characteristic of being able to identify confounding variables. These variables are associated with the cause and the effect but are not causally connected. For instance, in the case of _smoking_ and _lung cancer_, _age_ may act as a confounding variable related to both _smoking_ and _lung cancer_ but does not lie on the causal pathway between them. By including _age_ as a node in the causal graph, researchers can control for its effects and isolate the causal relationship between _smoking_ and _lung cancer_.\n\nCausal graphs can identify the minimal set of variables, which are imperative in estimating the causal effect of one variable on another. This technique is known as the _identification_ (discussed later), based on the notion that several paths may exist between two variables in a causal graph. However, only some are crucial in estimating the causal effect. By identifying the minimal set of variables needed, researchers can effectively and accurately estimate the causal effect with less bias and more efficiency.\n\n### Building Blocks in Causal Graphs\n\nIn graph theory, the term \"flow of association\" refers to the presence or absence of association between any two nodes in a given graph. This concept can also be expressed as the statistical dependence or independence between two nodes. In the following section, we shall delve into fundamental constituents such as the building block and terminologies essential to graphical representations of various causal associations between variables. Furthermore, we shall conduct preliminary analyses to investigate the variables' conditional independence or dependence within the building blocks.\n\n\n#### Chains\n\nA chain is a sequence of nodes such that each node is a parent of the next node in the sequence as shown in @fig-chains.\n\nThere is dependence between $X_1$ and $X_2$ and also between $X_2$ and $X_3$ because of the causal edges assumption. $X_1$ and $X_3$ are dependent as there is a flow of association or statistical dependence from $X_1$ to $X_3$ through $X_2$. Similarly, if we condition on the $X_2$, $X_1$ and $X_3$ are independent of each other. In other words, $X_2$ will block the association flow between $X_1$ and $X_3$ by conditioning.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Association flow in chains without conditioning and with conditioning](img/ChainFlowVsBlocked.png){#fig-chains fig-align='center' width=1353}\n:::\n:::\n\n\n\n\n#### Forks\n\nIn a fork, two variables have a single parent between them, as shown in @fig-forks. Similar to chains, there is dependence between $X_1$ and $X_2$ and between $X_2$ and $X_3$ because of the causal edges assumption. $X_1$ and $X_3$ are dependent as there is a flow of association or statistical dependence from $X_1$ to $X_3$ through $X_2$. Also, if we condition on the $X_2$, $X_1$ and $X_3$ are independent. In other words, $X_2$ will block the association flow between $X_1$ and $X_3$ by conditioning.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Association flow in forks without conditioning and with conditioning](img/ForkFlowVsBlocked.png){#fig-forks fig-align='center' width=638}\n:::\n:::\n\n\n\n\n#### Immoralities\n\nImmoralities refer to a configuration in a directed acyclic graph where a single node relies on two parents who are not directly connected, as depicted in @fig-immorality. The node $X_2$ in this structure is called a _collider_ and obstructs the flow of association between nodes $X_1$ and $X_3$ without conditioning, unlike in chains or forks. However, unlike chains or forks, by conditioning on the collider $X_2$, the flow of association or dependence persists between $X_1$ and $X_3$.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Association flow in immoralities without conditioning and with conditioning](img/ImmoralityFlowVsConditioned.png){#fig-immorality fig-align='center' width=638}\n:::\n:::\n\n\n\n\n\n\n#### Blocked Path\nThe concept of a blocked path is intimately tied to the flow of causal influence. A path between two nodes, $X$ and $Y$, can be blocked or unblocked by a conditioning set ($Z$). The two scenarios that one comes across are:\n\n1. If there exists a node $W$ on the path from $X$ to $Y$ such that it is part of a chain structure ($X \\rightarrow W \\rightarrow Y$) or a fork structure ($X \\leftarrow W \\rightarrow Y$), and $W$ is conditioned on ($W \\in Z$).\n\n2. There is a collider $W$ on the path that is not conditioned on ($W \\notin Z$), and none of its descendants are conditioned on, i.e. ($de(W) \\notin Z$).\n\n\n#### d-Separation\n\nThe concept of d-separation is tied to the concept of a blocked path in the graph. A path from a node(set) $X$ to a node(set) $Y$ is considered blocked given a set $Z$, if nodes block any node in $X$ and $Y$ in $Z$.\n\nThe concept of d-separation implies an important theorem that $X$ and $Y$ are conditionally independent given $Z$. On the contrary, if there exists at least one unblocked path from $X$ to $Y$ given $Z$, then $X$ and $Y$ are not d-separated by $Z$, suggesting that $X$ and $Y$ are not conditionally independent given $Z$. Mathematically,\n\n\n\n$$ X \\perp_G Y |Z \\implies X \\perp_P Y|Z $$\n\n### Causal Graphs and Structural Interventions\n\nIn the context of Causal graphs, a __structural intervention__ is represented as an exogenous variable $I$, a variable without any causes. It has two possible states (on/off), with a single arrow pointing to the variable it manipulates. When $I$ is set to the off state, the passive observational distribution is obtained over the variables. In contrast, when $I$ is set to the on state, all other arrows incident on the intervened variable are removed. The probability distribution over the intervened variable is determined solely by the intervention.\n\nFor example, consider a causal graph representing the relationship between smoking, age, and cancer, as shown in @fig-intervention. Suppose that _smoking_ and _age_ are the direct causes of _cancer_. A structural intervention can be performed on smoking, where $I$ is introduced as an exogenous variable with two states: on and off. When $I$ is off, the system behaves according to the passive observational distribution, where the effect of _age_confounds the effect of smoking on _cancer_. However, when $I$ is on, all other arrows pointing to _cancer_ from variables other than _smoking_ are removed, and the probability distribution of _cancer_ is a deterministic function of _smoking_ only, allowing for the identification of the causal effect of _smoking_ on _cancer_.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Structural Interventions](img/Intervention.png){#fig-intervention fig-align='center' width=1608}\n:::\n:::\n\n\n\n\nThis \"structural\" property is critical to understanding the effect of interventions on causal graphs. Suppose there are multiple simultaneous structural interventions on variables in the graph. In that case, the manipulated distribution for each intervened variable is independent of every other manipulated distribution, and the edge-breaking process is applied separately to each variable. This process implies that all edges between variables subject to intervention are removed. After removing all edges from the original graph incident to variables that are the target of a structural intervention, the resulting graph is called the post-manipulation graph, which represents the manipulated distribution over the variables.\n\n### Observational Data and Interventional Data\n\nNext, let us discuss the terminologies of observational and interventional data, which play a vital role in understanding the causal process in greater detail.\n\nObservational data is collected simply by passively observing a system or population without any intervention or change in the process. In contrast, interventional data is collected by actively manipulating the system or population in some way, like in randomized control trials. Observational studies may be subject to various types of biases, such as confounding, selection bias, and measurement bias, making it difficult to distinguish between causal and non-causal relationships in the dataset.\n\nInterventional data, on the other hand, is often considered to be the gold standard for establishing causal relationships between variables. This is because interventions allow actively manipulating the independent variable and observing the resulting changes in the dependent variable. Researchers can minimize the effects of confounding and other biases by randomly assigning participants to different treatment groups, like in randomized control trials.\n\nThe acquisition of observational data is generally less resource-intensive than interventional data, which can be expensive and impractical to obtain in specific scenarios. This raises the question of whether it is possible to derive interventional data from observational data.\n\n\n### The *do*-operator and Interventions\n\n::: {.callout-tip}\nThe *do*-operator and identification process are essential tools that facilitate us going from observational to interventional data. The *do*-operator helps distinguish interventional distributions from observational distributions, while identification helps determine which causal relationships can be inferred from the observed data. We will discuss the process in detail in the following section.\n:::\n\nThe *do*-operator is a symbolic notation used in causal inference to represent interventions. The *do*-operator is a notation to represent the population's intervention distribution. Given a treatment ($T=t$), __intervention__ corresponds to the __whole population__ subjected to that treatment and given by $do(T=t)$. This is different from the __conditional__ distribution ($P(Y|T=t)$), which represents a __subset__ of the population $(T=t)$ rather than the whole population in the case of intervention as shown in the @fig-conditionalvsinterventional. The interventional measures can be computed only through experiments, while the conditional measures can be computed directly from the observational data.\n\nAlso, the potential outcome ($P(Y(t)=y)$) in terms of interventional distribution using the *do*-operator is given by:\n\n$$P(Y(t)=y) \\triangleq P(Y=y | do(T=t)) \\triangleq P(y|do(t)) $$\nSimilarly, the average treatment effect (ATE) in the case of binary treatment using the *do*-operator is given by:\n\n$$ATE = \\mathbb{E}[Y|do(T=1)] - \\mathbb{E}[Y|do(T=0)]$$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Conditional vs Interventiona](img/ConditionalVsInterventional.png){#fig-conditionalvsinterventional fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n### Modularity assumptions\n\nA modular assumption is about the local impact of any intervention when applied to a causal graph. Modularity assumption is also known as invariance, autonomy, and independent mechanisms.\n\nIt states that if a node $X_i$ is being intervened, then only the mechanism $P(x_i|pa_i)$ changes and the intervention has no impact on any other mechanism changes, i.e., $P(x_j|pa_j)$ where $i \\neq j$ remain unchanged. Modularity assumptions is graphically demonstrated in @fig-parents. The violation of the modularity assumption implies that when a node is intervened, mechanisms of other non-parent nodes changes, and thus any local effect assumption goes away for computation purposes.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Modularity and Interventions](img/Parents.png){#fig-parents fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n### Modularity Assumptions and Truncated Factorization\n\nThe network factorization for @fig-parents can be written as:\n$$P(x_1, \\cdots, x_n) = \\prod_iP(x_i|pa_i) $$\nWhen we intervene on variable set $S$, the factorization becomes:\n$$P(x_1, \\cdots, x_n|do(S=s)) = \\prod_{i\\notin X S}P(x_i|pa_i) $$\nif the $x$ is consistent with the intervention, otherwise\n$$P(x_1, \\cdots, x_n|do(S=s)) = 0$$\nThe truncated factorization can be employed to estimate the causal effect of treatment $T$ on outcome $Y$ in a simple graph comprising three variables $X$, $Y$, and $T$, with $X$ as the confounder as shown in @fig-truncated. The aim is to estimate the causal quantity $P(y|do(T))$.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Modularity and Interventions](img/TruncatedFactorization.png){#fig-truncated fig-align='center' width=50%}\n:::\n:::\n\n\n\nThe network factorization gives us the following:\n\n$$ P(y,t,x) = P(x)P(t|x)P(y|t,x)$$\nWhen we intervene on treatment $T$, we can remove $t$ factor, i.e. $P(t|x)$, thus the truncated factorization gives us:\n\n$$P(y,x|do(T)) = P(x)P(y|t,x)$$\nNext, if we marginalize $x$ using summation (discrete) or integration (continuous), we get:\n$$P(y|do(T)) = \\sum_xP(y|t,x)P(x)$$\nThus, we got identifiability or could go from causal estimand $P(y|do(T))$ to statistical estimand $\\sum_xP(y|t,x)P(x)$ from observational data.\n\nNow, in a separate statistical world we can rewrite $P(y|t)$ using marginalization as:\n$$P(y|t) = \\sum_xP(y,x|t)$$\nUsing the the factorization property:\n$$P(y|t) = \\sum_xP(y|t)P(x|t)$$\nThus, comparing the two equations, we can conclude that\n$$P(y|do(t)) \\neq P(y|t)$$\nas $P(y|do(t))$ has $P(x)$ where as $P(y|t)$ has $P(x|t)$.\n\n\n### Structural Causal Models (SCM)\n\nStructural Causal Models (SCMs) are a critical component of modeling for causal inference. SCMs are mathematical models representing the causal relationships between variables in a system [@spirtes2000causation].\n\nStructural Causal Models (SCMs) consist of two fundamental constituents: endogenous variables' structural equations that portray causal relationships among variables and exogenous variables that represent system variables unaffected by other variables. Causal graph structures depict the structural equations as functions among the variables, visually showcasing the relationships between exogenous and endogenous variables.\n\nIn simplest form, if variable $X$ causes $Y$, then it can be written as a structural equation:\n\n$$ Y := f(X)$$\nStochasticity can be added to the structural equation in the form of noise variables ($U$), and the above equation can be rewritten as:\n\n$$ Y := f(X, U)$$\nWhen there are many cause-effect relationships, as shown in @fig-scm, the representation using structural causal models equation with exogenous, endogenous, and noise variables is given by:\n$$X = f_X(W,U_X)$$\n$$Z = f_Z(X,Y,U_Z)$$\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Structural Causal Models](img/SCM.png){#fig-scm fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n#### Interventions and Modularity Assumptions in SCM\n\nFor the basic causal model as shown in @fig-scmintervention, with single confounder $X$ affecting both the treatment $T$ and the outcome $Y$, the SCM equations can  be written as:\n$$ T = f_T(X, U_T)$$\n$$Y = f_Y(X,T, U_Y)$$\nThe interventional SCM or a submodel follows the same as above but replaces the function $f_T(X,U_T)$ with a variable $t$ as:\n$$ T = t$$\n$$Y = f_Y(X,T, U_Y)$$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Intervention and removing the edge corresponding to $do(T=t)$](img/SCM-Intervention.png){#fig-scmintervention fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n\n## Identification\n\n::: {.callout-tip}\nIdentification is the process of converting causal estimands to statistical estimands. i.e., to go from $P(Y|do(t))$ to $P(Y|t)$. If we can go from $P(Y|do(t))$ to $P(Y|t)$, we have __identifiability__.\n:::\n\nLet us consider a simple identification process with one treatment ($T$), one confounding variable ($X$), and one outcome ($Y$) as given in @fig-identification.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Identification](img/Identification.png){#fig-identification fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n__Identification__ is to find $P(y|do(T))$, which can be iteratively computed from the joint distribution as below:\n$$ P(y,t,x) = P(x)P(t|x)P(y|t,x)$$\n\nApplying the modularity assumption by intervening on the variable ($T$),we get $P(t|x)=1$ and thus the equation transforms:\n$$ P(y,x|do(T)) = P(x)P(y|t,x)$$\n\nIf we marginalize the variable ($X$), we get:\n$$ P(y|do(T)) = \\sum_x P(y|t,x)P(x)$$\n\nThus, from the causal estimand $P(y|do(T))$, and identification, we have an equation with only observational or statistical quantities $\\sum_x P(y|t,x)P(x)$. This is called the __adjustment formula__.\n\n::: {.callout-tip}\nAs highlighted in the overall process illustrated in @fig-identificationmath, the identification process involves a transformation from causal estimand $P(y|do(T))$ to statistical estimand ($\\mathbb{E}_{X} P(y|t,X)$ or $P(y|t)$) contingent upon the existence or absence of confounding variables.\n:::\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Identification in presence or absence of confouding variable ](img/IdentificationWithWithoutConfoudners.png){#fig-identificationmath fig-align='center' width=90%}\n:::\n:::\n\n\n\n\nIdentification methods can be classified into two broad categories with subcategories, as shown below.\n\n* Graphical Constraint-based Methods\n  * Randomized Control Tests\n  * Backdoor Adjustments\n  * Frontdoor Adjustments\n* Non-Graphical Constraint-based Methods\n  * Instrumental Variables\n  * Regression Discontinuity\n  * Difference-in-Differences\n  * Pearl's do-calculus\n\n\nNext, we will go over these different methods of identification.\n\n### Randomized Control Trials (RCT)\nAs elucidated in the second chapter, comprehending the impact of unseen confounding factors when measuring treatment effects on an outcome is challenging. One way to address this issue is through randomized controlled trials, which introduce randomness in the treatment allocation process and ensure that the resulting groups (binary $T=0$ and $T=1$) are comparable, thus mitigating the confounding impact. In other words, randomization ensures exchangeability, i.e., the treatment groups are entirely interchangeable because of the randomization. Thus the average outcome of the two groups $\\mathbb{E}[Y|(T=1)]=y_1]$ and $\\mathbb{E}[Y|(T=0)]=y_0]$ remain the same. From a causal graph perspective, the randomized treatment removes the edge between the confounder $X$ and the treatment $T$, removing the backdoor path and the confounding association.\n\n### Backdoor Criterion and Backdoor Adjustment\nThe interventional causal graph can have various paths from the treatment ($T$) to the outcome ($Y$). Some of the paths are non-causal, and some of them are causal. One method to perform identification is to block backdoor paths [@pearl2010causal]. Backdoor paths are the edges that flow into the intervening treatment.\n\nAs shown in @fig-observationalvscausal, the left side graph is observational $P(Y| t)$ with various causal and non-causal paths between the treatment ($T$) and the outcome ($Y$). There are many non-causal paths from $T$ to $Y$, and they are $T \\rightarrow W \\rightarrow U \\rightarrow V \\rightarrow Y$, $T \\rightarrow X \\rightarrow Y$ and $T \\rightarrow P \\rightarrow Y$. Only backdoor paths are $T \\rightarrow W \\rightarrow U \\rightarrow V \\rightarrow Y$ and$T \\rightarrow X \\rightarrow Y$ as the edges flow into the intervening treatment $T$. The non-causal path $T \\rightarrow P \\rightarrow Y$ is not a backdoor path. The path $T \\rightarrow X \\rightarrow Y$ is the only causal path. The equivalent interventional causal graph $P(Y | do(t))$ on the right side is with the backdoor paths blocked by removing the edges.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Observational Vs Causal](img/ObservationalVsCausal.png){#fig-observationalvscausal fig-align='center' width=80%}\n:::\n:::\n\n\n\n\nCan we get an equivalent interventional causal graph from the observational graph? The answer is yes, and it can be done by conditioning the nodes/variables in the backdoor paths. By conditioning on $W$ and $X$, we get $P(Y |t,w,x)$ and that is equivalent to removing the edges and blocking the paths as shown in @fig-causalvsconditoning\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Causal vs Conditoning](img/CausalVsConditioning.png){#fig-causalvsconditoning fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\nFormally, a set of variables $Z$ are set to satisfy __backdoor criterion__ with respect to treatment $T$ and outcome $Y$ if it satisfies the following:\n\n* If the variable set $Z$ blocks all the backdoor paths from $T$ to $Y$\n* $Z$ does not contain any descendant of the treatment $T$\n\nThus, based on the modularity assumption and the backdoor criterion, one can identify the causal effect by:\n\n$$P(y| do(t)) = \\sum_z P(y|t,z) P(z)$$\n\n\n### Front-door Adjustments\n\nJudea Pearl justifies the use of the front-door adjustment method through the illustration of an example in which the effect of smoking (treatment) on cancer (outcome) is studied while taking into account the influence of tar (observed mediator) and an unknown genotype (unobserved confounder), as depicted in @fig-pearlfrontdoor [@pearl2010causal]. In such Directed Acyclic Graphs (DAGs), the backdoor criterion is inapplicable due to an unobserved confounder.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Cancer and Smoking relationship](img/PearlFrontdoor.png){#fig-pearlfrontdoor fig-align='center' width=50%}\n:::\n:::\n\n\n\n\nThe more generic DAG is shown in @fig-genericfrontdoor The intuition behind the front-door adjustment can be broken into the following three steps as follows:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Generalized Front-door](img/GenericFrontdoor.png){#fig-genericfrontdoor fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n\n\n* Identify the causal impact of treatment ($T$) on the mediator ($M$)\n$P(m|do(t)$\nSince there are no backdoor paths, we can write:\n$P(m|do(t) = P(m|t)$\n\n* Identify the causal impact of mediator ($M$) on the outcome ($Y$)\n$P(y|do(m)$\nSince there is a backdoor path from $M$ to $Y$ through $T$, we can use the backdoor criterion by conditioning on $T$.\n\n$$P(y|do(m) = \\sum_t P(y|m,t)P(t)$$\n\n\n* Combined the two previous steps to identify the causal impact of treatment ($T$) on the outcome ($Y$)\n$$P(y|do(t) = \\sum_m P(m|do(t))P(y|do(m)) $$\n$$P(y|do(t) = \\sum_m P(m|t) \\sum_{t'}P(y|m,t')P(t') $$\nThe above equation is called the frontdoor adjustment. The set of variables $M$ satisfy the frontdoor criterion if:\n1. Variable $M$ mediates the effect of $T$ on $Y$.\n2. There is no unlocked backdoor path from $T$ to $M$.\n3. All backdoor paths from $M$ to $Y$ are blocked by $T$.\n\n### Instrumental Variable Analysis\n\nIn situations where specific variables affect the treatment variable(s) but do not directly influence the outcome variable, identification can be achieved through instrumental variable analysis. For instance, consider the example of three variables, namely smoking, cigarette prices, and cancer, as shown in @fig-ivexample. It is apparent that cigarette prices affect whether an individual smokes but do not directly impact the likelihood of developing cancer. Such variables that affect the treatment variable(s) but not the outcome variable are referred to as instrumental variables, as described by Pearl [@pearl2010causal].\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Example of Instrumental Variables](img/IVExample.png){#fig-ivexample fig-align='center' width=50%}\n:::\n:::\n\n\n\n\nThe role of instrumental variables in the identification process is to help address the problem of endogeneity, which occurs when a variable of interest is correlated with the error term in a regression model. This correlation leads to biased and inconsistent estimates of the treatment effect, making it difficult to establish a causal relationship between the treatment and the outcome variable. By using an instrumental variable that is uncorrelated with the error term and affects the treatment but not the outcome variable, the IV analysis can isolate the causal effect of the treatment on the outcome variable.\n\nThus, when generalized, as shown in @fig-ivgeneral, the identification process is a two-stage process. The first step is to measure the effect of the instrument variable on the treatment using regression as given by:\n$$\\hat{T} = \\beta_0 + \\beta_1Z $$\nand then use the predictor $\\hat{T}$ to measure the effect on the outcome $Y$ as another regression, given by:\n$$\\hat{Y} = \\beta_2 + \\beta_3\\hat{T} $$\nIn addition to the conditions described above being met, instrumental variable analysis is applicable when there are only moderate to small confounding effects and a good sample size of observational data.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Generalized relationship between Instrumental Variables, Treatment, and Outcome](img/IVGeneric.png){#fig-ivgeneral fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n\n### Regression Discontinuity\nThe regression discontinuity approach is a regression-based technique that is well-suited for identifying real-valued outcomes, particularly in scenarios where the outcome data contain thresholds or cut-offs. This method is commonly applied in cases where treatment is provided when the outcome surpasses a certain threshold but not when it falls below it [@imbens2008regression]. The treatment/intervention impact above/below the threshold can be used for causality estimation. Examples include receiving scholarships and their implications on admissions/SAT scores or receiving a specific medicine dosage for patients above a certain cut-off of diabetes or cholesterol etc.\n@fig-regressiondiscontinuity shows an example of student GPA as the outcome ($Y$) on the y-axis, and student test scores normalized as a variable ($X$) on the x-axis with a threshold deciding scholarship ($T$) and its impact as a shifted GPA on the y-axis. The shift is the regression discontinuity.\nRegression discontinuity can be measured using the same trend on either side of the discontinuity using the regression formula:\n$$\\hat{y} = \\beta_0 + \\beta_1X + \\beta_2I(x>x_0) $$\n$\\beta_2$ is the measure fo regression discontinuity.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Regression Discontinuity](img/RegressionDiscontinuity.png){#fig-regressiondiscontinuity fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n### Difference-in-Differences\n\nThe Difference-in-Differences (DID) approach is a regression-based method that effectively identifies real-valued outcomes when measured over time, as highlighted in [@lechner2011estimation]. Specifically, the DID approach allows for estimating the treatment effect by comparing the differences in outcomes over time between the treatment and control groups. This method is often applied at a particular time and enables estimating the treatment effect using regression analysis, wherein the significant differences between the treatment and control groups can be effectively captured.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Difference-in-Differences](img/DID.png){#fig-did fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n\nTo make things concrete, let us consider a simple use case of a binary treatment ($T=0, T=1$), with a real-valued outcome ($y \\in \\mathbb R$), and the outcome is measured w.r.t to time as shown in @fig-did Regression analysis can be done using a couple of variables: $D$ for treatment and $T$ for time as given by:\n\n$$ y= \\beta_0 + \\beta_1D + \\beta_2T + \\beta_3D \\times T + \\mu$$\nThe regression variables can be interpreted as $\\beta_1$ estimates the marginal effect of treatment, $\\beta_2$ estimates the baseline change over time for the control group, and $\\beta_3$ the treatment effect. There are several assumptions made from regression analysis and causal effect perspective, such as both the groups having common trends and non-independence of observations.\n\n### Pearl's do-calculus\nIf a query $Q$ is provided as a do-expression, such as $Q = P(y|do(x),z)$, its identifiability can be systematically determined using Pearl's do-calculus. In scenarios where both backdoor and front-door adjustment approaches fail to enable identification, Pearl's do-calculus provides an effective means of identifying any causal quantity that can be identified [@pearl2012calculus].\n\nConsider a causal Directed Acyclic Graph (DAG) $G$, where $X$, $Y$, $Z$, and $W$ are arbitrary disjoint sets of nodes. The graph $G_{\\overline{X}}$ is obtained by removing all arrows that point to nodes in $X$ from $G$. Similarly, the graph $G_{\\underline{X}}$ is obtained by deleting all arrows from nodes in $X$ from $G$. To indicate the removal of both incoming and outgoing arrows, we employ the notation $G_{\\overline{X}\\underline{Z}}$.\n\nThe following three rules apply to all interventional distributions that align with the structure of G.\n\n1. __Rule 1 (Insertion/deletion of observations)__:\n\nPer Rule 1, any observational node that fails to influence the outcome through a given path or is d-separated from the outcome can be safely disregarded.\n\nThe following is the formal definition:\n$$P(y|do(x),z,w) = P(y|do(x),w)\\ if\\ (Y \\perp Z|X,W)_{G_{\\overline{X}}}$$\nIf the nodes $Y$ and $Z$ are d-separated from one another, once we factor in both $W$ and $X$, we can eliminate $Z$ and remove it from the $P(y| z, w)$.\n\n2. __Rule 2 (Action/observation exchange)__:\n\nIn the context of a randomized controlled trial, researchers can assign treatment and perform either $do(x)$ or not $do(x)$. However, with observational data, it is not feasible to directly perform $do(x)$. It would be a significant advantage if we could treat an intervention like $do(x)$ as regular non-interventional observational data. Rule 2 facilitates this transformation.\n\nPer Rule 2, interventions, represented by $do(x)$, can be handled as observations when the causal effect of a variable on the outcome, specifically $X \\rightarrow Y$, influences the outcome solely through directed paths. Formally this can be written as:\n\n$$P(y|do(x),do(z),w) = P(y|do(x),z,w)\\ if\\ (Y \\perp Z|X,W)_{G_{{\\overline{X}}{\\underline{Z}}}}$$\nOne can note that the left-hand side involves the interventional operator $do(z)$, while the right-hand side employs the observed variable $z$. As long as the condition $(Y \\perp Z \\mid W)_{G{\\underline{Z}}}$ holds, we can convert $do(z)$ to $z$ and solely rely on observational data.\n\n\n3. __Rule 3 (Insertion/deletion of actions__)__:\nRule $3$ states that if an intervention (or a $do(\\cdot)$ expression) does not influence the outcome through any uncontrolled path, it can be disregarded. Specifically, we can eliminate $do(z)$ if no causal association (or unblocked causal paths) runs from $Z$ to $Y$.\n\n$$P(y|do(x),do(z),w) = P(y|do(x),w)\\ if\\ (Y \\perp Z|X,W)_{G_{\\overline{XZ(W)}}}$$\n\nBoth front-door and backdoor adjustment formulae can be derived using solely the do-calculus. It has been established that the do-calculus is complete, i.e., it can identify all the causal estimands if they exist [@huang2012pearl, @shpitser2006identification]. This theorem implies that if the repeated application of these three rules cannot eliminate the do-operations, the query $Q$ cannot be identified.\n\n## Estimation  {#sec-estimationprocess}\nThis section will discuss various methods to compute estimation from statistical estimands.\n\nThere are two broad types of estimation methods:\n\n**Covariate Adjustment Methods**:\n\nCovariate adjustment techniques involve utilizing the covariates or features ($X$) and the treatment ($T$) as inputs to one or more machine learning models, which are then used to fit the inputs to the potential outcome output ($Y), thereby capturing the relationship between them. The appropriate model selection, such as linear or non-linear, is contingent on the nature of the relationship between these variables. There are numerous covariate adjustment techniques commonly utilized in practice, which we will describe in detail, including:\n\n\n1. COM Estimator\n2. GCOM Estimator\n3. X-Learner\n4. TarNET\n5. Matching\n6. Doubly Robust Learners\n\n**Propensity Score Methods**:\n\nIn these methods, a propensity score is defined as the conditional probability of treatment assignment given a set of observed covariates $X$. The propensity score adjusts for differences in observed covariates between treated and control groups, creating a pseudo-population in which the covariate distribution is balanced between the two groups.\n\nSome of the techniques are:\n1. Propensity Score Matching\n2. Propensity Score Stratification\n3. Inverse Propensity Score Weighting\n\n#### Conditional Outcome Modeling Estimator (COM Estimator or S-Learner) {#sec-slearner}\nAs discussed in Chapter 2, the individualized treatment effect (ITE) is fundamentally unknowable; hence, large randomized experiments allow us to measure the average treatment effect (ATE).\nThe individualized treatment effect $\\tau_i$ for  a binary treatment is given by:\n$$ \\tau_i \\triangleq Y_i(1)- Y_i(0)$$\nThe average treatment effect $\\tau$ is given by:\n$$\\tau = \\mathbb{E}[Y_i(1)- Y_i(0)]$$\nWhen there are other covariates $x$ present (observed and/or unobserved), we estimate a more specific effect using those covariates, and it is called the conditional average effect (CATE)\n\n$$\\tau(x) \\triangleq \\mathbb{E}[Y(1)- Y(0)|X=x] $$\nFrom the identification discussion, given a sufficient adjustment set $W$ and the covariates $X$, we assume that $W \\cup X$ is also a sufficient adjustment set and satisfies the backward criterion, giving us the unconfoundedness.\n\nThe ATE is given by:\n\n$$\\tau \\triangleq \\mathbb{E}[Y(1)- Y(0)] = \\mathbb{E}_W[\\mathbb{E}[Y|T=1,W]- \\mathbb{E}[Y|T=0,W]]$$\nThus from the causal estimand $\\mathbb{E}[Y(1)- Y(0)]$ we can get statistical estimand $\\mathbb{E}_W[\\mathbb{E}[Y|T=1,W]- \\mathbb{E}[Y|T=0,W]]$.\n\nTo compute the statistical estimand, a machine learning model (for example, a regression model) can be used $\\hat{\\mu} \\approx \\mu$ to compute the conditional expectation $\\mathbb{E}[Y|T,W]$ and an empirical mean ($\\frac{1}{n}\\sum_i$) can be computed over all the data ($n$) to approximate $\\mathbb{E}_W$\n\n$$\\mu(1,W) - \\mu(0,W) = \\mathbb{E}[Y|T=1,W]- \\mathbb{E}[Y|T=0,W]$$\nThe model $\\hat{\\mu}$ is known as the conditional outcome model, and the estimator as the COM estimator or S-Learner (Single) [@kunzel2019metalearners].\n\nThus, the ATE using COM estimator is denoted by $\\hat{\\tau}$ and can be given by:\n\n$$\\tau = \\frac{1}{n}\\sum_i(\\hat{\\mu}(1,w_i) - \\hat{\\mu}(0,w_i))$$\n\nNow, CATE estimation using both the adjustment set $W$ and the observed (and unobserved) covariates $X$, using the model $\\mu$\n\n$$\\mu(t,w,x) \\triangleq \\mathbb{E}[Y|T=t,X=x,W]$$\nThus the statistical model $\\hat{\\mu}$ can be used to compute the CATE $\\tau{x}$ using the COM estimator as:\n$$\\hat{\\tau(x)} = \\frac{1}{n_x}\\sum_{i:x_i=x}(\\hat{\\mu}(1,w_i,x) - \\hat{\\mu}(0,w_i,x))$$\n\nThus, ITE (which is primarily the measure we want) can be approximated using the difference in the predictions of two models as $n_x=1$$:\n$$\\tau_i = \\hat{\\mu}(1,w_i,x) - \\hat{\\mu}(0,w_i,x)$$\n\n### Grouped Conditional Outcome Modeling Estimator (GCOM Estimator)\n\nIn most cases, the treatment $T$ is binary while the adjustment set and the covariates $W \\cup X$ are high dimensional. As seen in the equation, with binary treatment ($0,1$) and a high dimensional $w$, the model fitting the two differences $\\hat{\\mu}(1,w_i) - \\hat{\\mu}(0,w_i)$ will ignore the treatment and resulting ATE will be closer to zero.\nOne easy fix would be to compute two different models for each treatment. Grouping all the data with treatment $T=1$ and fitting a model $\\hat{\\mu}_1(w)$ to predict outcome $Y$ from $W \\cup X$ and doing the same for treatment $T=0$ by fitting a model $\\hat{\\mu}_0(w)$ and computing the average as below:\n$$\\hat{\\tau(x)} = \\frac{1}{n_x}\\sum_{i:x_i=x}(\\hat{\\mu_1}(w_i,x) - \\hat{\\mu_0}(w_i,x))$$\nSince the estimation is done by grouping based on the treatment values, the estimator is known as grouped conditional outcome model estimator (GCOM estimator). Though the GCOM estimator overcomes the dimensionality imbalance issue over the COM estimator, it has the disadvantage of not using all the data in estimation and fitting the statistical model compared to the COM estimator.\n\n### TARNet\nCOM estimators combine treatment $T$ and the input covariates $W$, making the estimator biased towards zero. The GCOM estimator builds two separate estimators for each treatment (binary $T=0$ and $T=1$), and since it does not use all the data, it leads to a higher variance model. TARNet estimators can combine the two by first learning a representation $\\hat{\\mu}$ from all the input covariates, and then the layer can branch to two heads as shown in @fig-tarnet [@shalit2017estimating].\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![TARNet estimator](img/TARNet.png){#fig-tarnet fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n\n\n\n### X-Learner {#sec-xlearner}\n\nKunzell et al. proposed a meta-learner, the X-learner, to overcome the limitations of Generalized Causal Outcome Model (GCOM) estimators. The GCOM approach falls short in its failure to utilize the complete dataset for estimating the Conditional Average Treatment Effect (CATE). In contrast, the X-learner uses all available data for both models that comprise the estimator, particularly in scenarios involving binary treatment variables, as detailed in [@kunzel2019metalearners].\n\nX-learner has the following three stages:\n\n**Step 1**\n\nAssume $X$ is a sufficient adjustment set and is also covers all the covariates, given a binary treatment, two models $\\hat{\\mu_0}(x)$ and  $\\hat{\\mu_1}(x)$ are estimated similar to GCOM estimator for each group.\n\n**Step 2(a)**\n\nIn the first part, imputed ITE is computed for treatment group $\\hat{\\tau_{1,i}}$ using the observed potential outcome $Y_i(1)$ and the imputed counterfactual that we get from the first step $\\hat{\\mu_0}(x)$. Similarly, we compute imputed ITE for the control group $\\hat{\\tau_{0,i}}$ using the observed potential outcome $Y_i(0)$ and the corresponding imputed counterfactual that we get from the first step $\\hat{\\mu_1}(x)$\n$$\\hat{\\tau_{1,i}} = Y_i(1) + \\hat{\\mu_0}(x_i)$$\n$$\\hat{\\tau_{0,i}} = Y_i(0) + \\hat{\\mu_1}(x_i)$$\nOnly individual elements from the treatment or control groups are used to compute the ITEs.\n\n**Step 2(b)**\n\nIn this step, a supervised machine learning algorithm like regression can be used to fit a model $\\hat{\\tau_1}(x)$ to predict $\\hat{\\tau_{1,i}}$ from the above step for each $x_i$ in the treatment group. Thus the model $\\hat{\\tau_1}(x)$ uses all the data from the treatment group in this step and the control group data $\\hat{\\mu_0}$ from the previous step. Similarly, a model is fit $\\hat{\\tau_0}(x)$ to predict $\\hat{\\tau_{0,i}}$ from the last step for each $x_i$ in the control group.\n\n**Step 3**\n\nThe two estimators are combined using a weighting function $0 < g(x) <1$ as given:\n\n$$\\hat{\\tau}(x) = g(x)\\hat{\\tau_0}(x) + (1-g(x))\\hat{\\tau_1}(x)$$\nThe authors found that the propensity score performs well as a weighting function.\n\n### Matching\n\nMatching is a relatively straightforward estimation technique wherein individuals from the treated and control groups are matched based on their covariates or confounders $X$, using a similarity or distance metric $d(\\cdot,\\cdot)$, as described in [@stuart2010matching].\n\n@fig-matching shows a simplified view with two dimensions $(X_1,X_2)$ how the nearest neighbor (1-NN) can be used for matching between the treated and the control group.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Matching Algorithm](img/Matching1.png){#fig-matching fig-align='center' width=50%}\n:::\n:::\n\n\n\n\nFormally, the following procedure is followed for 1-NN as below:\n\n1. Define a similarity or a distance metric $d(\\cdot,\\cdot)$.\n2. For each individual, define $\\mathcal{J}(i)$ so that we find the closest counterfactual match (treatment ($t_j \\neq t_i$)) with another individual $j \\neq i$\n$$\\mathcal{J}(i) = \\text{argmin}_{j\\ s.t\\ t_j \\neq t_i} d(x_j,x_i)$$\n3. Thus, every individual ITE can be computed using the actual and the potential counterfactual outcome obtained from $\\mathcal{J}(i)$ above.\n$$\\hat{\\tau_{1, i}} = y_i(1) - y_{\\mathcal{J}(i)}$$\n$$\\hat{\\tau_{0, i}} = y_{\\mathcal{J}(i)} - y_i(0)$$\nThese two can be combined into a single notation:\n$$\\hat{\\tau_{i}} = (2t_i -1)(y_i - y_{\\mathcal{J}(i)})$$\n\n3. Thus, ATE can be computed using the average across all the individuals\n$$\\hat{\\tau(x)} = \\frac{1}{n}\\sum_{i=1}^{n} \\hat{\\tau_{i}}$$\nConsequently, calculating the average treatment effect across the matched groups enables the causal effect estimation since the confounders are similar within these groups, and any differences are attributed solely to the treatment. This simplistic method works particularly well when the number of confounders is limited. However, as the number of dimensions or confounders increases, the method may suffer from the curse of dimensionality. Despite this drawback, the matching technique is easily interpretable by domain experts, although it heavily relies on the underlying metric of distance or similarity.\n\nNotably, it has been demonstrated that the matching algorithm employing the 1-Nearest Neighbor (1-NN) method is equivalent to the covariate adjustment method, which facilitates relating theoretical properties based on this method.\n\n\n\n### Doubly Robust Estimator\nConditional outcome modeling ($\\hat{\\mu}(x)$) and propensity score-based estimators ($\\hat{e}(x)$) can be combined to form the doubly robust estimator [@robins1994estimation].\n$$\\hat{\\tau}(x) = \\frac{1}{n}\\sum_i[\\hat{\\mu}(1,x_i)-\\hat{\\mu}(0,x_i)]$$\n$$\\hat{\\tau}(x) = \\frac{1}{n}\\sum_i[\\hat{\\mu}(1,\\hat{e}(x_i))-\\hat{\\mu}(0,(1-\\hat{e}(x_i))]$$\n\nThe doubly robust method has a property that they are consistent estimators of ATE $\\hat{\\tau}$ if either the conditional outcome modeling estimator ($\\hat{\\mu}(x)$) or the propensity score-based estimator ($\\hat{e}(x)$) are consistent. Also, the technique converges much faster to $\\tau$ than either ($\\hat{\\mu}(x)$) converging to ($\\mu(x)$) or ($\\hat{e}(x)$) converging to ($e(x)$). This technique has a distinct advantage in high-dimensional data.\n\n\n### Double Machine Learning\nDouble machine learning estimators, as the name suggests, use machine learning to learn estimators in two stages to \"partial out\" the confounders (and other covariates)  $X$ as shown in @fig-doubleml [@chernozhukov2018double].\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Double Machine Learning](img/DoubleMachineLearning.png){#fig-doubleml fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n1. Stage\n\n  1.1 Fit a machine learning model to predict $Y$ from $X$ to get $\\hat{Y}$\n\n  1.2 Fit a machine learning model to predict $T$ from $X$ to get $\\hat{T}$\n\n2. Partial out the confounding effect by fitting another model to predict $Y-\\hat{Y}$ and $T-\\hat{T}$\n\n\n### Causal Trees and Causal Forests\n\nCausal trees are similar to classification/regression trees, where leaf nodes, similar to the decision trees, are outcome variables, but the internal nodes are only limited to covariates and do not include the treatment [@wager2018estimation].\nThe general algorithm is:\n\n1. First, the observational data is divided into a train ($\\mathcal{S}^{train}$) and a test ($\\mathcal{S}^{test}$) set. The train set is used for building the tree, and the test set is used for estimation.\n\n2. A greedy algorithm creates the splits like a regular decision tree. The goal of creating partition ($\\Pi$) using the covariates is slightly different in causal trees compared to standard decision trees. The purpose of creating splits is to find the best covariate to split the node such that the treated group have a different outcome than the control group. The Kullback-Leibler Divergence is one of the techniques used to measure divergence between the outcome class distributions. If there are $i$ outcomes, $p_i$ and $q_i$ are the outcome distribution in the treated and control groups, respectively, the KL divergence $D$ between the two is given by:\n$$D(P:Q) = \\sum_i p_i\\log \\frac{p_i}{q_i}$$\nFor a covariate $X$ that splits a node into children nodes, with total $N$ instances into $N_x$ children, a conditional divergence test can be performed using KL divergence\n\n$$D(P(Y|T=1):P(Y|T=0)|X) = \\sum_x \\frac{N_x}{N}D(P(Y|T=1,x):P(Y|T=0,x))$$\nFor the best split, the objective is to maximize the gain of the divergence between the outcome class distributions between treatment and control, and can be computed using:\n\n$$D_{gain}(X) = D(P(Y|T=1):P(Y|T=0)|X) - D(P(Y|T=1):P(Y|T=0))$$\n3. Cross-validation is used to select the depth $d*$ with pruning that minimizes the MSE of treatment effects using the folds as proxies for the test set.\n\n4. Once the tree is fully constructed, the test set $$\\mathcal{S}^{test}$$ is used to estimate the treatment effects at the leaf nodes.\n\nCausal Forests are an extension of the idea of Causal trees for estimating the ATE. If we have a training set $\\{(X_i, Y_i,T_i)\\}^n_{i=1}$, a test data $x$ and a causal tree predictor given by:\n$$\\hat{\\tau}(x) = T(x;\\{(X_i, Y_i,T_i)\\}^n_{i=1})$$\nThe intention behind causal forests is that instead of one causal tree if many different trees $T^*$ are built, the average across them would be:\n\n$$\\hat{\\tau}(x) = \\frac{1}{B}\\sum_{b=1}^BT_b^*(x;\\{(X_i, Y_i,T_i)\\}^n_{i=1})$$\nMany techniques, such as bagging or subsampling the training set, can be employed to build many causal trees for the causal forest. Employing random covariates from the whole set for the splitting criterion can also result in different trees. Creating a tree from one subsample of observations and estimating the effect using other leads to a more unbiased estimate for each tree, making them ``honest'' and the average of these honest trees make it an unbiased estimate overall. Regression forests built using the honest trees were shown to have a nice theoretical property of the estimates asymptotically normal as the observations go towards infinity.\n\n### Propensity Score-Based\n\nAs previously discussed, unbiased estimation of the average treatment effect can be achieved through a randomized controlled test, wherein individuals are assigned to either the treatment or control group based on a coin flip. The propensity score technique, on the other hand, aims to re-weight the observational data such that it resembles pseudo-randomized control test data [@imbens2015causal].\n\nConsider a scenario involving binary treatment and two covariates within an observational dataset. The data points can be separated into two regions with opposing distributions. The propensity scores method involves re-weighting the samples, as depicted, to modify the distribution through weighting such that it is similar and closely approximates randomized assignment.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Observed data before and after Propensity reweighting](img/Propensity-before.png){fig-align='center' width=40% height=35%}\n:::\n\n::: {.cell-output-display}\n![Observed data before and after Propensity reweighting](img/Propensity-after.png){fig-align='center' width=40% height=35%}\n:::\n:::\n\n\n\n\n__Propensity score__ is the probability of being subjected to the treatment $(T=1)$ given the adjustment set $W$ and is denoted by:\n$$e(W) \\triangleq P(T=1|X)$$\nThe propensity score can be learned using machine learning algorithms as any other regression problem. If the samples are re-weighted using the inverse propensity score of the treatment they received, thus changing the distribution to be more random and balanced.\n\n__Propensity score theorem__\nGiven the positivity assumption, the unconfoundedness given the adjustment set $W$ implies unconfoundedness given the propensity score $e(W)$ and formally written as:\n$$(Y(1),Y(0)) \\perp T|W \\implies (Y(1),Y(0)) \\perp T| e(W) $$\nThe advantage of propensity scores and the theorem is that during conditioning, when the adjustment set $W$ is high dimensional, one can use the propensity score $e(W)$, which is a scalar.\n\n\n__Inverse Propensity Weighting (IPW) Estimator__\nGiven the data $(X_1,T_1,Y_1),\\cdots,(X_n,T_n,Y_n)$ the machine learning algorithm is first used to learn the estimator $\\hat{p}(T=t|X)$. The ATE can be estimated using:\n$$\\hat{\\tau(x)} = \\frac{1}{n_1}\\sum_{i\\ s.t. t_i=1} \\frac{y_i}{\\hat{p}(T=1|x_i)}- \\frac{1}{n_0}\\sum_{i\\ s.t. t_i=0} \\frac{y_i}{\\hat{p}(T=0|x_i)}$$\n\n### Propensity Score Matching\n\nThe Propensity Score Matching (PSM) algorithm is a methodology that emulates a Randomized Controlled Trial (RCT) in its approach to contrasting outcomes between treated and untreated cohorts within the sample that Propensity Score has matched.\n\nHowever, the implementation of PSM necessitates careful consideration of certain caveats:\n\n1. The first caveat, termed 'Common Support', necessitates that the distribution of propensity for treatment is analogous or identical across both treated and untreated cases.\n\n2. The second caveat demands the exclusive utilization of baseline attributes unaffected by the intervention during the Matching process.\n\n3. Thirdly, potential confounding variables must be both observable and without any hidden variables. A failure in this respect could result in biased estimates.\n\n4. Finally, the fourth caveat advises matching the most pertinent characteristics rather than indiscriminately incorporating every variable into the equation.\n\nThe PSM process involves several key steps, as outlined by Jalan and Ravallion [@jalan2003estimating]:\n\n1. The calculation of the Propensity Score for all units.\n2. The matching of treatment cohorts with control cohorts is performed following a predetermined matching strategy; for instance, a strategy could involve using the nearest neighbor method between the treated and control groups, implemented without replacement.\n3. The evaluation of covariate balance. In the event of an imbalance, revisiting the first and second steps and incorporating alternative specifications is advisable.\n4. The computation of the average outcome difference between the treatment and control groups.\n\n### Propensity Score Stratification\n\nKing and Nielsen propose that Propensity Score Matching (PSM) is designed to replicate a fully randomized experiment instead of a blocked randomized one. They further discuss that the exact matching procedure in PSM exacerbates issues such as imbalance, inefficiency, model dependence, and bias while also being unable to effectively mitigate the imbalance [@king2019propensity].\n\nPropensity Score (PS) stratification serves as a balancing mechanism, ensuring that the distribution of observed covariates appears comparable between treated and control groups when conditioned on the PS [@austin2011introduction]. As a result, it facilitates adjusting imbalances in the covariates by modifying the score accordingly.\n\n\n\nThe specific steps to execute for PS stratification are:\n\n1. Calculate the Propensity Score (PS) using logistic regression.\n2. Mutually exclusive strata are established based on the estimated PS.\n3. Both the treated and control units are grouped into each stratum.\n4. The difference in means between treated and control groups is calculated within each stratum.\n5. The means within each stratum are then weighted to achieve the target estimate.\n\nIn the second step of the process, studies have shown that approximately 90% of the bias inherent in the unadjusted estimate can be removed using five strata [@rosenbaum1984reducing]. However, the idea that increasing the number of strata beyond this point would lead to a further decrease in bias is not empirically supported. Indeed, simulation studies have indicated that the most favorable outcomes are achieved with between 5 and 10 strata, with different strata beyond this range contributing only minor improvements [@neuhauser2018number]. It is also essential to consider the practical implications of increasing the number of strata. As the number of strata increases, the number of data points available within each stratum decreases.\n\nDuring the fifth step of the process, Propensity Score (PS) stratification enables the calculation of both the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATT), contingent on the weighting method utilized for the means. For the estimation of the ATE, the weighting is determined by the number of units within each stratum. On the other hand, the estimation of the ATT involves assigning weights according to the count of treated units present in each stratum [@imbens2004nonparametric].\n\n\n\n\n## Evaluation and Validation Techniques\nEvaluating and validating causal models differ from traditional machine learning models, where techniques such as cross-validation and test set evaluations are performed. This section will highlight some standard metrics and methodologies to evaluate causal models.\n\n### Evaluation Metrics\nThe two broad categories for the evaluation metrics are based on whether the subpopulation is homogeneous or heterogeneous and are:\n\n1. Standard Causal Effect Estimation\n2. Heterogeneous Effect Estimation\n\n#### Standard Causal Effect Estimation\nAssuming the potential outcome to be real-valued, ff there $M$ experiments being performed, and for each experiment, the observed ATE is $\\tau$, and the predicted ATE is $\\hat{\\tau}$, then various regression-based evaluation metrics are:\n\n1. Mean Squared Error of Average Treatment Effect:\n$$\\epsilon_{MSE\\_ATE} = \\frac{1}{M}\\sum_{j=1}^{M}(\\tau_j - \\hat{\\tau}_j)^2$$\n2. Root Mean Squared Error of Average Treatment Effect:\n$$\\epsilon_{RMSE\\_ATE} = \\sqrt{\\frac{1}{M}\\sum_{j=1}^{M}(\\tau_j - \\hat{\\tau}_j)^2}$$\n3. Mean Absolute Error of Average Treatment Effect:\n$$\\epsilon_{MAE\\_ATE} = \\frac{1}{M}\\sum_{j=1}^{M}|\\tau_j - \\hat{\\tau}_j|$$\n\n\n#### Heterogeneous Effect Estimation\n\n1. Uplift Curve\n\nUplift modeling aims to identify the effect of an intervention on a particular individual rather than a population, especially in the case of heterogeneity [@pmlr-v67-gutierrez17a]. Thus, uplift modeling attempts to estimate the ITE (or CATE), i.e., the treatment outcome of a given individual and how it would differ in the absence.\n\nThe methodology to generate the uplift curve has parallels to ROC curves in standard machine learning, and the steps are:\n1. Use the machine learning method as discussed for estimation and generate CATE for each individual ($\\hat{\\mu}^1_i - \\hat{\\mu}^0_i$)\n2. Sort the uplift scores by decreasing order and compute percentiles in a range ($(0,10)(10,20)\\cdots(90,100)$)\n3. For each bucket in the percentiles, one can estimate the difference in prediction for the treatment and the control group predictions on responses.\nThe difference in average is taken for each decile.\n\n$$\\bigg(\\frac{Y^1}{N^1} - \\frac{Y^0}{N^0}\\bigg)(N^1 + N^0)$$\nwhere $Y^1$ and $N^1$ are the sum of the treated individual outcome and the number of treated individuals for the bin. Similarly, $Y^0$ and $N^0$ are the control observations' sum and number.\n\n4. The uplift curve is then plotted with the x-axis representing the percentiles of the population and the y-axis representing the uplift gain from the above corresponding to each group.\n\nThe advantage of the uplift curve is that we can select the decile that maximizes the gain as the limit of the population to be targeted next time rather than the whole population.\n\n2. Qini Curve\n\nQini curve is a variant of uplift curve where Qini score is computed instead of the uplift score as:\n\n$$Y^1 - Y^0 \\frac{N^0}{N^1}$$\nWhen there is an imbalance between the treatment and control groups, this measure offers more correction than the uplift score.\n\n3. Uplift(Qini) Coefficient\n\nSimilar to AUC-ROC curve, one can compute the area under the uplift (qini) curve, and is referred to as the uplift (qini) coefficient and is given by:\n\n$$Uplift_{Coef} = \\sum_{a=0}^{N-1}(Uplift(a+1) + Uplift(a))$$\n$$Qini_{Coef} = \\sum_{a=0}^{N-1}(Qini(a+1) + Qini(a))$$\n\n\n\n### Robustness Checks and Refutation Techniques\n\nSeveral assumptions are made in every step of causal inference, from building the causal model to estimation. Assumptions are made at the modeling level, such nonexistence of unobserved variables, the relationship between variables (edges in the graph), etc. We might make parametric assumptions for deriving the estimand at the identification step. At the estimation step, we might assume a linear relationship between treatment and observed and similarly between treatment and outcome. Many of these assumptions can be and should be tested for violations, if any. There are many assumptions that are not possible to be validated or refuted.\n\nSimilar to standard software testing, there is unit or modular testing and integration testing.\n\n#### Unit or Modular Tests\n\nDesigning tests or validations to individually check the assumptions on the model, identification, and estimation process. Some of the tests are:\n\n1. Conditional Independence Tests: Using the dependence graph and data to validate various independence assumptions, for example, with two variables ($X_1,X_2$) and their relationship with treatment $T$ if $P(T|X_1,X_2) =P(T|X_1)P(T|X_2)$\n\n2. D-Separation Tests: Conditional and marginal independence can be tested between variables in graphs using moralize, orient, delete/add edges, etc.\n\n3. Bootstrap Sample Validation: Replacing the dataset completely with bootstrapped samples from the graph helps calculate statistically significant changes in the estimand.\n\n4. Data Subsets Validation: Replacing the given dataset with a randomly selected subset helps to compute changes in the estimands and gauge the impact.\n\n\n#### Integration or Complete Tests\n\nIn integration testing, comprehensive testing on the entire process for validating many underlying assumptions rather than on single steps. Some of them are:\n\n1. Placebo Treatment Refuter: What would impact the outcome if the treatment variable is replaced by a random variable (e.g., Gaussian)? It should have no impact (zero value of estimation) if the assumptions are all correct or some steps must be corrected.\n\n2. Adding Random Common Cause: Adding an independent random variable as a common cause should keep the estimate the same. This method can be easily tested on the dataset to see the significance of the estimation change.\n\n3. Dummy Outcome Refuter: The estimated causal effect should be zero if we replace the outcome variable with an independent random variable.\n\n4. Simulated Outcome Refuter or Synth Validation: If multiple datasets are generated very close to the generation process of the existing dataset and the assumptions made, the estimation effect should remain the same. This technique is also known as the synth validation technique and is one of the most comprehensive tests for process validation.\n\n5. Adding Unobserved Confounder: One of the real-world cases if missing the observed confounder from the data or modeling. By simulating a confounder based on some correlation $\\rho$ between the outcome and the treatment, one can run the analysis and see the difference in the estimation. A significant change illustrates a robustness issue in the process.\n\n\n\n## Unconfoundedness: Assumptions, Bounds, and Sensitivity Analysis\nThroughout the discussion, we assumed unconfoundedness or observed confounding in our inference process. However, Manski et al., in their work, showed that the no unobserved confounding assumption is unrealistic in the real world [@manski2003partial].\n\nIn the simplest case, we assume an unobserved confounder $U$ along with an observed confounder $W$ as shown in @fig-observedconfounding. Thus, the ATE can be written using the adjustment formula as:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Observed Confounding](img/ObservedConfounding.png){#fig-observedconfounding fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n$$\\mathbb{E}[Y(1) -Y(0)] = \\mathbb{E}_{W,U}[\\mathbb{E}[Y|T=1,W,U] - ]\\mathbb{E}[Y|T=0,W,U]]$$\nSince $U$ is unobserved, the ATE can be approximated as\n\n$$\\mathbb{E}[Y(1) -Y(0)] \\approx \\mathbb{E}_{W}[\\mathbb{E}[Y|T=1,W] - ]\\mathbb{E}[Y|T=0,W]]$$\nThe impact of this approximation and how close the ATE in the equation and equation depends on many underlying conditions. Thus, instead of the ATE being a single value becomes an interval with bounds that depend on the assumptions.\n\nWith the simple assumption that the outcome $Y$ is bounded between $0$ and $1$, we know that the individual treatment effect is bounded between the maximum limit as below\n$$0-1  \\leq Y_i(1) -Y_i(0) \\leq 1-0$$\nThus,\n$$-1  \\leq Y_i(1) -Y_i(0) \\leq 1$$\nHence the expectations can be bounded as follows:\n$$-1  \\leq \\mathbb{E}[Y(1) -Y(0)] \\leq 1$$\nMore generally if potential outcomes are bounded between $l$ and $h$, the ATE bounds have the interval length $2(h-1)$ and given by:\n\n$$l-h  \\leq \\mathbb{E}[Y(1) -Y(0)] \\leq h-l$$\n\n### Observational Counterfactual Decomposition\n\nThe ATE can be written in terms of observational and counterfactual components, known as observational-counterfactual decomposition.\n\nThe linearity of expectations gives:\n$$\\mathbb{E}[Y(1) -Y(0)] = \\mathbb{E}[Y(1)]- \\mathbb{E}[Y(0)]$$\nConditioning and marginalization on the treatments give:\n$$\n\\begin{aligned}\n\\mathbb{E}[Y(1) -Y(0)] = P(T=1)\\mathbb{E}[Y(1)|T=1] + P(T=0)\\mathbb{E}[Y(1)|T=0]\\\\\n                                           - (P(T=1)\\mathbb{E}[Y(0)|T=1] +P(T=0)\\mathbb{E}[Y(0)|T=0])\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n\\mathbb{E}[Y(1) -Y(0)]  = P(T=1)\\mathbb{E}[Y(1)|T=1] + P(T=0)\\mathbb{E}[Y(1)|T=0]\\\\\n- P(T=1)\\mathbb{E}[Y(0)|T=1] - P(T=0)\\mathbb{E}[Y(0)|T=0]\n\\end{aligned}\n$$\n\n\n$$\\begin{aligned}\n\\mathbb{E}[Y(1) -Y(0)]  = P(T=1)\\mathbb{E}[Y|T=1] + P(T=0)\\mathbb{E}[Y(1)|T=0]\\\\\n- P(T=1)\\mathbb{E}[Y(0)|T=1] - P(T=0)\\mathbb{E}[Y|T=0]\n\\end{aligned}$$\n\n\nThus the equation has observational elements $P(T=1)\\mathbb{E}[Y|T=1], P(T=0)\\mathbb{E}[Y|T=0]$ and the counterfactual elements $P(T=0)\\mathbb{E}[Y(1)|T=0], P(T=1)\\mathbb{E}[Y(1)|T=1]$ and hence the observational-counterfactual decomposition.Now if we denote $P(T=1)$ with $\\pi$ and $P(T=0)$ becomes $1-\\pi$, the equation can we written as:\n\n$$\\mathbb{E}[Y(1) -Y(0)]  = \\pi\\mathbb{E}[Y|T=1] + (1-\\pi)\\mathbb{E}[Y(1)|T=0]\n- \\pi\\mathbb{E}[Y(0)|T=1] - (1-\\pi)\\mathbb{E}[Y|T=0]$$\n\n### Bounds\n\nWe will provide an overview of nonparametric bounds and elucidate the process of deriving them.\n\n#### No-Assumption Bounds\nThe no-assumption bounds are the simplest bound that reduces the interval $2(h-1)$ by half to $(h-1)$ as given below:\n$$\\mathbb{E}[Y(1) -Y(0)]  \\leq \\pi\\mathbb{E}[Y|T=1] + (1-\\pi)h\n- \\pi l - (1-\\pi)\\mathbb{E}[Y|T=0]$$\n\n$$\\mathbb{E}[Y(1) -Y(0)]  \\geq \\pi\\mathbb{E}[Y|T=1] + (1-\\pi)l\n- \\pi h - (1-\\pi)\\mathbb{E}[Y|T=0]$$\n\nThus, the interval length is:\n$$(1-\\pi)h\n+ \\pi h -\\pi l - (1-\\pi)l$$\n\n$$= h-1$$\nThe idea with more assumptions, as discussed next, is to get a tighter lower and upper bound than the no-bounds interval.\n\n#### Nonnegative Monotone Treatment Response Assumption\n\nAssuming that the treatment always helps, i.e., $\\forall i\\ Y_i(1) \\geq Y_i(0)$ means that ITE is always greater than $0$, and thus the lower bound changes from $l-h$ to 0. The higher bound remains what we got from the no-assumption bounds. Thus, the intervals are:\n\n$$0 \\leq \\mathbb{E}[Y(1) -Y(0)]  \\leq \\pi\\mathbb{E}[Y|T=1] + (1-\\pi)l\n- \\pi h - (1-\\pi)\\mathbb{E}[Y|T=0]$$\n\n\nBy assuming the reverse, the treatment never helps, i.e., $\\forall i\\ Y_i(1) \\leq Y_i(0)$, gives us the ITE always less than $0$, and thus the upper bound changes from $h-1$ to 0. The lower bound remains what we got from the no-assumption bounds. Thus, the intervals are:\n\n$$\\pi\\mathbb{E}[Y|T=1] + (1-\\pi)h\n- \\pi l - (1-\\pi)\\mathbb{E}[Y|T=0] \\leq \\mathbb{E}[Y(1) -Y(0)] \\leq 0$$\n\n#### Monotone Treatment Selection Assumption\n\nThe assumption is that the treatment groups' potential outcomes are better than the control groups.\nThus, we get $\\mathbb{E}[Y(1)|T=1] \\geq \\mathbb{E}[Y(1)|T=0]$ for $Y(1)$ and  $\\mathbb{E}[Y(0)|T=1] \\geq \\mathbb{E}[Y(1)|T=0]$ for $Y(0)$. It can be shown that under the monotone treatment selection assumption, the ATE is bounded by the associational difference of the observations and given by:\n$$\\mathbb{E}[Y(1)-Y(0)] \\leq \\mathbb{E}[Y|T=1] - \\mathbb{E}[Y|T=0]$$\nThus, by combining the nonnegative monotone response lower bound assumption and monotone treatment selection upper bound response, we get a tighter interval than the no-assumption bounds and is:\n\n$$0 \\leq \\mathbb{E}[Y(1)-Y(0)] \\leq \\mathbb{E}[Y|T=1] - \\mathbb{E}[Y|T=0] $$\n\n#### Optimal Treatment Selection Assumption\n\nThe assumption here is that each individual gets the treatment that is best suited, i.e. $\\forall i\\ T_i=1 \\implies Y_i(1) \\geq Y_i(0)$ and $\\forall i\\ T_i=0 \\implies Y_i(0) \\geq Y_i(1)$.\nThus, we have $\\mathbb{E}[Y(1)|T=0] \\leq \\mathbb{E}[Y|T=0]$ and $\\mathbb{E}[Y(0)|T=1] \\leq \\mathbb{E}[Y|T=1]$ which when plugged into the observational-counterfactual equation we get\n$$\\mathbb{E}[Y(1) - Y(0)] < \\pi\\mathbb{E}[Y|T=1] - \\pi l$$\n$$\\mathbb{E}[Y(1) - Y(0)] \\geq (1-\\pi)l -(1-\\pi)\\mathbb{E}[Y|T=0]$$\n\n### Sensitivity Analysis\n\nGiven the presence of observed $W$ and the unobserved $U$, the sensitivity analyses help us to quantify the unconfoundedness difference between the ATE adjusted to both $\\mathbb{E}_{W,U}$ as compared to just the observed $\\mathbb{E}_{W}$ [@cinelli2019sensitivity].\n\nConsidering a simple setting with observed variable $W$ and an unobserved variable $U$ with only linear impacts as shown in @fig-sensitivityanalysis, the structural causal model equations as a linear function will be:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Sensitivity Analysis](img/SensitivityAnalyses.png){#fig-sensitivityanalysis fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n$$T = \\alpha_w W + \\alpha_u U$$\n$$Y = \\beta_w W + \\beta_u U + \\delta T$$\nIt can be shown that when adjusted for both $W,U$, we get $\\delta$\n$$\\mathbb{E}[Y(1) -Y(0)]  = \\mathbb{E}_{W,U}[\\mathbb{E}[Y|T=1,W,U] - \\mathbb{E}[Y|T=0,W,U]] =\\delta$$\nAlso, when adjusted only for the observed $W$, we get\n$$\\mathbb{E}[Y(1) -Y(0)]  = \\mathbb{E}_{W}[\\mathbb{E}[Y|T=1,W] - \\mathbb{E}[Y|T=0,W]] = \\delta + \\frac{\\beta_u}{\\alpha_u}$$\nThus the bias, i.e., what would be the difference between when we do not adjust for the unobserved $U$ as compared to when we adjust for both $W,U$ will be the difference of the two and is $\\frac{\\beta_u}{\\alpha_u}$.\n\nA contour plot for different values of $\\beta_u$ and $\\alpha_u$ gives the sensitivity to single unobserved confounding as shown in @fig-sensitivityplot\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Sensitivity Plot](img/SensitivityPlots.png){#fig-sensitivityplot fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n\nMany researchers, such as Cinelli et al. and Vetich et al., have shown techniques to reduce the constraints (linear assumptions, single variable) and yet be able to perform sensitivity analyses similar to the simple case [@cinelli2020making,veitch2020sense].\n\n## Case Study\nWe will go through different steps and processes of causal inference to demonstrate and give a practical hands-on experience with a real-world dataset. The goal is to take various steps highlighted in the chapter using the tools. A version of the Python code used in this case study can be found in [this Colab notebook](https://colab.research.google.com/drive/1e4Ab5pAyQlKBlS3URBDe9rYIS8Z9yjd-?usp=sharing).\n\n\n### Dataset\nEconomist Jean-Jacques Lalonde collected the Lalonde dataset in the 1970s, and has been widely used in research on the evaluation of social programs. The NSWD program was designed to test the effectiveness of a job training and placement program for disadvantaged individuals. The program provided job training, job placement services, and a wage subsidy to disadvantaged individuals (the treatment group), while a control group received no treatment. The goal of the program was to determine whether the treatment had a positive effect on the employment and earnings of the participants. The dataset includes a variety of variables, including demographic characteristics (such as age, education level, and race), employment status, and income.\n\n### Tools and Library\nWe will use **doWhy**, a Python library for causal inference, for most modeling and analysis. The library includes tools for performing various causal inference tasks, such as identifying the causal effect of a treatment on an outcome variable, estimating the total effect of a treatment on an outcome variable using various interchangeable estimators and assessing the robustness of causal estimates to assumptions about the data generating process.\nIn the case study we use **causalml** and **causallift** for further distributional analysis and uplfit modeling. Python libraries such as **pandas**, **matplotlib**, **scikit-learn** etc. are used for data processing, visualization and machine learning.\n\n\n### Exploratory Data Analysis\nPlotting the treated group vs. control group with various variables (age, race, income, education) for understanding the distribution across the two as shown:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Treatment vs (Race, Age, Education)](img/Black.png){fig-align='center' width=50% height=35%}\n:::\n\n::: {.cell-output-display}\n![Treatment vs (Race, Age, Education)](img/Age.png){fig-align='center' width=50% height=35%}\n:::\n\n::: {.cell-output-display}\n![Treatment vs (Race, Age, Education)](img/Income.png){fig-align='center' width=50% height=35%}\n:::\n\n::: {.cell-output-display}\n![Treatment vs (Race, Age, Education)](img/Education.png){fig-align='center' width=50% height=35%}\n:::\n:::\n\n\n\n\n\nOne can see that the dataset is not balanced between the treated and the control group. The difference between the treated and control groups is quite evident for various variables such as education, age, and hispanic.\nThis may cause issues in many estimation processes and in the propensity-based estimation, we will highlight how the propensity-based techniques change the distribution through weights.\n\n### Estimation and Results\n\n#### Identification of Estimand\n\nAs discussed we first identify the estimand with variables __treat__ as the treatment $T$, __re78__ as the outcome $Y$ and other nominal/numeric ones such as __nodegr__, __black__, __hisp__, __age__, _educ__ and __married as the covariates $X$ as shown in the listing.\n\n```python\nfrom dowhy import CausalModel\n\nmodel = CausalModel(\n  data = lalonde_df,\n  treatment='treat',\n  outcome='re78',\n  common_causes='nodegr+black+hisp+age+educ+married'.split('+')\n)\nidentified_estimand = model.identify_effect()\n```\nThe causal graph showing the relationships between the outcome, treatment, and observed confounders is shown in @fig-causallalonde\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Causal Graph for Identification](img/CausalModel-lalonde.png){#fig-causallalonde fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n\n#### Estimation and Robustness\n\nWe have explored many linear, non-linear, propensity-based, and causal tree-based estimators to give the readers a more comprehensive view.\n\nA simple linear regression estimation is shown, and the results.\n\n```python\nlinear_regression_estimate = model.estimate_effect(\n  identified_estimand,\n  method_name=\"backdoor.linear_regression\",\n  control_value=0,\n  treatment_value=1\n)\nprint(linear_regression_estimate)\n```\n\n```\n*** Causal Estimate ***\n\n## Identified estimand\nEstimand type: EstimandType.NONPARAMETRIC_ATE\n\n### Estimand : 1\nEstimand name: backdoor\nEstimand expression:\n   d\n--------(E[re78|age,nodegr,married,educ,hisp,black])\nd[treat]\nEstimand assumption 1, Unconfoundedness: If U{treat} and Ure78 then\nP(re78|treat,age,nodegr,married,educ,hisp,black,U) =\nP(re78|treat,age,nodegr,married,educ,hisp,black)\n\n## Realized estimand\nb: re78~treat+age+nodegr+married+educ+hisp+black\nTarget units: ate\n\n## Estimate\nMean value: 1671.1304316174173\n```\n\nAs discussed in the exploratory data analysis, the data distribution was not symmetrical between the control and the treated group, so we used the inverse propensity-score weighting technique as one of the estimators.\n\n```python\ncausal_estimate_ipw = model.estimate_effect(\n  identified_estimand,\n  method_name=\"backdoor.propensity_score_weighting\",\n  target_units = \"ate\",\n  method_params={\"weighting_scheme\":\"ips_weight\"}\n)\n\nprint(causal_estimate_ipw)\n\n```\n\nThe __doWhy__ library provides interesting interpreting techniques to understand the change in distribution, as shown in the listing.\n\n```python\ncausal_estimate_ipw.interpret(\n  method_name=\"confounder_distribution_interpreter\",\n  var_type='discrete',\n  var_name='married',\n  fig_size = (10, 7),\n  font_size = 12\n)\n```\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Married distribution before and after inverse propensity weighting](img/PropensityTechnique.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\nTable for Estimator Comparison\n\n| Estimator   | ATE         |\n| ----------- | ----------- |\n| Naive      | 1794.342       |\n| Linear Regression   | 1671.13        |\n| T-Learner      | 1693.76       |\n| X-Learner   | 1763.83        |\n| T-Learner      | 1693.76       |\n| Double Machine Learner   | 1408.93        |\n| Propensity Score Matching | 1498.55 |\n| Propensity Score Stratification | 1838.36 |\n| Propensity Score and Weighting | 1639.80 |\n\n### Refutation and Validation\nNext, we highlight some refutation and validation tests performed on the model, as discussed in the chapter.\n\n#### Removing Random Subset of Data\nWe choose the causal estimate from inverse causal weighting to perform the refutation as shown:\n\n```python\nres_subset = model.refute_estimate(\n  identified_estimand,\n  causal_estimate_ipw,\n  method_name=\"data_subset_refuter\",\n  show_progress_bar=True,\n  subset_fraction=0.9\n)\n\n```\nThe difference between the two is around $17$, and since the p-value is $0.98 > 0.05$ we can safely say that the null hypothesis is valid and the refutation task had no impact on the estimation.\n```\nRefute: Use a subset of data\nEstimated effect:1639.7956658905296\nNew effect:1656.1009245901791\np value:0.98\n\n```\n\n#### Placebo Treatment\nReplacing treatment with a random (placebo) variable as shown:\n\n\n```python\nimport numpy as np\nres_placebo = model.refute_estimate(\n  identified_estimand,\n  causal_estimate_ipw,\n  method_name=\"placebo_treatment_refuter\",\n  show_progress_bar=True,\n  placebo_type=\"permute\"\n)\n```\n\nThe output\n\n```\nRefute: Use a Placebo Treatment\nEstimated effect:1639.7956658905296\nNew effect:-209.15727259572515\np value:0.78\n```\n\nThe causal estimation through inverse probability weighting can be considered robust based on the p-value.\n\n\n",
    "supporting": [
      "03-causal-estimation-process_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}