{
  "hash": "a5175a2082a79c18054502ecbd5da267",
  "result": {
    "markdown": "# Time-dependent Causal Inference\n\nStudying temporal data, focusing on causal inference, is a highly interdisciplinary endeavor that draws from statistics, machine learning, econometrics, and domain-specific fields like finance, medicine, and the social sciences. Many methods have been developed for this purpose. This chapter can help you get started.\n\n<!-- Consider discussing non-causal methods like seasonal decomposition, ARIMA/SARIMA, and Holt-Winters before moving onto the causal methods. -->\n\nWe'll delve into time-series data and explore the broader realm of temporal data, such as that found in biological networks. It's essential to address both forms of time-dependence because there are temporal processes of interest where the process cannot be decomposed into a sequence of discrete observations.\n\n## Time-series Data {#sec-timeseries}\n\nA time-series is a collection of observations obtained sequentially over time. It's characterized by its temporal order, meaning the order in which the data is collected matters. In other words, a time series $\\mathbf{X}$ is a sequence of data points $(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_T)$, where the time indices $t_1, t_2, \\ldots, T$ are such that $t_1 < t_2 < \\ldots < T$, and $\\mathbf{x}_t=(x_{t}^1, x_{t}^2, \\dots, x_{t}^d)$ is a $d$-dimensional feature vector of observations at time $t$. The single-variable case can be recovered by setting the number of features, $d$, to $1$.\n\nIn many cases, time series data points are collected at uniformly spaced intervals (e.g., hourly, daily, or monthly). However, this is not a strict requirement; some time series data may be irregularly spaced. Also, the value of a time-series at a given time point might depend on previous values in the series. Time series data can often be decomposed into components that describe how the data varies. Two such components are trends and seasonality, for example. A trend is the direction in which the data is moving over a long period of time. Seasonality is a regular fluctuation that repeats over a fixed period (e.g., hourly).\n\nSome common examples of time series include stock prices, daily temperature measurements, hourly counts of unique visitors to a website. In each of these examples, observations are made sequentially over time, and the order of the observations affects any analysis or modeling that might be done. So, how is causality typically defined for time-series data?\n\n### Causality in Time-Series\n\nThe debate surrounding the nature of causality, its connection to time, and its definition concerning time-series or other time-dependent data has a rich history. [@hoover2001causality, @kleinberg_2012, @Eichler2013CausalIW] provide comprehensive insights into both the philosophical and empirical facets of this conversation. Drawing from these resources, we highlight the following core concepts:\n\n- **Temporal precendence**: A cause precedes effect in time\n- **Physical influence**: Changes in the cause lead to changes in the effect\n- **Probability raising**: A cause increases the likelihood of an effect's occurrence\n\n## Granger Causality {#sec-granger}\n\nGranger causality is a statistical hypothesis test used to test if one time series can predict another time series [@Granger1969]. This technique says that $X$ is a \"Granger-cause\" of $Y$ if past values of $X$ reduce the variance in the prediction of $Y$ [@Granger1980, @Shojaie2021GrangerCA], that is if\n\n$$Var\\left[Y_t - P(Y_t \\vert \\mathit{H}_{< t})\\right] < Var\\left[Y_t - P(Y_t \\vert \\mathit{H}_{< t}\\setminus X_{< t})\\right],$$\n\nwhere $\\mathit{H}_{< t}$ is the history of all relevant variables before time $t$ and $\\mathit{H}_{< t}\\setminus X_{< t}$ is the history before time $t$, minus all values of $X$ before time $t$ [@Shojaie2021GrangerCA]. In practice, $\\mathit{H}_{< t}$ includes the values of all known and relevant variables in the problem up to time $t-1$.\n\nWhile Granger causality assumes that the ability to predict correctly means that there must be an underlying causal effect [@Shojaie2021GrangerCA], [@hoover2001causality] asserts that it does not establish the existence of causal relationships. The Granger causality test can only tell us that by including past values of $X$ in the predictor for $Y$, we obtain a better prediction for $Y$ than we would without $X$. It does not place any conditions on how well $X$ must predict $Y$ compared to other possible predictors for $Y$ in order to be a Granger-cause of $Y$ [@kleinberg_2012].  Despite this, Granger causes can be considered *prima facie*, or potential, causes [@Eichler2013CausalIW] that can be investigated further. As such, Granger causality can be considered as a causal discovery method [@Gong2023CausalDF].\n\nIn practice, the Granger test is most commonly performed by fitting the data using a vector autoregressive model (VAR) [@Kilian2006NEWIT] of the form\n\n$$\\mathbf{A}^0 \\mathbf{x}_t = \\sum_{k=1}^m \\mathbf{A}^k \\mathbf{x}_{t-k} + \\mathbf{e}_t,$$\n\nwhere each $\\mathbf{A}^k$ is a $d$-dimensional square matrix describing the relationship between the $d$ features  at lags $k=0, 1, \\ldots, m$, where the lag is just the number of time units the data is shifted by. For example, the $3rd$ term in the sum describes the contribution that data from time $t-3$ makes to the data at time $t$. $\\mathbf{e}_t$ is a $d$-dimensional error term [@Shojaie2021GrangerCA].\n\n### Assumptions required to use a VAR model {#sec-granger-assumptions}\n\nIn order to use a VAR model of the form shown above, several assumptions need to be made about the data [@Shojaie2021GrangerCA]:\n\n- **Continuous values**: The time-series data cannot have discrete or categorical features, only real numbers. Many real-world datasets, especially those of interest in machine learning (including one used as a case-study in this chapter), have categorical and discrete features.\n- **Linear data**: Granger causality assumes linear relationships. If the relationship between the time series is not approximatelylinear, then the results of the standard Granger test might not be valid. There are extensions of the Granger causality test to deal with nonlinear relationships, but the standard test assumes linearity.\n- **Consistent sampling rate**: sampling frequency must be constant and match the real-world time lag of any underlying causal process we hope to detect. If the sampling rate is mismatched with the time-scale of any underlying causal relationship, the Granger causality test is likely to produce misleading results.\n- **Known lags**: Granger causality tests for whether past values of one time series ($X$) predicts future values of another series ($Y$) and depends on the number of past observations (the lag) of $X$ that are used to predict the future values of $Y$. Poorly chosen lags can lead to inaccurate or misleading results.\n- **Stationarity**: A stationary time-series' statistical properties are time-invariant. The underlying process need not be static in time. For example, it can oscillate, have trends, or cyclical patterns; they just have to be consistent over time.\n- **No measurement errors or unknown variables**: Measurement errors and unknown variables can lead to endogeneity in the model, which is when a variable that affects the dependent variable and is correlated with the independent variable(s) is omitted from the model (e.g. an unobserved confounder).\n\nIf any of the above assumptions do not hold for a time-series, then using a VAR model in a Granger causality test will not be able to identify Granger causality [@Shojaie2021GrangerCA]. However, if the assumptions do hold, then Granger causality can be tested for using the following procedure:\n\n1. **Null Hypothesis**: The null hypothesis for a Granger causality test is that past values of $X$ do not reduce the variance in predicting $Y$.\n2. **Model Fitting to test null hypothesis**: Two models are fitted\n    - One using only the past values of $Y$ to predict future values of $Y$\n    - The other uses the past values of both $Y$ and $X$ to predict future values of $Y$.\n3. **Statistical Test**: An F-test is usually conducted to compare the two models. If the model trained on past values of both $Y$ and $X$ is better at predicting $Y$ than the model trained with just past $Y$ values, then we reject the null hypothesis and conclude that $X$ Granger-causes $Y$.\n\n### An example calculation {#sec-granger-example}\n\nWe will create a synthetic bike-sharing dataset to use for demonstrating calculations in this chapter. The dataset will use a temperature feature and the number of bike rentals will be the target variable. We set a random seed so that the dataset that is generated will be fixed. The dataset will be 1000 points with a mean temperature of 70 degrees (with a std deviation of 10).\n\nWe then perform a Granger causality test using the [**statsmodels**](https://www.statsmodels.org/stable/index.html) Python library.\n\n\n```python\nimport numpy as np\nimport pandas as pd\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Number of data points\nn = 1000\ntemperature = np.random.normal(70, 10, n)\nerror = np.random.normal(0, 5, n)  # Some random error\n\n# Assume that bike rentals depend on temperature with a lag\nlagged_temperature = np.roll(temperature, shift=1)\nlagged_temperature[0] = temperature[0]  # Handle the first value\n\nbike_rentals = 10 + 0.5*temperature + 0.3*lagged_temperature + error\n\ndf = pd.DataFrame({\n    'Temperature': temperature,\n    'Bike_Rentals': bike_rentals\n})\n```\n\nThe Temperature column represents daily temperatures, and the Bike Rentals column represents the number of bike rentals for the day. Let's plot the data.\n\n```python\nimport matplotlib.pyplot as plt\n\n# Plotting the dataset\nplt.figure(figsize=(14, 6))\n\nplt.plot(df['Temperature'], label='Temperature', color='blue', alpha=0.7)\nplt.plot(df['Bike_Rentals'], label='Bike Rentals', color='red', alpha=0.7)\nplt.title('Temperature and Bike Rentals over Time')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.legend()\n\nplt.tight_layout()\nplt.grid(True)\nplt.show()\n```\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Temperature (blue) and Bike Rentals (red) over Time](img/granger.png){#fig-granger fig-align='center' width=75%}\n:::\n:::\n\n\n@fig-granger visualizes the relationship between Temperature (in blue) and Bike Rentals (in red) over time. You can observe the relationship and fluctuations between the two variables. Given the synthetic data generation process, the bike rentals generally trend alongside the temperature, with some noise added to the rentals.\n\nOrdinarily, before running a Granger causality test we should check if, for the dataset being studied, the assumptions in @sec-granger-assumptions are true. Since we have constructed a synthetic dataset, we know that it meets all the criteria and the stationarity test can be omitted. When you do need to test for stationarity, two commonly used statistical tests are the Augmented Dickey-Fuller (ADF) test [@said1984testing] and the Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test [@kwiatkowski1992testing].\n\nTo perform the Granger causality test, we need to ensure the dataset is in a format suitable for time series analysis with lags.\n\n```python\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import grangercausalitytests\n\n# Define data in the format for Granger test\ndata_for_test = df[['Bike_Rentals', 'Temperature']]\n\nmax_lag = 5\ngc_test = grangercausalitytests(data_for_test, max_lag, verbose=True)\n```\n\nThe output is:\n```\nGranger Causality\nnumber of lags (no zero) 1\nssr based F test:         F=123.8638, p=0.0000  , df_denom=996, df_num=1\nssr based chi2 test:   chi2=124.2369, p=0.0000  , df=1\nlikelihood ratio test: chi2=117.0979, p=0.0000  , df=1\nparameter F test:         F=123.8638, p=0.0000  , df_denom=996, df_num=1\n\nGranger Causality\nnumber of lags (no zero) 2\nssr based F test:         F=60.4864 , p=0.0000  , df_denom=993, df_num=2\nssr based chi2 test:   chi2=121.5820, p=0.0000  , df=2\nlikelihood ratio test: chi2=114.7275, p=0.0000  , df=2\nparameter F test:         F=60.4864 , p=0.0000  , df_denom=993, df_num=2\n\nGranger Causality\nnumber of lags (no zero) 3\nssr based F test:         F=39.8454 , p=0.0000  , df_denom=990, df_num=3\nssr based chi2 test:   chi2=120.3814, p=0.0000  , df=3\nlikelihood ratio test: chi2=113.6504, p=0.0000  , df=3\nparameter F test:         F=39.8454 , p=0.0000  , df_denom=990, df_num=3\n\nGranger Causality\nnumber of lags (no zero) 4\nssr based F test:         F=29.5971 , p=0.0000  , df_denom=987, df_num=4\nssr based chi2 test:   chi2=119.4681, p=0.0000  , df=4\nlikelihood ratio test: chi2=112.8290, p=0.0000  , df=4\nparameter F test:         F=29.5971 , p=0.0000  , df_denom=987, df_num=4\n\nGranger Causality\nnumber of lags (no zero) 5\nssr based F test:         F=24.6424 , p=0.0000  , df_denom=984, df_num=5\nssr based chi2 test:   chi2=124.5895, p=0.0000  , df=5\nlikelihood ratio test: chi2=117.3848, p=0.0000  , df=5\nparameter F test:         F=24.6424 , p=0.0000  , df_denom=984, df_num=5\nThe Granger causality test for the new\n```\n\nFor all lags in the calculation (1 to 5), the p-values are essentially 0. This indicates strong evidence against the null hypothesis, which means that temperature Granger-causes (AKA has a predictive effect on) bike rentals for all tested lags. This is consistent with the synthetic data generation process, where bike rentals were influenced by both the current temperature and the temperature from the previous day (lag 1).\n\n### Violating an assumption {#sec-violating-granger}\n\nNow we will synthesize another bike rental dataset that violates the no endogeneity assumption discussed in @sec-granger-assumptions to illustrate the importance of the assumptions. We introduce a weather variable, `WeatherEffect`, that explicitly confounds the relationship between temperature and bike rentals. `WeatherEffect` will be omitted from the Granger test.\n\n```python\nimport numpy as np\nimport pandas as pd\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\nn = 1000\ntemperature = np.random.normal(70, 10, n)\nerror = np.random.normal(0, 5, n)\n\n# Generate WeatherEffect variable\nweather_effect = np.random.normal(0, 5, n)\n\n# Modify Temperature and Bike Rentals generation to account for WeatherEffect\ntemperature_with_effect = temperature + 0.2 * weather_effect\nbike_rentals_with_effect = 10 + 0.5 * temperature_with_effect + 0.3 * np.roll(temperature_with_effect, shift=1) + error + 0.5 * weather_effect\n\n# Create dataframe without WeatherEffect for Granger test\ndf_effect = pd.DataFrame({\n    'Temperature': temperature_with_effect,\n    'Bike_Rentals': bike_rentals_with_effect\n})\n```\n\n```python\nimport matplotlib.pyplot as plt\n\n# Plotting the confounded dataset\nplt.figure(figsize=(14, 6))\n\nplt.plot(df_effect['Temperature'], label='Temperature (with WeatherEffect)', color='blue', alpha=0.7)\nplt.plot(df_effect['Bike_Rentals'], label='Bike Rentals (with WeatherEffect)', color='red', alpha=0.7)\nplt.title('Temperature and Bike Rentals with Confounding WeatherEffect')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.legend()\n\nplt.tight_layout()\nplt.grid(True)\nplt.show()\n```\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Temperature (blue) and Bike Rentals (red) over Time](img/granger_confounder.png){#fig-granger-confounded fig-align='center' width=75%}\n:::\n:::\n\n\n@fig-granger-confounded visualizes the relationship between temperature (influenced by the `WeatherEffect`, shown in blue) and bike rentals (also influenced by the `WeatherEffect`, shown in red) over time.\n\nThe confounding WeatherEffect is not directly shown in the plot, but its influence is visible in the synchronized movements of temperature and bike rentals. This is a good visual representation of how omitting a relevant variable can lead to spurious relationships between other variables.\n\nNow we run the Granger causality test on this confounded data.\n\n```python\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import grangercausalitytests\n\n# Define data in the format for Granger test\ndata_for_test = df_effect[['Bike_Rentals', 'Temperature']]\n\nmax_lag = 5\ngc_test = grangercausalitytests(data_for_test, max_lag, verbose=True)\n```\n\nOutput of the Granger test is similar to the unconfounded result from @sec-granger-example:\n```\nGranger Causality\nnumber of lags (no zero) 1\nssr based F test:         F=67.3699 , p=0.0000  , df_denom=996, df_num=1\nssr based chi2 test:   chi2=67.5729 , p=0.0000  , df=1\nlikelihood ratio test: chi2=65.3856 , p=0.0000  , df=1\nparameter F test:         F=67.3699 , p=0.0000  , df_denom=996, df_num=1\n\nGranger Causality\nnumber of lags (no zero) 2\nssr based F test:         F=28.1724 , p=0.0000  , df_denom=993, df_num=2\nssr based chi2 test:   chi2=56.6286 , p=0.0000  , df=2\nlikelihood ratio test: chi2=55.0803 , p=0.0000  , df=2\nparameter F test:         F=28.1724 , p=0.0000  , df_denom=993, df_num=2\n\nGranger Causality\nnumber of lags (no zero) 3\nssr based F test:         F=18.1634 , p=0.0000  , df_denom=990, df_num=3\nssr based chi2 test:   chi2=54.8755 , p=0.0000  , df=3\nlikelihood ratio test: chi2=53.4186 , p=0.0000  , df=3\nparameter F test:         F=18.1634 , p=0.0000  , df_denom=990, df_num=3\n\nGranger Causality\nnumber of lags (no zero) 4\nssr based F test:         F=13.8168 , p=0.0000  , df_denom=987, df_num=4\nssr based chi2 test:   chi2=55.7713 , p=0.0000  , df=4\nlikelihood ratio test: chi2=54.2658 , p=0.0000  , df=4\nparameter F test:         F=13.8168 , p=0.0000  , df_denom=987, df_num=4\n\nGranger Causality\nnumber of lags (no zero) 5\nssr based F test:         F=11.1134 , p=0.0000  , df_denom=984, df_num=5\nssr based chi2 test:   chi2=56.1880 , p=0.0000  , df=5\nlikelihood ratio test: chi2=54.6588 , p=0.0000  , df=5\nparameter F test:         F=11.1134 , p=0.0000  , df_denom=984, df_num=5\n```\n\nAgain, the p-values are essentially 0 for all lags in the calculation (1 to 5). Again, this indicates strong evidence that temperature Granger-causes bike rentals for all tested lags. However, since the temperature and bike rentals are being influenced by the omitted confounding `WeatherEffect` variable, we have an endogeneity problem and cannot trust the results of the Granger causality test. This example shows that one must be careful to check that the dataset to which the Granger test is being applied does not violate the necessary assumptions because the test itself cannot distinguish between data that does or does not violate the underlying assumptions.\n\n### Extensions to Granger causality {#sec-general-granger}\n\nTo address the limitations posed by many of the assumptions discussed in @sec-granger-assumptions, many extensions of Granger causality have been devised. [@Shojaie2021GrangerCA] contains a detailed discussion of the extensions and we would encourage you to look there for details on any of these methods. This section simply provides an *incomplete* list of the extensions from [@Shojaie2021GrangerCA].\n\n- **network Granger causality**: Used to identify Granger-causes and represent them as a graph when there are many variables, such as in a gene regulation network [@Basu2013RegularizedEI, @Bernanke2003WhatET, @Fujita2007ModelingGE, @Lozano2009GroupedGG, @Nicholson2015VARXLSR]. Generally speaking, in these methods, Granger causal relationships appear as nonzero entries in the lag matrices $\\mathbf{A}^k$ and instantaneous causal effects appear as nonzero entries in the inverse covariance matrix of the error term $\\mathbf{e}_t$ [@Shojaie2021GrangerCA].\n- **Lag selection VAR models**: Estimate the optimal lag from the data. This helps with the known lag assumption [@Shojaie2010DiscoveringGG, @Shojaie2012AdaptiveTF, @Nicholson2015VARXLSR].\n- **Non-stationary VAR models**: Models to circumvent the stationarity assumption of Granger causality [@Fox2010BayesianNI, @Nakajima2013BayesianAO, @Safikhani2017JointSB, @Bai2020MultipleCP].\n- **Discrete-valued time-series**: Applications to data with values that are not continuous, e.g. categorical data [@Tank2017GrangerCN, @Hall2016InferenceOH, @Wei2018AnIT]\n- **Mismatched sampling rates**: Addresses the sampling rate not matching the timescale of underlying causal relationships [@Eichler2016GraphicalMF, @Zhou2013LearningSI]\n- **Nonlinear time-series**: Methods to go beyond the requirement of linear relationships between variables  [@Tank2018NeuralGC, @Vicente2010TransferEM, @Amblard2010OnDI]\n\n\n## Logic-based Methods {#sec-zk}\n\nThis section introduces logic-based methods of studying time series data. Logic-based methods were introduced in [@Kleinberg2009TheTL], which represented causal relationships as propositions of formulas in probabilistic temporal logic. Logic-based methods are a combination of causal discovery *and* causal inference in time-series data [@Gong2023CausalDF]. In what follows, we introduce the concepts needed to understand logic-based methods, before working through a detailed example of an advancement that was introduced in [@Zheng2017AMF].\n\n### Probabilistic Computation Tree Logic (PCTL) {#sec-PCTL}\n\nThe methods discussed in this book thus far don’t explicitly include the window of time between events with a causal relationship when computing things like average treatment effects. That makes it difficult to apply such methods to time-series problems where the time it takes for a cause to produce its effect is important and the time interval between cause and effect can impact the probability of the effect’s occurrence. This chapter focuses on methods that accounts for these temporal relationships and also describes a method of automated reasoning that represents causal relationships as propositions of formulas in probabilistic temporal logic.\n\n#### Temporal Logic\n\nTemporal logic is a formal system for expressing and reasoning about propositions that hold at different points in time. It provides a set of operators and symbols for specifying temporal relationships that lets us represent causal relationships as temporal logic formulas. These formulas can then be manipulated and evaluated to analyze patterns in temporal data, such as the timing and sequence of events, and infer causal relationships between those events. However, this does not allow for probabilistic relationships.\n\nIf we want an automated way of capturing the temporal and probabilistic relationships between events, then we need to describe temporal relationships between variables, in the presence of uncertainty. This is provided by probabilistic temporal logic (PTL).\n\n#### Probabilistic temporal logic {#sec-PTL}\n\nProbabilistic temporal logic (PTL) extends traditional temporal logic by incorporating probability. In PTL, propositions are assigned probabilities rather than being considered purely true or false. This allows for more nuanced reasoning about the temporal relationships between events. For example, if event A *typically* happens before event B but not always, we can express this uncertainty using probabilities. However, PTL alone does not allow us to study how the system evolves in time, since a probabilistic system can have many possible futures.\n\nBy combining probabilistic temporal logic with computation tree logic (CTL)  [@Clarke1996ModelC], we get probabilistic computation tree logic (PCTL) [@Hansson1990ALF], which is a probabilistic extension of CTL that is used to reason about the behavior of probabilistic systems that evolve in time. PCTL can be used to specify and verify various properties of probabilistic systems, such as the probability of reaching a certain state, the expected time to reach a certain state, and the long-term behavior of the system. PCTL can also specify temporal constraints on the behavior of a probabilistic system, answer questions about possible futures, and compute the likelihood of a possible future event happening in a specific time window.\n\n#### Probabilistic Kripke Structures {#sec-PKS}\n\nProbabilistic Computation Tree Logic describes a system's possible transitions with a probabilistic Kripke structure (PKS), which is a directed graph where each node represents a possible state of the system, and each edge (1) represents a probabilistic transition between states and (2) is labeled with a proposition that describes the conditions under which the transition can occur.\n\nBy modeling the behavior of a probabilistic system, the PKS allows us to define the truth of temporal statements with respect to the structure. For example, a temporal statement such as *\"eventually, property $P$ holds with probability at least 0.9\"* is true in a state of the structure if there is a path from that state to a future state where property $P$ is true with probability at least 0.9. PKSs are also used in the formal verification of probabilistic systems, where they provide a way to model system and verify that it satisfies a given specification.\n\n##### Describing the System\n\nA system in PCTL is described using two components: a set of atomic propositions and a probabilistic Kripke structure. In PCTL, what we know about a variable that might appear in a typical causal graph, is written as a logical proposition that can also have a probability and temporal aspects.\n\nThe set of atomic propositions is commonly written as $A$ and the PKS is written as a four-tuple $K=(S, s_i, L, T)$ [@kleinberg_2012], where\n\n* $S$ is a finite set of states\n* $s_i$ is the initial state\n* $L: S \\to 2^{|A|}$ is a labeling function that tells us which atomic propositions are true at each state\n* $T: S\\times S \\to [0,1]$ is a probabilistic transition function, where $\\forall s \\in S, \\sum_{s^{\\prime}\\in S} T(s,s^{\\prime})=1$\n\n$K$ defines which transitions are possible and the probability of each transition and $L(s)$ is the label(s) of state $s$. Note that, in practice, we will not define or infer $K$.\n\nPCTL has also been extended to support the case where discrete and continuous values are present in the same dataset [@Kleinberg2011ALF].\n\n\n##### Example: The Monty Hall Problem\n\nThis section presents a detailed example of how you would describe a system with a probabilistic Kripke structure (PKS). Our example is the famous Monty Hall problem. The Monty Hall problem is a probability puzzle based on a game show called \"Let's Make a Deal,\" which was hosted by Monty Hall. The problem is named after him and became popular after a reader's question appeared in Marilyn vos Savant's \"Ask Marilyn\" column in Parade magazine in 1990 [@WikipediaMontyHallProblem].\n\nThe problem is as follows:\n\n1. There are three closed doors: Door 1, Door 2, and Door 3.\n2. Behind one of the doors, there is a car (the desired prize), and behind the other two doors, there are goats (undesired prizes).\n3. The player chooses one of the doors (say, Door 1).\n4. The host, Monty Hall, who knows what's behind each door, opens one of the remaining doors (say, Door 2) to reveal a goat.\n5. The host then asks the player if they want to switch their choice to the other unopened door (in this case, Door 3) or stick with their original choice (Door 1).\n\nThe Monty Hall problem asks the player to determine the best strategy: should they stick with their original choice or switch to the other unopened door? We will not discuss the solution to the problem here. Instead, we describe the problem as a probabilistic Kripke structure.\n\n**Monty Hall System States**\n\nFirst, we need a set of atomic propositions, $A$, that is representative of the problem:\n\n| **Proposition**      | **Description**                             |\n|----------------------|---------------------------------------------|\n| CarBehindDoor1       | The car is behind door 1                    |\n| CarBehindDoor2       | The car is behind door 2                    |\n| CarBehindDoor3       | The car is behind door 3                    |\n| PlayerChoosesDoor1   | The player initially chooses door 1         |\n| PlayerChoosesDoor2   | The player initially chooses door 2         |\n| PlayerChoosesDoor3   | The player initially chooses door 3         |\n| PlayerSwitches       | The player switches their choice of door    |\n| HostOpensDoor1       | The host opens door 1 to reveal a goat      |\n| HostOpensDoor2       | The host opens door 2 to reveal a goat      |\n| HostOpensDoor3       | The host opens door 3 to reveal a goat      |\n| PlayerWins           | Player's final choice is door with the car   |\n| PlayerLoses          | Player's final choice is door with a goat    |\n\nNext, we need a finite set of states. In the Monty Hall problem, the state of the system can be described from the perspective of the player or the host. The host already knows which doors the car and goats are found behind. The player does not have this information at the start of the game. From the player's perspective, the state of the system is defined by a combination of the truth values of the atomic propositions in the above table. In other words, the state is what the player knows to be true and false.\n\nSince the Monty Hall problem is simple, we can describe the sequence of states in a simple table. Below, we present three sample runs of the game, in table form. The first column lists the atomic propositions, the remaining columns are labeled with the time steps showing the five steps in the game. When a column has an entry of 1, it means the proposition is true at that time step. The first column in a row with a 1 is the time step where the proposition transitions from false to true. The value of each proposition at each time step shows which transitions are possible.\n\n**Game 1**\n\nCar is behind door 1. Player chooses door 2, host opens door 3, player chooses to switch doors, player wins.\n\n| **Proposition**     | $t_1$ | $t_2$ | $t_3$ | $t_4$ | $t_5$ |\n|---------------------|-------|-------|-------|-------|-------|\n| CarBehindDoor1      |   1   |   1   |   1   |   1   |   1   |\n| CarBehindDoor2      |   0   |   0   |   0   |   0   |   0   |\n| CarBehindDoor3      |   0   |   0   |   0   |   0   |   0   |\n| PlayerChoosesDoor1  |   0   |   0   |   0   |   1   |   1   |\n| PlayerChoosesDoor2  |   0   |   1   |   1   |   0   |   0   |\n| PlayerChoosesDoor3  |   0   |   0   |   0   |   0   |   0   |\n| PlayerSwitches      |   0   |   0   |   0   |   1   |   0   |\n| HostOpensDoor1      |   0   |   0   |   0   |   0   |   0   |\n| HostOpensDoor2      |   0   |   0   |   0   |   0   |   0   |\n| HostOpensDoor3      |   0   |   0   |   1   |   0   |   0   |\n| PlayerWins          |   0   |   0   |   0   |   0   |   1   |\n| PlayerLoses         |   0   |   0   |   0   |   0   |   0   |\n\n**Game 2**\n\nCar is behind door 2. Player chooses door 2, host opens door 1, player chooses to switch doors, player loses.\n\n| **Proposition**     | $t_1$ | $t_2$ | $t_3$ | $t_4$ | $t_5$ |\n|---------------------|-------|-------|-------|-------|-------|\n| CarBehindDoor1      |   0   |   0   |   0   |   0   |   0   |\n| CarBehindDoor2      |   1   |   1   |   1   |   1   |   1   |\n| CarBehindDoor3      |   0   |   0   |   0   |   0   |   0   |\n| PlayerChoosesDoor1  |   0   |   0   |   0   |   0   |   0   |\n| PlayerChoosesDoor2  |   0   |   1   |   1   |   0   |   0   |\n| PlayerChoosesDoor3  |   0   |   0   |   0   |   1   |   1   |\n| PlayerSwitches      |   0   |   0   |   0   |   1   |   0   |\n| HostOpensDoor1      |   0   |   0   |   1   |   0   |   0   |\n| HostOpensDoor2      |   0   |   0   |   0   |   0   |   0   |\n| HostOpensDoor3      |   0   |   0   |   0   |   0   |   0   |\n| PlayerWins          |   0   |   0   |   0   |   0   |   0   |\n| PlayerLoses         |   0   |   0   |   0   |   0   |   1   |\n\n**Game 3**\n\nCar is behind door 2. Player chooses door 3, host opens door 1, player chooses to switch doors, player wins.\n\n| **Proposition**     | $t_1$ | $t_2$ | $t_3$ | $t_4$ | $t_5$ |\n|---------------------|-------|-------|-------|-------|-------|\n| CarBehindDoor1      |   0   |   0   |   0   |   0   |   0   |\n| CarBehindDoor2      |   1   |   1   |   1   |   1   |   1   |\n| CarBehindDoor3      |   0   |   0   |   0   |   0   |   0   |\n| PlayerChoosesDoor1  |   0   |   0   |   0   |   1   |   1   |\n| PlayerChoosesDoor2  |   0   |   0   |   0   |   0   |   0   |\n| PlayerChoosesDoor3  |   0   |   1   |   1   |   0   |   0   |\n| PlayerSwitches      |   0   |   0   |   0   |   1   |   0   |\n| HostOpensDoor1      |   0   |   0   |   1   |   0   |   0   |\n| HostOpensDoor2      |   0   |   0   |   0   |   0   |   0   |\n| HostOpensDoor3      |   0   |   0   |   0   |   0   |   0   |\n| PlayerWins          |   0   |   0   |   0   |   0   |   1   |\n| PlayerLoses         |   0   |   0   |   0   |   0   |   0   |\n\nThe initial state is that all doors are closed. In the next time step, $t_1$, the player chooses a door, making one of the propositions `PlayerChoosesDoor<n>` true $(n=1,2,3)$. Next, the host opens one of the other two doors that is hiding a goat, thereby setting one of the `HostOpensDoor<m>` propositions to true $(m=1,2,3)$ and one of the `CarBehindDoor<k>` propositions to false $(k=1,2,3)$. Finally, the player can switch to the remaining closed door, which will set `PlayerSwitches` to true, or they can keep their initial selection. The final state is either `PlayerWins` or `PlayerLoses`. Lastly, note that the position of the car is unknown to the player at the start of the game. If we rewrite the three example games above from the perspective of the player, then we would have to mark the `CarBehindDoor<n>` propositions as unknown. Note that we could also add propositions for the positions of the two goats, but those are complements for the position of the car. We mention this to note that the atomic propositions used for a problem are not necessarily etched in stone, but depend on how you wish to describe the problem and the states of the system at each time step.\n\n###### Properties of State and Path Formulas\n\nComputation tree logic has formulas that state what must be true for a certain state (state formulas) and formulas that define what must be true for an entire sequence of states (path formulas). For example, stating that one of the atomic proposition in $A$ must be true in a given state can be specified as a state formula. To make it more concrete, consider the Monty Hall problem.\n\nWe can summarize properties of state and path formulas as follows:\n\n1. Each proposition in $A$ is a state formula\n2. If $f$ and $g$ are state formulas, so are $\\neg f$, $f \\lor g$, $f \\land g$, and $f \\to g$.\n3. If f and g are state formulas, and $t$ is a non-negative integer or $\\infty$, then $f\\, U^{\\leq t} g$ and $f\\, W^{\\leq t} g$ are path formulas\n4. If $f$ is a path formula and $0 \\leq p\\leq 1$, $[f]_{\\geq p}$ and $[f]_{> p}$ are state formulas.\n\nThe latter three properties in the above list need to be explained:\n\nProperty (2) says that state formulas can be combined to make new state formulas using the standard propositional logic operations of *or*, *and*, and *not* and from the state transitions allowed by computation tree logic. The state is simply a collection of statements that are true at that time, which means we can use this property to automatically label states (@kleinberg_2012).\n\nThe $U$ and $W$ shown in property (3) are the *until* and *unless* operators of PCTL, respectively. $f\\,U^{\\leq t}\\, g$ is a shorthand that means $f$ must be true at every state along the path *until* there is a state where $g$ becomes true, and this must happen within $t$ time steps. $f\\, W^{\\leq t}\\, g$ is a shorthand that means $f$ must be true at every state along the path for at least $t$ time steps *unless* $g$ becomes true within $t$ time steps. For this reason, $W$ is also called the *weak until*, hence it's choice of letter.\n\nProperty (4) tell us that we can construct state formulas out of the *until* and *unless* path formulas by adding probabilities to them. For example, $[f\\, U^{\\leq t}\\, g]_{\\geq p}$, which can also be written as $f\\, U^{\\leq t}_{\\geq p}\\, g$, means that with a minimum probability of $p$, $g$ will become true within $t$ time steps and $f$ will remain true along the path until that happens. This is a state formula, whose probability is calculated by summing the probabilities of the paths that originate from this state. Note that a path probability is the product of transition probabilities along the path.\n\nBefore moving on to considering how we can identify causes, we need to introduce one more temporal operator, $\\leadsto$, \"leads to\". $f_1 \\leadsto^{\\leq  t}_{\\geq p} f_2 \\equiv AG[f_1 \\to F^{\\leq  t}_{\\geq p} f_2]$, means that for every path from the current state, states where $f_1$ is true leads to states where $f_2$ is true, and that the transitions between those states occur within $t$ time step, with probability $p$. Given this definition, consider two formulas, $a$ and $b$, that occur at times $t_1$ and $t_2$, respectively. This relationship can be described by the time interval between them, $|t_1 - t_2|$. If we want $a$ to simply occur at any time before $b$, then $|t_1 - t_2|$ can be replaced with $\\infty$ [@Kleinberg2010TheTL].\n\n### Logical Conditions for Causality\n\nThe logical framework we have been discussing, PCTL, lets us describe probabilistic relationships between events in time as formulas in temporal logic. This reframes the problem in the language of model checking [@Clarke1996ModelC], allowing us to verify which formulas are true in relation to the model. Note that the \"model\" is the collection of information described in @sec-PKS. Section 3.3.1 of [@kleinberg_2012] discusses the algorithm for checking the set of propositions against a model. We will not cover model checking in this chapter, since, in practice, we will not have a model that tells us things like the set of possible states, labels, and transition probabilities. Instead these are found from the data itself. We can start with identifying potential causes, we can then identify which potential causes are likely to be true causes.\n\n#### Identifying Potential Causes {#sec-potentialCauses}\n\nThis section describes how to find things that _might_ be causes. The literature calls these _prima facie_ causes. _Prima facie_ is Latin for \"at first sight\" or \"on the face of it.\" In the legal world, the term refers to a case that, based on the evidence presented, _appears_ to be valid. Consider two PCTL formulas, $c$ and $e$. $c$ is a potential cause of $e$ if there is a probability $p$ such that _all_ of the following conditions hold:\n\n1. $F^{\\leq  \\infty}_{> 0} c$\n2. $c \\leadsto^{\\geq  1,\\leq \\infty}_{\\geq p} e$\n3. $F^{\\leq  \\infty}_{< p} e$\n\nCondition (1) means that there is at least one state where $c$ will eventually be true with probability $p>0$, condition (2) says that $c$ being true \"leads to\" a state where $e$ is true in the future, with probability $p>0$, and condition (3) says that the probability of the system eventually transitioning from the current state to one where $e$ is true is less than $p$. The combination of these three conditions means that $c$ must be true at least once and that the conditional probability of $e$ given $c$ is higher than the marginal probability of $e$. By using these conditions, we can generate a list of potential causes that must be filtered for their level of significance.\n\n#### Causal significance\n\nIn @sec-populationMeasures, we learned about the average treatment effect as a way of quantifying the relationship between a treatment (which is a potential cause) and and outcome (an observed effect). [@Kleinberg2010TheTL] introduces a quantity called the average causal significance (ACS), $\\epsilon_{avg}$, that quantifies the causal significance of a potential cause $c$ to an effect $e$, where $c \\leadsto {}^{\\geq r, \\leq s}_{\\geq p} e$; i.e. after $c$ becomes true, $e$ occurs in a time window $[r, s]$, with probability $p$:\n\n$$\\epsilon_{avg}(c_{r-s}, e) = \\displaystyle\\frac{\\sum_{x\\in X\\setminus c } P(e\\vert c \\land x) - P(e\\vert \\neg{c} \\land x) }{\\vert X \\setminus{c} \\vert},$$\n\nwhere $X$ is the set of potential causes, $c$ is the potential cause in $X$ that is under consideration, $e$ is the effect. Note that $X \\setminus c$ means $c$ is being excluded from the set. The numerator in the ACS is the difference in the probability of the effect with and without the potential cause, and is conceptually similar to the average treatment effect. More formally, the numerator is the difference between the probability of the system moving to a state where $e$ is true from one where $c$ and $x$ are true and the probability of the system moving to a state where $e$ is true from one where $x$ is true but $c$ is not true.\n\n### Identifying Token Causes\n\n#### Types of Causes\n\nThere are two types of causal relationships: token-level and type-level [@kleinberg_2012]. Type-level relationships describe general relationships between variables, indicating a causal relationship that holds across multiple events. In contrast, token-level relationships are specific to individual events and refer to the causal relationship between a particular cause and effect pair. In this chapter, we have focused on type-level relationships.\n\n#### Defining Token-level Causal Significance\n\n[@Zheng2017AMF] shows that the ACS can be used to compute the significance of a token-level cause. The significance of $c$ at time $t^{\\prime}$ of $e$ at time $t$, where $c_{t^{\\prime}}$ is a specific instance of cause type $c$, is given by\n\n$$S(c_{t^{\\prime}}, e_t) = \\epsilon_{avg}(c_{r-s}, e) \\times P(c_{t^{\\prime}}\\vert \\mathcal{V}) \\times \\mathcal{f}(c_{t^{\\prime}}, e_t, r, s)$$\n\nwhere $\\mathcal{V}$ is the sequence of time series data, $\\epsilon_{avg}$ is the average causal significance of type-level cause $c$ on effect $e$, $P(c\\vert\\mathcal{V})$ is the probability of the token cause $c_{t^{\\prime}}$ given the sequence of observations, and $\\mathcal{f}$ is a weighting function that captures how closely the time gap between $c_{t^{\\prime}}$ and $e_t$, $t-t^{\\prime}$, fits into the type-level window $[r, s]$. There are two constraints on $\\mathcal{f}$:\n\n1. When $t^ \\in [t^{\\prime} + r, t^{\\prime} + s]$, $\\mathcal{f}(c_{t^{\\prime}} , e_t) = 1$\n2. $\\mathcal{f}$ decreases monotonically outside of the specified range range\n3. $\\mathcal{f}(c_{t^{\\prime}} , e_t) \\in [0, 1]$\n\n\n#### Computing Significance of Token Causes\n\nTo compute the significance of a token cause, we need to know $\\mathcal{f}$ and the average causal significance of the corresponding type-level cause. [@Zheng2017AMF] describes an algorithm for computing $\\mathcal{f}$ and an algorithm for finding token-level causes. They rest upon the assumption that token causes are observed in the data. Computing token significance gives us the token-level causes. The algorithm for finding token-level causes limits itself to token causes with a single variable.\n\n##### Computing $\\mathcal{f}$ from data\n\nThis section describes the algorithm from [@Zheng2017AMF] for estimating $\\mathcal{f}$ from a time-series dataset.\n\n**Algorithm 1: compute_$f(V, T, H, l_{max}, D)$**\n\n__Input:__\n\n* $V$, a set of variables\n* $T$, length of the time series\n* $H$, a set of all causal relationships between $v \\in V$\n* $l_{max}$, the maximum time lag to test\n* $D$, a $V \\times T$ matrix, stores value of each variable at each time step\n\n__Output:__\n\n* $F$, a list with a function $\\mathcal{f}$ for each relationship in $H$\n\n__Procedure:__\n\n1. for each causal relationship $h\\in H$ for which $c \\leadsto {}^{\\geq r, \\leq s}_{\\geq p} e$, do\n    1. Calculate $\\epsilon_{avg}$ for each lag $l \\in [1, lmax]$ using $D$\n    2. Normalize $\\epsilon_{avg}$ by dividing by its maximum value\n    3. Delete outliers where $\\epsilon_{avg} < 0$\n    4. $\\mathcal{f}(c_{t^{\\prime}}, e_t, r, s)=1$ when $(t - t^{\\prime}) \\in [r, s]$\n    5. $\\mathcal{f}(c_{t^{\\prime}}, e_t, r, s)$ for $(t - t^{\\prime}) \\in [1, r) \\bigcup (s, lmax]$ is fit to $\\epsilon_{avg}$ with nonlinear least squares. Then add $\\mathcal{f}$ to $F$.\n2. return $F$\n\nTwo explanatory notes:\n\n* The time lag mentioned above is the time difference between a potential cause and the effect. Recall that each causal relationship being considered has a time window $[r, s]$ where any time difference in the window is allowed. Each interval that is in the window is a time lag.\n* The matrix $D$ is a matrix representation of the time series, where each row represents a variable in $\\mathcal V$ and each column represents one time step in the time series.\n\nAlgorithm 1 gives us $\\mathcal{f}$ and $\\epsilon_{avg}$, which are two of the three things we need to compute the token significance, $S(c_{t^{\\prime}}, e_t)$. The remaining quantity is $P(c\\vert\\mathcal{V})$, the probability of token cause $c_{t^{\\prime}}$, given $\\mathcal{V}$. That can be counted from the dataset.\n\n#### Finding Token-level Causal Explanations\n\nThis section describes a method for identifying and removing known causes from the set of events to be explained. Then, the remaining events are tested to see if any variables can explain them. The set of events that cannot be explained by the known causes is the residual set, and the causes of these events are to be found without removing any potential causes of other events. Variables that are not in their initial state or have recently changed state are considered as possible causes.\n\n\n**Algorithm 2: `find-novel-causes`$(V, T, D, k, m, lmax, E)$**\n\n__Input:__\n\n* $V$, a set of variables\n* $T$, length of the time series\n* $D$, a $V \\times T$ matrix, stores value of each variable at each time step\n* $k$, number of explanations to select (for top-$k$)\n* $m$, max time lag for detecting state change\n* $l_{max}$, the maximum time lag for finding novel explanation\n* $E$, set of explained events (pairs of form: $(c_{t^{\\prime}} , e_{t^{\\prime}})$)\n\n__Output:__\n\n* $E'$, token causal explanations for each event\n\n__Procedure:__\n\n1. for each $e \\in V$ do\n    1. Use $E$ to remove explained events to get matrix `De`\n    2. for each $c \\in V$ do\n        1. for each $l \\in [1, l_{max}]$ do\n            1. Get $P (e\\vert c_l)$. When $c$ has a default state, use instances of $c$ where $c$ is not in its default state or changed state up to $m$ time units before.\n        2. for each non-null $D_e [e, i]$, $1 \\leq i \\leq T$ do\n            1. $L_i \\gets [ ]$\n            2. for each $D_e [c, j]$, where $i - l_{max} \\leq j \\leq i - 1$ do\n            - Add $(c_j , e_i, P (e\\vert c_j-i))$ to $L_i$\n        3. Add top $k$ explanations for event $e_i$ to $E'$ (tuples: $(c_j , e_i, P (e\\vert c))$)\n2. return $E'$\n\nThe output is a list of token causes, the events they explain, and their conditional probability.\n\n<!---## General Temporal Data {#sec-temporal}--->\n\n## Case Study\n\nIn this section, we walk through a detailed example of how to explain observed events in time-series data using type-level causal relationships (Algorithm 1). We also look at how to explain observed events that are *not* explained by type-level  causes and instead explain them with token-level causal relationships (Algorithm 2). This case study can also be [viewed on Google Colab](https://colab.research.google.com/drive/1Lsa5DYGvxuiHfuVILNYggMWv3HHsMhCZ).\n\n### Dataset\n\nWe use the [bike sharing dataset](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) that is in the UC Irvine Machine Learning Repository [@FanaeeT2013EventLC]. This is one of the datasets the algorithms above were applied to in [@Zheng2017AMF].\n\nThe dataset contains information on the hourly and daily count of rental bikes in the Capital bikeshare system between 2011 and 2012, including weather and seasonal data. It may provide insights into bike usage patterns and their correlation with environmental factors. We're interested in identifying causes for high bike rental numbers, using the hourly data.\n\n### Variables\n\nThe Bike Sharing dataset has several columns that we will use as causal variables, and three as possible causal effects. Each type of column is listed below. For more information about this dataset, visit the link above.\n\n**Effect variables**\n\n- casual: count of casual users\n- registered: count of registered users\n- cnt: count of (casual + registered) users\n\n**Causal variables**\n\n- season: `{1:\"winter\", 2:\"spring\", 3:\"summer\", 4:\"fall\"}`\n- holiday: 1 if day is holiday else 0\n- weekday: day of the week (1-7)\n- workingday: 1 if neither weekend nor holiday else 0\n- weathersit:\n    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n- temp: Normalized temperature in Celsius\n- atemp: Normalized feeling temperature in Celsius\n- hum: Normalized humidity\n- windspeed: Normalized wind speed\n\n\n#### Loading the data\n\nWe will convert the following column names as follows:\n\n- dteday $\\rightarrow$ date\n- yr $\\rightarrow$ year\n- mnth $\\rightarrow$ month\n- hr $\\rightarrow$ hour\n- holiday $\\rightarrow$ is_holiday\n- weekday $\\rightarrow$ day_of_week\n- workingday $\\rightarrow$ is_workingday\n- weathersit $\\rightarrow$ weather\n- hum $\\rightarrow$ humidity\n- causal $\\rightarrow$ num_casual_users\n- registered $\\rightarrow$ num_registered_users\n- cnt $\\rightarrow$ num_total_users\n\nAlso, we'll convert the values in the `season` column into categorical values using this map: `{1:\"winter\", 2:\"spring\", 3:\"summer\", 4:\"fall\"}`. The values in the `year` column can be converted using this map: `{0: 2011, 1:2012}`.\n\n```python\nimport pandas as pd\n\ndef load_data(filename):\n    df = pd.read_csv(filename)\n    column_mapping = {\n        'dteday': 'date',\n        'yr': 'year',\n        'mnth': 'month',\n        'hr': 'hour',\n        'holiday': 'is_holiday',\n        'weekday': 'day_of_week',\n        'workingday': 'is_workingday',\n        'weathersit': 'weather',\n        'hum': 'humidity',\n        'casual': 'num_casual_users',\n        'registered': 'num_registered_users',\n        'cnt': 'num_total_users',\n    }\n\n    df.rename(columns=column_mapping, inplace=True)\n    df['season'] = df['season'].map({\n        1: 'winter',\n        2: 'spring',\n        3: 'summer',\n        4: 'fall',\n    }).astype('category')\n    df['year'] = df['year'].map({0: 2011, 1: 2012})\n\n    df['timestamp'] = pd.to_datetime(\n        df['date'] + ' ' + df['hour'].astype(str) + ':00:00'\n    )\n    df.drop(['date', 'year'], axis=1, inplace=True)\n    return df\n```\n\nWe define binary-valued columns in a dataframe for each of the following causal variable:\n\n- is_holiday\n- is_workingday\n- is_weekend\n- is_raining\n- is_bad_weather\n- is_mild_precipitation\n- is_daytime\n- is_nighttime\n- is_`<season>` (spring, summer, fall, winter)\n- is_high_temp\n- is_high_atemp\n- is_low_temp\n- is_low_atemp\n- is_high_humidity\n- is_low_humidity\n- is_windy\n- not_windy\n\nFor non-binary valued variables, we follow the process used in [@Zheng2017AMF] and divide the continuous variables into three bins of equal size.\n\n```python\ndef convert_to_bins(df, column):\n    return pd.qcut(\n        df[column],\n        3,\n        labels=['low', 'medium', 'high'],\n    )\n\n# convert the temp column into three bins\ndf_hour['temp_bin'] = convert_to_bins(df_hour, 'temp')\n\n# convert the atemp column into three bins\ndf_hour['atemp_bin'] = convert_to_bins(df_hour, 'atemp')\n\n# convert the humidity column into three bins\ndf_hour['humidity_bin'] = convert_to_bins(df_hour, 'humidity')\n\n# convert the windspeed column into three bins\ndf_hour['windspeed_bin'] = convert_to_bins(df_hour, 'windspeed')\n```\n\nBinary columns are defined as follows:\n\n```python\ndef is_holiday(df, t):\n    return int(df.loc[t, 'is_holiday'] == 1)\n\ndef is_workingday(df, t):\n    return int(df.loc[t, 'is_workingday'] == 1)\n\ndef is_weekend(df, t):\n    return int(df.loc[t, 'day_of_week'] in [0, 6])\n\ndef is_bad_weather(df, t):\n    return int(df.loc[t, 'weather'] in [3, 4])\n\ndef is_mild_precipitation(df, t):\n    return int(df.loc[t, 'weather'] == 3)\n\ndef is_rush_hour(df, t):\n    _tush = [7, 8, 9, 17, 18, 19]\n    return int(df.loc[t, 'hour'] in _rush)\n\ndef is_daytime(df, t):\n    _daytime = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n    return int(df.loc[t, 'hour'] in _daytime)\n\ndef is_nighttime(df, t):\n    _nighttime = [0, 1, 2, 3, 4, 5, 20, 21, 22, 23]\n    return int(df.loc[t, 'hour'] in _nighttime)\n\ndef is_spring(df, t):\n    return int(df.loc[t, 'season'] == 'spring')\n\ndef is_summer(df, t):\n    return int(df.loc[t, 'season'] == 'summer')\n\ndef is_fall(df, t):\n    return int(df.loc[t, 'season'] == 'fall')\n\ndef is_winter(df, t):\n    return int(df.loc[t, 'season'] == 'winter')\n\ndef is_high_temp(df, t):\n    return int(df.loc[t, 'temp_bin'] == 'high')\n\ndef is_low_temp(df, t):\n    return int(df.loc[t, 'temp_bin'] == 'low')\n\ndef is_high_atemp(df, t):\n    return int(df.loc[t, 'atemp_bin'] == 'high')\n\ndef is_low_atemp(df, t):\n    return int(df.loc[t, 'atemp_bin'] == 'low')\n\ndef is_high_humidity(df, t):\n    return int(df.loc[t, 'humidity_bin'] == 'high')\n\ndef is_low_humidity(df, t):\n    return int(df.loc[t, 'humidity_bin'] == 'low')\n\ndef is_windy(df, t):\n    return int(df.loc[t, 'windspeed_bin'] == 'high')\n\ndef not_windy(df, t):\n    return int(df.loc[t, 'windspeed_bin'] == 'low')\n```\n\nWe group these to make it easy to add columns to the dataframe:\n\n```python\nlabelers = {\n    'is_holiday': is_holiday,\n    'is_workingday': is_workingday,\n    'is_weekend': is_weekend,\n    'is_bad_weather': is_bad_weather,\n    'is_mild_precipitation': is_mild_precipitation,\n    'is_rush_hour': is_rush_hour,\n    'is_daytime': is_daytime,\n    'is_nighttime': is_nighttime,\n    'is_spring': is_spring,\n    'is_summer': is_summer,\n    'is_fall': is_fall,\n    'is_winter': is_winter,\n    'is_high_temp': is_high_temp,\n    'is_low_temp': is_low_temp,\n    'is_high_atemp': is_high_atemp,\n    'is_low_atemp': is_low_atemp,\n    'is_high_humidity': is_high_humidity,\n    'is_low_humidity': is_low_humidity,\n    'is_windy': is_windy,\n    'not_windy': not_windy\n}\n\nidx2labeler = {}\nlabel_fns = []\nfor i, feature in enumerate(labelers.keys()):\n    idx2labeler[i] = feature\n    label_fns.append(labelers[key])\n\n```\n\nLastly, we also make the continuous effect variables categorical:\n\n```python\ndef binning(x, mean, std):\n    \"\"\"\n    Maps x to a string, based on the mean and standard deviation:\n    low: 1 std dev below the mean\n    high: 2 std dev above the mean\n    medium: everything else\n    \"\"\"\n    if x < mean - std:\n        return \"low\"\n    elif x > mean + 2*std:\n        return \"high\"\n    else:\n        return \"medium\"\n\ndef label_by_std(df, col_name):\n    \"\"\"\n    Labels column entries based on categorical standard deviation\n    \"\"\"\n    std = df[col_name].std()\n    mean = df[col_name].mean()\n    is_low = lambda x: x < mean - std\n    is_high = lambda x: x > mean + 2*std\n    df[f'{col_name}_bin'] = df[col_name].apply(\n        lambda x: binning(x, mean, std)\n    )\n    return df\n\ndf_hour = label_by_std(df_hour, 'num_total_users')\ndf_hour = label_by_std(df_hour, 'num_casual_users')\ndf_hour = label_by_std(df_hour, 'num_registered_users')\n```\n\n```python\ndef high_total_users(df, t):\n    return int(df.loc[t, 'num_total_users_bin'] == 'high')\n\ndef low_total_users(df, t):\n    return int(df.loc[t, 'num_total_users_bin'] == 'low')\n\ndef high_casual_users(df, t):\n    return int(df.loc[t, 'num_casual_users_bin'] == 'high')\n\ndef low_casual_users(df, t):\n    return int(df.loc[t, 'num_casual_users_bin'] == 'low')\n\ndef high_registered_users(df, t):\n    return int(df.loc[t, 'num_registered_users_bin'] == 'high')\n\ndef low_registered_users(df, t):\n    return int(df.loc[t, 'num_registered_users_bin'] == 'low')\n```\n\n### Tools and Library\n\nWe use the Pandas and Numpy libraries.\n\n### Procedure\n\n\n#### Using type-level relationships to explain observed events\n\nWe use part of Algorithm 1 to identify significant type-level relationships from the time-series dataset that explain why bike rentals are high/low.\n\nHere are the steps in the process:\n\n1. Identify the type-level relationships in the dataset\n2. Compute causal significance of the type-level relationships, $\\epsilon_{avg}(c_{r-s},e)$\n\nWe aim to evaluate the significance of each type-level cause for each instance of high bike rentals. For each instance of high bike rentals, we use a significance threshold or only keep the top $k$ relationships to partition the relationships/events into those for which sufficiently significant type-level causes have been found and those for which sufficiently significant type-level causes have *not* been found. Any instances of high rentals that are not explained by type-level causes would be used as input for Algorithm 2.\n\n##### Identify token-level relationships\n\nWe find type-level relationships by aggregating token-level events. We will pass through the dataset, and at each point, we will use the values of the various variables to identify *prima facie* causes. Recall that a *prima facie* cause is a type-level relationship of the form $c \\leadsto {}^{\\geq r, \\leq s}_{\\geq p} e$\n\n```python\nimport numpy as np\n\nV = len(labelers)  # number of variables\nT = df_day.shape[0]  # number of time steps\nD = np.zeros((T, V), dtype=np.int32)  # time-series\n\nfor t in range(T):\n    for i in range(V):\n        D[t, i] = label_fns[i](df_day, t)\n\nvariable_names = idx2labeler.values()\ndf_D = pd.DataFrame(D, columns=variable_names)\n\n# add effect columns to the dataframe\n# this makes it easy to filter to compute probabilities\ndef add_column(df, f):\n    df[f.__name__] = [f(df_hour, t) for t in range(T)]\n    return df\n\ndf_D = add_column(df_D, high_total_users)\ndf_D = add_column(df_D, low_total_users)\ndf_D = add_column(df_D, high_casual_users)\ndf_D = add_column(df_D, low_casual_users)\ndf_D = add_column(df_D, high_registered_users)\ndf_D = add_column(df_D, low_registered_users)\ndf_D.head()\n```\n\nHere we introduce several data structures that we will use to work with type-level and token-level relationships.\nFirst, we have two class that let us store the names of causal variables, so that we don't have to keep passing around strings.\n\n```python\nfrom typing import List\n\nVariableIndex = int\n\n\nclass BidirectionalDict(dict):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._backward = {v: k for k, v in self.items()}\n\n    def __setitem__(self, key, value):\n        super().__setitem__(key, value)\n        self._backward[value] = key\n\n    def forward_lookup(self, key):\n        return self[key]\n\n    def backward_lookup(self, value):\n        return self._backward[value]\n\n    def keys(self):\n        return list(super().keys())\n\n    def values(self):\n        return list(self._backward.keys())\n\n\nclass VariableStore:\n    def __init__(self):\n        self.storage = BidirectionalDict()\n\n    def add(self, variable_name: str):\n        if variable_name not in self.storage:\n            self.storage[variable_name] = len(self.storage)\n\n    def lookup_by_name(self, name: str) -> VariableIndex:\n        return self.storage.forward_lookup(name)\n\n    def lookup_by_index(self, index: VariableIndex) -> str:\n        return self.storage.backward_lookup(index)\n\n    def __len__(self) -> int:\n        return len(self.storage)\n\n    def __contains__(self, name) -> bool:\n        return name in self.storage\n\n    @property\n    def names(self) -> List[str]:\n        return sorted(self.storage.keys())\n\n    @property\n    def ids(self) -> List[VariableIndex]:\n        return sorted(self.storage.values())\n```\n\nNext, we have data structures for time window, type-level and token-level causal relationships, and significance scores.\n\n```python\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Set, Union\n\nclass Window:\n    \"\"\"\n    A window of time, of the closed interval [start, end]\n    \"\"\"\n\n    def __init__(self, start: int, end: int):\n        if start > end:\n            raise ValueError(\"Window start must be <= than end\")\n        if start < 0 or end < 0:\n            raise ValueError(\"Window start and end must be >= 0\")\n        self.start = start\n        self.end = end\n\n    def __repr__(self):\n        return f\"Window({self.start}, {self.end})\"\n\n    def __eq__(self, _value: object) -> bool:\n        if isinstance(_value, Window):\n            return (\n                self.start == _value.start\n                and self.end == _value.end\n            )\n        return False\n\n    def __hash__(self) -> int:\n        return hash((self.start, self.end))\n\n\n@dataclass(frozen=True)\nclass CausalRelation:\n    \"\"\"\n    A cause and effect pair\n    \"\"\"\n\n    cause: VariableIndex\n    effect: VariableIndex\n\n\n@dataclass(frozen=True)\nclass TokenCause:\n    \"\"\"\n    A token event that supports a potential cause\n    \"\"\"\n\n    relation: CausalRelation\n    t_cause: int  # time step where cause is true\n    t_effect: int  # time step where effect is true\n\n    @property\n    def lag(self) -> int:\n        \"\"\"The lag between the cause and effect\"\"\"\n        return self.t_effect - self.t_cause\n\n\nclass TypeLevelCause:\n    r\"\"\"\n    A potential cause of an effect, with a time window, a\n    probability of occurrence, and a list of token events\n    that support this type-level cause. In PCTL language,\n    `c \\leadsto {}^{\\geq r, \\leq s}_{\\geq p} e`\n    \"\"\"\n\n    def __init__(\n        self,\n        relation: CausalRelation,\n        window: Window,\n        prob: float,\n        token_events: List[TokenCause],\n    ):\n        \"\"\"\n        Create a type-level cause\n\n        Parameters\n        ----------\n        relation : CausalRelation, the cause and effect\n        window : Window, the time window in which the effect\n                 occurs after the cause\n        prob : float, the probability of the effect occurring\n               in the window\n        token_events : List[TokenCause], the token events that\n                       support this type-level cause\n        \"\"\"\n        self.relation = relation\n        self.window = window\n        self.prob = prob\n        self.token_events = token_events\n\n    def __repr__(self):\n        return f\"TypeLevelCause({self.relation}, {self.window}, \\\n            {self.prob})\"\n\n    # define equality and hashing based on the relation, window,\n    # prob, and token events\n    def __eq__(self, other):\n        if isinstance(other, TypeLevelCause):\n            return (\n                self.relation == other.relation\n                and self.window == other.window\n                and self.prob == other.prob\n                and self.token_events == other.token_events\n            )\n        return False\n\n    def __hash__(self):\n        return hash((\n            self.relation,\n            self.window,\n            self.prob,\n            tuple(self.token_events),\n        ))\n\n\n@dataclass(frozen=True)\nclass CompositeScore:\n    lag_scores: np.array\n\n    @property\n    def score(self):\n        return np.sum(self.lag_scores)\n```\n\nWe also added a class that identifies the causal relationships and computes their relative significance. This makes it easy to compute things with a few simple method calls. The class, `CausalTree`, is many lines of code and so it can be found in a file `cause.py` in the same directory as the Jupyter notebook. Now we can identify the type-level relationships and find the most significant ones.\n\n```python\ntree = CausalTree(df_D, list(variable_names), \"high_total_users\")\ntree.build(\"high_total_users\", max_lag=3)\ntree.compute_significance()\ntree.prune()\n\n@dataclass\nclass ScoredTypeLevelCause:\n    cause: TypeLevelCause\n    score: float\n\n    def __str__(self):\n        return f\"{self.cause}: {self.score:.2f}\"\n\n    def __repr__(self):\n        return str(self)\n\nscored_causes = []\nfor i, cause in enumerate(tree.type_level_causes):\n    composite_score = tree.type_level_significance_scores[i]\n    scored_causes.append(\n        ScoredTypeLevelCause(cause, composite_score.score)\n    )\n\nscored_causes.sort(key=lambda x: x.score, reverse=True)\n\nfor c in scored_causes[:10]:\n    cause = tree.get_variable_name(c.cause.relation.cause)\n    effect = tree.get_variable_name(c.cause.relation.effect)\n    print(f\"{cause} => {effect}: {c.score:.2f}, \\\n        {c.cause.prob:.2f} p-value, {c.cause.window} \")\n```\n### Discussion\n\nThe result of the previous command shows which type-level causes contributed the most to high bike rentals, and in which time window.\n\n\n\n",
    "supporting": [
      "08-time-dependent-causal-inference_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}