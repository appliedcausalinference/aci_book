<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="This is a book which covers applications of causality, ranging from a practical overview of causal inference to cutting-edge applications of causality in machine learning domains.">

<title>Applied Causal Inference - 6&nbsp; Computer Vision</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./08-time-dependent-causal-inference.html" rel="next">
<link href="./06-nlp.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./07-computer-vision.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Computer Vision</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Applied Causal Inference</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro-to-causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to Causality</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-potential-outcomes-framework.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Causal Inference: Theory and Basic Concepts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-causal-estimation-process.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Causal Inference: A Practical Approach</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-causal-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Causal Discovery</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-part-2-break.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part 2: Causality in ML Domains</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">NLP</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-computer-vision.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Computer Vision</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-time-dependent-causal-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Time-dependent Causal Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-part-3-break.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part 3: Advanced Topics in Causality</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Fairness</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-reinforcement-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-current-state-of-computer-vision" id="toc-the-current-state-of-computer-vision" class="nav-link active" data-scroll-target="#the-current-state-of-computer-vision"><span class="header-section-number">6.1</span> The Current State of Computer Vision</a></li>
  <li><a href="#causal-methods-in-computer-vision" id="toc-causal-methods-in-computer-vision" class="nav-link" data-scroll-target="#causal-methods-in-computer-vision"><span class="header-section-number">6.2</span> Causal Methods in Computer Vision</a>
  <ul class="collapse">
  <li><a href="#image-classification" id="toc-image-classification" class="nav-link" data-scroll-target="#image-classification"><span class="header-section-number">6.2.1</span> Image Classification</a></li>
  <li><a href="#few-shot-learning" id="toc-few-shot-learning" class="nav-link" data-scroll-target="#few-shot-learning"><span class="header-section-number">6.2.2</span> Few-Shot Learning</a></li>
  <li><a href="#weakly-supervised-semantic-segmentation" id="toc-weakly-supervised-semantic-segmentation" class="nav-link" data-scroll-target="#weakly-supervised-semantic-segmentation"><span class="header-section-number">6.2.3</span> Weakly Supervised Semantic Segmentation</a></li>
  <li><a href="#vision-language-tasks" id="toc-vision-language-tasks" class="nav-link" data-scroll-target="#vision-language-tasks"><span class="header-section-number">6.2.4</span> Vision-Language Tasks</a></li>
  <li><a href="#sec-cv-robustness" id="toc-sec-cv-robustness" class="nav-link" data-scroll-target="#sec-cv-robustness"><span class="header-section-number">6.2.5</span> Robustness and Transfer</a></li>
  </ul></li>
  <li><a href="#case-study" id="toc-case-study" class="nav-link" data-scroll-target="#case-study"><span class="header-section-number">6.3</span> Case Study</a></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions"><span class="header-section-number">6.4</span> Conclusions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Computer Vision</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>This chapter focuses on applications combining causal inference and computer vision. Computer vision (CV) refers to a domain of machine learning which involves the use of images or videos as inputs. CV has gained significant adoption over the last several years, with applications appearing within industries including technology, medicine, manufacturing, and defense. In recent years, researchers have developed some exciting methods using causal inference techniques to overcome common problems within CV – in this chapter, we will give a brief overview of common tasks and methodologies in CV, and detail the various ways that causality is being used.</p>
<section id="prerequisite-knowledge" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="prerequisite-knowledge">Prerequisite Knowledge</h4>
<p>In addition to the causal inference concepts introduced in the first part of this book, relatively little background knowledge is necessary for this chapter. Readers with more familiarity or experience with computer vision will likely benefit more from this chapter, but we will introduce relevant concepts, tasks, challenges, and methods as needed.</p>
<p>Readers who wish to learn more about computer vision more holistically are encouraged to explore the following resources:</p>
<ul>
<li>The <em>Fast AI</em> book <span class="citation" data-cites="howard2020deep">(<a href="#ref-howard2020deep" role="doc-biblioref">Howard and Gugger 2020</a>)</span> has two chapters on computer vision<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
<li>The Coursera course on Convolutional Neural Nets and Computer Vision from Andrew Ng’s Deep Learning specialization<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></li>
</ul>
</section>
<section id="the-current-state-of-computer-vision" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="the-current-state-of-computer-vision"><span class="header-section-number">6.1</span> The Current State of Computer Vision</h2>
<p>Computer vision has come a long way since Yann LeCun introduced the convolutional neural network (CNN) for handwritten digit recognition <span class="citation" data-cites="726791">(<a href="#ref-726791" role="doc-biblioref">Lecun et al. 1998</a>)</span>. Starting in around 2012 – when AlexNet <span class="citation" data-cites="NIPS2012_c399862d">(<a href="#ref-NIPS2012_c399862d" role="doc-biblioref">Krizhevsky, Sutskever, and Hinton 2012</a>)</span> shattered existing state-of-the-art on the ImageNet task using a deep CNN – computer vision began to take off, with its trajectory aided by an increase in the availability of computing power coinciding with the “big data” boom. Since then, previously difficult benchmarks have become too easy to use, and boundaries continue to be pushed.</p>
<p>Today, computer vision comprises a wide variety of tasks, ranging from image classification and object detection to image generation and question answering based on input image. These models are used in applications such as medical image segmentation <span class="citation" data-cites="Milletar2016VNetFC">(<a href="#ref-Milletar2016VNetFC" role="doc-biblioref">Milletarì, Navab, and Ahmadi 2016</a>)</span>, quality control in factories <span class="citation" data-cites="Zhou2021ComputerVT">(<a href="#ref-Zhou2021ComputerVT" role="doc-biblioref">Zhou, Zhang, and Konz 2021</a>)</span>, and identification of pedestrians, structures, and other vehicles in self-driving cars <span class="citation" data-cites="https://doi.org/10.48550/arxiv.2207.12939">(<a href="#ref-https://doi.org/10.48550/arxiv.2207.12939" role="doc-biblioref">Cakir et al. 2022</a>)</span>. Deep learning dominates modern CV, and model design depends largely upon the application; however, many commonalities exist: convolutional networks are still used in many settings, while sequence models such as Transformers have also gained popularity in many tasks due to their success in natural language processing.</p>
<p>There are a handful of common issues that occur within CV projects, and they are similar in nature to problems encountered more broadly throughout machine learning. These issues include a lack of robustness when transferring to slightly different datasets, which is often due to the tendency of greedy, data-hungry ML models to latch onto spurious correlations — for instance, random patterns in the background or textures in an image. The issue of spurious correlations is exacerbated by the fact that deep learning techniques are inherently uninterpretable. Techniques exist to “peek into the black box,” such as highlighting a salient region using gradient flow through a network; however, these models do not provide reliable methods of interpreting their output or decisions, making it difficult to truly understand how a model learns a particular pattern or identify when it happens.</p>
</section>
<section id="causal-methods-in-computer-vision" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="causal-methods-in-computer-vision"><span class="header-section-number">6.2</span> Causal Methods in Computer Vision</h2>
<p>As we have seen throughout this book thus far, handling spurious correlation is one of the major features of causal inference, making CV a compelling area of application for causal methods. In fact, a recent paper in the journal <em>Nature</em> describes the importance of causal reasoning within computer vision in clinical healthcare settings <span class="citation" data-cites="Castro_2020">(<a href="#ref-Castro_2020" role="doc-biblioref">Castro, Walker, and Glocker 2020</a>)</span>. In this paper, the authors primarily focus on the necessity of understanding the causal direction of the data generating process, describing the biases which can appear when observations and annotations are sampled in ways that do not match the real-world setting in which the model will be deployed. These mismatches can be subtle, but the corresponding sampling bias can have a catastrophic impact on a deployed model’s performance. Their work shows that causal language can prevent issues like these from creeping into clinical computer vision projects. As we will see in this chapter, causal inference methods can aid CV practitioners in other ways as well: from improving model performance to making models more robust in out-of-domain settings.</p>
<p>In the sections that follow, we will give an overview of the current state of causal methods within various subdomains of computer vision, including object recognition and segmentation, few-shot learning, vision-language tasks, and improving robustness. After introducing these areas of application in causal computer vision, we will provide a case study detailing one of these methods.</p>
<section id="image-classification" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="image-classification"><span class="header-section-number">6.2.1</span> Image Classification</h3>
<p>Image classification, also called visual recognition, is one of the most common tasks in computer vision: given an input image, a model is trained to predict the class of the image from a set of defined classes (e.g.&nbsp;categorizing photos of clothing as shirts, pants, jackets, etc.). One longstanding image classification benchmark is ImageNet, a dataset of over 14 million images labeled in a hierarchical class structure made up of thousands of categories <span class="citation" data-cites="5206848">(<a href="#ref-5206848" role="doc-biblioref">Deng et al. 2009</a>)</span>. An example of one of these images is given in <a href="#fig-dog">Figure&nbsp;<span>6.1</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-dog" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ImageNetExample.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;6.1: An example from the ImageNet training set, labeled “Staffordshire bull terrier”</figcaption>
</figure>
</div>
</div>
</div>
<p>We can see that the image contains an adorable puppy – this image is given the label “Staffordshire bull terrier,” and it contains some examples of the issues that arise within CV. Imagine that we are training a classifier to distinguish Staffordshire bull terriers from another dog breed, for example a Boston terrier. Ideally, we would want this model to focus on the true signal made up by meaningful differences between these breeds: Boston terriers are often black and white, while Staffordshire terriers have more uniform coats; Staffordshire terriers have more pronounced muzzles, while Bostons have a short snout. However, in reality, a deep learning model will look for any correlations present in the data in order to minimize the loss on the training set.</p>
<p>An obvious issue in this image is the watermark in the bottom right corner of <a href="#fig-dog">Figure&nbsp;<span>6.1</span></a>, which is a URL for a breeder of Staffordshire bull terriers in the Czech Republic. If many of our training images for Staffies come from this breeder, the model will likely learn to look for the shape of that URL watermark instead of more nuanced features like differences in the shapes of ears or muzzles. A model trained to identify the URL of a dog breeder will be useless if we want to deploy it on natural images “in the wild.” This is a prime example of how CV models can learn undesirable patterns, just like the spurious correlations introduced earlier in this book. This type of problem is a central focus in the area of robustness, and two approaches for mitigating spurious correlation in image classification are detailed in <a href="#sec-cv-robustness"><span>Section&nbsp;6.2.5</span></a>.</p>
<p>The image in <a href="#fig-dog">Figure&nbsp;<span>6.1</span></a> also includes potential confounders: for example, the puppy is photographed while being held by its owner. If our training set primarily includes images of this breed being held, the model might look for the shape of human hands rather than the dog’s fur length or color. Confounders like this one can appear in background color, lighting, textures, and other co-occurring elements within an image, and issues can arise when a classifier relies on the presence of a confounder rather than the primary subject of interest. One way to remedy this issue would be to manually label any confounders present in an image and intervene on their presence when training the model, but this would require significantly more effort than standard image classification labeling. One promising alternative, dubbed Causal Attention Module (CaaM), is a modification of the visual attention mechanism which allows for a model to <em>attend</em> to particular regions or channels of an image with greater focus. CaaM models confounders in an unsupervised manner, which allows for intervention without the need to manually annotate confounders during the labeling process. Their method works within standard visual attention settings, including vision Transformers (ViT) and attention CNNs, and achieves greater in-domain and out-of-domain performance than both attention-based and non-attention-based models <span class="citation" data-cites="https://doi.org/10.48550/arxiv.2108.08782">(<a href="#ref-https://doi.org/10.48550/arxiv.2108.08782" role="doc-biblioref">T. Wang et al. 2021</a>)</span>.</p>
</section>
<section id="few-shot-learning" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="few-shot-learning"><span class="header-section-number">6.2.2</span> Few-Shot Learning</h3>
<p><em>Transfer learning</em> has played a major role in the success of deep learning models in the last several years: we first <em>pretrain</em> on a task with abundant data, such as image reconstruction, and then <em>fine-tune</em> on the actual task we want the model to perform. Pretrained models allow for better transfer with less labeled data, accelerating the process of training a useful model whenever a relevant pretrained base model is available. Popular pretrained CV models include ResNet <span class="citation" data-cites="https://doi.org/10.48550/arxiv.1512.03385">(<a href="#ref-https://doi.org/10.48550/arxiv.1512.03385" role="doc-biblioref">He et al. 2015</a>)</span> and Vision Transformer <span class="citation" data-cites="https://doi.org/10.48550/arxiv.2010.11929">(<a href="#ref-https://doi.org/10.48550/arxiv.2010.11929" role="doc-biblioref">Dosovitskiy et al. 2020</a>)</span> for classification as well as the YOLO family of models for object detection <span class="citation" data-cites="https://doi.org/10.48550/arxiv.1506.02640">(<a href="#ref-https://doi.org/10.48550/arxiv.1506.02640" role="doc-biblioref">Redmon et al. 2015</a>)</span>.</p>
<p><em>Few-shot learning</em> is a special case of transfer learning, in which the target task has very few labeled observations available for fine-tuning Few-shot learning (FSL) has become a common tool for testing the performance of pretrained models, since it assesses the base model’s ability to generalize to new domains and tasks with little information. The successes of deep learning models in the few-shot setting is an exciting development, because it represents a major shift in what was previously impossible: that a deep neural network can reach meaningful performance with only a handful of relevant observations for a given task. The magic of FSL comes from the pretraining/fine-tuning paradigm – the model is able to extend the patterns it learns in the less relevant yet data-rich setting (such as low-level shapes and textures) beyond the immediate context and into an actually meaningful task.</p>
<p>However, is it possible that the “knowledge” gained in the pretraining setting is also potentially harmful in the few-shot fine-tuning setting? This is a phenomenon known as <em>negative transfer</em>, where the biases in the pretrained dataset actually degrade performance on the downstream task. When we examine this phenomenon from a causal perspective, it becomes clear why this might happen. A recent paper at NeurIPS <span class="citation" data-cites="NEURIPS2020_1cc8a8ea">(<a href="#ref-NEURIPS2020_1cc8a8ea" role="doc-biblioref">Yue et al. 2020</a>)</span> frames the few-shot transfer setting within the context of the causal graph in <a href="#fig-ifsl-graph">Figure&nbsp;<span>6.2</span></a>, where <span class="math inline">\(D\)</span> represents the pretrained knowledge of the base model, <span class="math inline">\(X\)</span> denotes the feature representation produced initially by <span class="math inline">\(D\)</span>, <span class="math inline">\(C\)</span> denotes the transformed representation of <span class="math inline">\(X\)</span> in the low-rank manifold, and <span class="math inline">\(Y\)</span> represents the output of the task-specific final layer.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ifsl-graph" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/InterventionalFewShot.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;6.2: Causal graph for interventional few-shot learning</figcaption>
</figure>
</div>
</div>
</div>
<p>In plain terms, this causal graph is saying that the biases present in the pretrained model are actually a confounding presence on the performance in the FSL task. For example, consider the possibility that a pretrained model had only seen Staffordshire terriers like the one in <a href="#fig-dog">Figure&nbsp;<span>6.1</span></a> indoors, standing on rugs – such a pretrained model might misinterpret the correlation between the dogs and their surroundings as something meaningful, and we may not have enough example images in the FSL context to overrule that bias.</p>
<p>As a way of addressing this, the authors suggest an interventional approach to remove the impact of the confounders. Specifically, if the standard (i.e.&nbsp;<em>many-shot</em>) setting for model transfer produces a task-specific layer <span class="math inline">\(Y\)</span> which is more or less fully adapted to the task, we can think of this action as <span class="math inline">\(P(Y|X) \approx P\big(Y|{do}(X)\big)\)</span>. This is not the case in the few-shot setting due to the substantial influence of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span>, however, this can be addressed by intervening on <span class="math inline">\(D \rightarrow X\)</span> via a backdoor adjustment and effectively modeling <span class="math inline">\(P\big(Y|{do}(X)\big)\)</span> directly. The authors do this via two specific adjustments: a feature-wise adjustment using the output of the final pretrained layer, and a class-wise adjustment to account for pretraining tasks which are classification-based (e.g.&nbsp;1000-class ImageNet, or BERT’s masked language modeling).</p>
<p>The result of the interventional few-shot learning (IFSL) approach is a method that improves FSL performance on different types of tasks and in different CV architectures. The authors show that the benefit of IFSL is greater in 1-shot tasks than in 5-shot tasks, and that improvements are more significant when the FSL domain is more different from the pretraining domain. The authors also demonstrate that existing methods of boosting FSL performance – such as data augmentation – are approximations of a similar intervention.</p>
</section>
<section id="weakly-supervised-semantic-segmentation" class="level3" data-number="6.2.3">
<h3 data-number="6.2.3" class="anchored" data-anchor-id="weakly-supervised-semantic-segmentation"><span class="header-section-number">6.2.3</span> Weakly Supervised Semantic Segmentation</h3>
<p>Semantic segmentation is a computer vision task which involves the generation of a pixel-level mask corresponding to individual entities within an image, which produces groups of regions corresponding to semantically similar objects. For example, a segmentation model trained on medical images may learn to distinguish suspicious masses from fluid, bone, and other normal-appearing tissue also visualized within the scan; and a segmentation model in an autonomous driving setting may try to identify masks of pedestrians, other vehicles, and road signs within the images produced by the vehicle’s camera. Semantic segmentation provides richer context through labeling classes for the masks within an image, such as denoting each distinct “person” mask within an image as belonging to the same “person” class. A semantic segmentation model may identify masked regions for both “dog” and “person” in our example image in <a href="#fig-dog">Figure&nbsp;<span>6.1</span></a>, while keeping their masks separate from each other as well as the nondescript background of the image.</p>
<p>One challenge with semantic segmentation is the cost of labeling: relative to the effort required to label other CV tasks (e.g.&nbsp;image classification), labeling images for semantic segmentation is difficult and time-consuming due to the nature of masking pixels for every instance of a particular entity. <em>Weakly supervised</em> semantic segmentation (WSSS) seeks to lower this barrier by collecting image-level labels, training a classifier, and utilizing information from the classifier to generate “pseudo masks” for semantic segmentation corresponding to the labels in the classification task <span class="citation" data-cites="ahn2018learning">(<a href="#ref-ahn2018learning" role="doc-biblioref">Ahn and Kwak 2018</a>)</span>. The “weak supervision” aspect comes through using one model’s output as the ground-truth input for another training task – it may not be a perfect or unbiased approach, but it significantly reduces the effort required to label and train semantic segmentation models, which in turn allows for labeled data to be collected at a much greater rate.</p>
<p><span class="citation" data-cites="NEURIPS2020_07211688">(<a href="#ref-NEURIPS2020_07211688" role="doc-biblioref">Zhang et al. 2020</a>)</span> propose a causal approach to WSSS, based on a backdoor adjustment to mitigate the impact of spurious correlations introduced by class co-occurrence and incomplete image-level labeling. Their fundamental WSSS approach is similar to other typical WSSS pipelines, comprising of an image classifier which generates attribution masks for training the segmentation model. However, the authors propose a method called Context Adjustment (CONTA) which models an intervention on class co-occurrence. Their causal graph is provided in <a href="#fig-conta-graph">Figure&nbsp;<span>6.3</span></a>, where <span class="math inline">\(X\)</span> represents the image observation, <span class="math inline">\(Y\)</span> represents the multi-label classification labels, <span class="math inline">\(C\)</span> represents the “context prior” inherent within the data-generating process, and <span class="math inline">\(M\)</span> is the image-specific manifestation of <span class="math inline">\(C\)</span> for image <span class="math inline">\(X\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-conta-graph" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ContaGraph.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;6.3: Causal graph for Context Adjustment in weakly supervised semantic segmentation</figcaption>
</figure>
</div>
</div>
</div>
<p>It is worth noting that the context prior <span class="math inline">\(C\)</span> does not directly influence the class labels in <span class="math inline">\(Y\)</span>, but instead only through observation-specific pathways through <span class="math inline">\(X\)</span> and <span class="math inline">\(M\)</span>. The objective of CONTA is to train the initial image classifier to learn <span class="math inline">\(P\big(Y|{do}(X)\big)\)</span> instead of the typical <span class="math inline">\(P(Y|X)\)</span> used in training machine learning models. In order to model <span class="math inline">\(P\big(Y|{do}(X)\big)\)</span>, the authors propose a framework which uses class-wise average pseudo masks to provide the intervention while training the image classifier. This results in an iterative algorithm similar in nature to expectation-maximization (EM) algorithms:</p>
<ol type="1">
<li>Train the multilabel image classifier using a custom loss function incorporating similarity against class-averaged masks</li>
<li>Generate pseudo-masks using the trained classifier’s attribution map</li>
<li>Train the segmentation model on pseudo-masks</li>
<li>Update the class averaged masks using the output of the segmentation model</li>
</ol>
<p>After a few iterations, this process results in a segmentation model which can produce cleaner boundaries between entities and more reliably handle complex images. A major benefit of this framework is that it is agnostic to the architecture and post-processing applied to the segmentation task, so that the latest art in segmentation can be used while also leveraging the causal techniques. The code for this framework is available at https://github.com/dongzhang89/CONTA.</p>
<section id="additional-work-in-wsss" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="additional-work-in-wsss">Additional work in WSSS</h4>
<p>A recent paper at CVPR proposes a WSSS technique for medical image segmentation based on similar foundations as CONTA – also using the WSSS approach of pseudo-masking via a classifier’s saliency maps. <span class="citation" data-cites="Chen_2022_CVPR">(<a href="#ref-Chen_2022_CVPR" role="doc-biblioref">Chen et al. 2022</a>)</span> extends the idea of intervening on influences from the data generating process by introducing an additional map for on anatomical structures present within the scan. The authors show that their method outperforms prior WSSS state of the art on various medical image segmentation tasks.</p>
</section>
</section>
<section id="vision-language-tasks" class="level3" data-number="6.2.4">
<h3 data-number="6.2.4" class="anchored" data-anchor-id="vision-language-tasks"><span class="header-section-number">6.2.4</span> Vision-Language Tasks</h3>
<p>The intersection of vision and language is gaining popularity as more breakthroughs are published, such as DALL-E’s results on image generation from a text prompt <span class="citation" data-cites="ramesh2021zeroshot">(<a href="#ref-ramesh2021zeroshot" role="doc-biblioref">Ramesh et al. 2021</a>)</span>. Common tasks in this area include generating captions for images, answering questions about images, and performing reasoning about an image. Common techniques include visual attention, with larger Transformer-based architectures like BEiT <span class="citation" data-cites="wang2022image">(<a href="#ref-wang2022image" role="doc-biblioref">W. Wang et al. 2022</a>)</span> achieving recent success across many vision-language tasks. This section will provide an overview of three applications of causal inference designed to improve performance and reliability of vision-language models.</p>
<section id="visual-commonsense-r-cnn" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visual-commonsense-r-cnn">Visual Commonsense R-CNN</h4>
<p><span class="citation" data-cites="Wang_2020_CVPR">(<a href="#ref-Wang_2020_CVPR" role="doc-biblioref">T. Wang et al. 2020</a>)</span> introduces Visual Commonsense R-CNN (VC R-CNN), which combines the commonly used region-based CNN (R-CNN) with a “visual commonsense” module designed to learn more robust features for question-answering and captioning. VC R-CNN approaches this in an unsupervised manner by identifying confounders present in an image, which it controls via the backdoor adjustment. VC R-CNN begins by generating regions of interest from an existing pretrained model (such as Faster R-CNN), averaging the regions of interest for each possible category in the dataset, and using the resulting dictionary of average shapes as a feature in the model. Specifically, the “dictionary” of regions of interest for each class is used alongside the observation image as inputs to scaled dot-product attention. The authors show a performance boost within vision-language tasks but note that the improvement is smaller on VQA than in captioning and reasoning. Their code is available at https://github.com/Wangt-CN/VC-R-CNN.</p>
</section>
<section id="causal-attention" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="causal-attention">Causal Attention</h4>
<p>Attention is commonly used in vision-language tasks, as it creates stronger relationships within an image as well as between the image and text. However, as with other techniques in deep neural networks, attention is prone to exploiting spurious correlations learned during the training process which may not hold in the deployed setting. The authors of <span class="citation" data-cites="Yang_2021_CVPR">(<a href="#ref-Yang_2021_CVPR" role="doc-biblioref">Yang et al. 2021</a>)</span> propose a general causal attention (CATT) module intended to remove the impact of confounders. CATT is designed similarly to the attention mechanism used in vision Transformers; however, it is used to intervene by using a front-door adjustment, reducing the effect of confounders without needing explicit information about them (unlike VC R-CNN).</p>
<p>CATT does this by introducing additional sampling from other observations (dubbed cross-sample sampling, or CS-Sampling), which stratifies the input values in order to produce a deconfounded predictor. The authors provide the following illustration for this technique:</p>
<blockquote class="blockquote">
<p>Intuitively, CS-Sampling approximates the “physical intervention” which can break the spurious correlation caused by the hidden confounder. For example, the annotation “man-with-snowboard” is dominant in captioning dataset [19] and thus the predictor may learn the spurious correlation between the snowboard region with the word “man” without looking at the person region to reason what actually the gender is. CS-Sampling alleviates such spurious correlation by combining the person region with the other objects from other samples, e.g., bike, mirror, or brush, and inputting the combinations to the predictor. Then the predictor will not always see “man-with-snowboard” but see “man” with the other distinctive objects and thus it will be forced to infer the word “man” from the person region.</p>
</blockquote>
<p>This example shows the intuition behind the front-door adjustment happening in CATT along with its purpose of producing a deconfounded predictor. CATT achieves this by projecting the regions of interest for the training set as embeddings; the in-sample sampling (IS-Sampling) focuses only on the input image, while CS-Sampling uses other related images from the training set as the keys and values in the attention operation. Their method extends to both <em>self-attention</em> (regions of the image attending to each other) and <em>top-down</em> attention (the image attending to the embedded text), and it can be stacked in deep networks such as Transformers. The authors show an improvement over standard VQA methods as well as the ability to improve smaller architectures by adding CATT. Their code is available at https://github.com/yangxuntu/lxmertcatt.</p>
</section>
<section id="counterfactual-visual-question-answering" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="counterfactual-visual-question-answering">Counterfactual Visual Question Answering</h4>
<p>The final causal method for vision-language we will discuss is rather different from CATT and VC R-CNN. One aspect of visual question-answering which is not covered in the previously discussed methods is the risk of language bias affecting the performance of the VQA model. Language bias can occur in many ways, including the nature of the questions in the dataset. The authors of <span class="citation" data-cites="niu2021counterfactual">(<a href="#ref-niu2021counterfactual" role="doc-biblioref">Niu et al. 2021</a>)</span> focus largely on this source of bias, pointing out for example that 90% of the “do you see a…” questions in the VQA v1.0 dataset are correctly answered as “yes,” which incentivizes the model to focus on the type of question rather than the contents of the image. The authors propose a counterfactual approach to understanding and removing language bias in VQA, effectively allowing the model to imagine what would have happened, for example, if the question had arrived with no visual evidence.</p>
<p>Specifically, <span class="citation" data-cites="niu2021counterfactual">(<a href="#ref-niu2021counterfactual" role="doc-biblioref">Niu et al. 2021</a>)</span> frames the causal question of language bias in terms of commonly used counterfactual quantities capturing both <em>direct</em> and <em>indirect</em> effects. This approach seeks to disentangle the impact of three factors on the answer <span class="math inline">\(A\)</span> produced in the VQA setting:</p>
<ol type="1">
<li>The input image in isolation, <span class="math inline">\(V\)</span></li>
<li>The input question in isolation, <span class="math inline">\(Q\)</span>: the pathway of language bias</li>
<li>The model’s ability to perform multimodal reasoning, <span class="math inline">\(K\)</span>: the primary objective of VQA</li>
</ol>
<p>Readers familiar with NLP or attention mechanisms will notice that the variable names <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> also correspond to the query, key, and value components of the attention function. This is by design, as it is helpful to delineate the flow of information through the VQA process. In this case, the direct effect of <span class="math inline">\(Q\rightarrow A\)</span> is the undesirable effect: we do not want the answer to be determined in any large part by the question in isolation – instead, we want the model to understand the question and assess the input image to determine the appropriate answer. The authors approach this by quantifying the direct and indirect effects of <span class="math inline">\(Q\)</span> on <span class="math inline">\(A\)</span> and selecting the generated answer which produced the largest total indirect effect, an effective alternative for sampling based on the largest posterior probability.</p>
</section>
</section>
<section id="sec-cv-robustness" class="level3" data-number="6.2.5">
<h3 data-number="6.2.5" class="anchored" data-anchor-id="sec-cv-robustness"><span class="header-section-number">6.2.5</span> Robustness and Transfer</h3>
<p>The prior sections have discussed methods leveraging causal techniques to improve model performance within certain subdomains of CV. In this section, we will take a broader look at efforts focused within model robustness and transferability.</p>
<p>Despite the various directions of success in CV, most of the existing work on robustness in CV has been focused on applications in image classification. As a result, most of the research in causal-based robustness also applies to image classification. In this section, we will detail two such approaches: one for removing the impact of spurious correlation on features like texture and background, and another for evaluating model generalization via causal methods. As the world of computer vision continues to evolve, so too will efforts in robustness.</p>
<section id="counterfactual-data-augmentation-for-robust-image-classifiers" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="counterfactual-data-augmentation-for-robust-image-classifiers">Counterfactual Data Augmentation for Robust Image Classifiers</h4>
<p>The issue of complex, greedy ML models latching onto spurious correlations is not unique to computer vision, but it is a common problem facing CV practitioners. <span class="citation" data-cites="sauer2021counterfactual">(<a href="#ref-sauer2021counterfactual" role="doc-biblioref">Sauer and Geiger 2021</a>)</span> introduces Counterfactual Generative Networks, a GAN-based approach of creating counterfactual images that disentangle and separately model an image’s shape, texture, and background. The supervision for this model is only supplied by the image’s class label and the inductive biases from the orchestration of the GANs which leverage pretrained models, and the result is a series of models which can generate an image with specified shape, texture, and background. <a href="#fig-cgn-example">Figure&nbsp;<span>6.4</span></a> shows somewhat of an extreme example: the generated image from the authors’ prompt of an “ostrich” shape with “strawberry” texture on a “diving” background.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-cgn-example" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/CGNExample.png" class="img-fluid figure-img" width="482"></p>
<figcaption class="figure-caption">Figure&nbsp;6.4: An image generated by Counterfactual Generative Network</figcaption>
</figure>
</div>
</div>
</div>
<p>Using the generative models, the authors generate synthetic training data for the image classifier, such MNIST digits with slightly different colors or shapes, or animals with different backgrounds. The authors show that this additional information helps the model generalize better based on test set performance, providing an interesting direction for future work in causal robustness and counterfactual modeling.</p>
</section>
<section id="causal-inference-for-model-generalization" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="causal-inference-for-model-generalization">Causal Inference for Model Generalization</h4>
<p>Another important focus of robustness is centered around model transferability. This consists of two primary goals:</p>
<ol type="1">
<li>Ensuring that a trained model transfers well to new data</li>
<li>Understanding when and how things go wrong in the event that the model fails to transfer</li>
</ol>
<p>In the context of computer vision, models can often generalize properly on new data drawn from the same distribution as the training set. However, performance often degrades substantially when attempting to generalize on data from another distribution. Using our earlier example, a model trained to identify Staffordshire bull terriers like the one in <a href="#fig-dog">Figure&nbsp;<span>6.1</span></a> could perform well on images of puppies being held by their breeders, but might not generalize to adult dogs photographed in slightly different settings.</p>
<p>One way to view the issue of model transfer is through the lens of <em>transportability</em>. Transportability is a causal term used to describe whether information learned in one environment is applicable in another environment <span class="citation" data-cites="6137426">(<a href="#ref-6137426" role="doc-biblioref">Pearl and Bareinboim 2011</a>)</span>. A lack of transportability in image classification means that there is some material difference in the relationship between images and their labels when comparing the in-distribution and out-of-distribution settings. <span class="citation" data-cites="mao2022causal">(<a href="#ref-mao2022causal" role="doc-biblioref">Mao et al. 2022</a>)</span> demonstrate this by first developing a causal graph to represent the out-of-distribution transfer process, which they use to show that the primary source of generalization failure is due to a lack of transportability in the association between images and labels.</p>
<p>In probabilistic terms, this issue of transportability means that the conditional probability <span class="math inline">\(P(Y|X)\)</span> learned by the model in the in-distribution context does not match what is encountered out-of-distribution. After establishing that there is an issue of transportability, the authors decompose <span class="math inline">\(P(Y|X)\)</span> into two components for each setting:</p>
<ol type="1">
<li>The causal effect: an invariant quantity which is consistent in both the in-domain and out-of-domain setting (for example, the shape of a dog’s muzzle, or the color and length of its fur)</li>
<li>The spurious effect: the part of the distribution which is inconsistent between contexts, leading to generalization failure (for example, different backgrounds or lighting)</li>
</ol>
<p>The authors proceed to show that it is possible to estimate this invariant causal effect – that is, <span class="math inline">\(P\big(Y|{do}(X)\big)\)</span> – for the image classification task without observing additional variables. This is made possible by adding a causal structure to the representations learned by deep neural networks, which is built upon the following assumptions:</p>
<ul>
<li>That the data generating process does indeed follow the decomposition outlined above (i.e.&nbsp;the images are made up of causal and spurious features)</li>
<li>That a neural model can sufficiently learn representations which contain relevant the causal factors</li>
<li>That the classifier will mimic the behavior of the true labeler when combining the causal factors from an in-distribution image <span class="math inline">\(x\)</span> with the spurious factors of an out-of-distribution image <span class="math inline">\(x'\)</span></li>
</ul>
<p>For settings where these assumptions hold, the authors present a framework to be used during training and estimation which can identify the causal transportability effect without needing additional information. During training, the framework leverages either a variational autoencoder (VAE) or another pretrained encoder to produce representations for the training images. The classifier then learns to categorize images based on the combination of representations of an image with other inputs from related training images. By doing this within-class sampling, the classifier learns more robust features about the image’s subject, allowing for more robust transfer. A similar sampling process is used to estimate <span class="math inline">\(P\big(Y|{do}(X)\big)\)</span> during evaluation to assess the transportability of the image classifier and produce more robust predictions.</p>
<p>The work introduced in <span class="citation" data-cites="mao2022causal">(<a href="#ref-mao2022causal" role="doc-biblioref">Mao et al. 2022</a>)</span> provides a promising direction for reducing the impact of spurious correlation in image classification, and their results on adversarial domain generalization tasks show that the framework provides a meaningful boost in generalization. We will now provide a case study demonstrating the process on one of these tasks.</p>
</section>
</section>
</section>
<section id="case-study" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="case-study"><span class="header-section-number">6.3</span> Case Study</h2>
<p>This case study can be <a href="https://colab.research.google.com/drive/1nOO77UGrcE9aUlaXwkEQB2QoPv8Y9v0l?usp=sharing">accessed via Google Colab</a>, with a walk-through here as well.</p>
<p>The case study for this chapter includes a simple reimplementation of <span class="citation" data-cites="mao2022causal">(<a href="#ref-mao2022causal" role="doc-biblioref">Mao et al. 2022</a>)</span>. The case study illustrates their transportable classification method on Colored MNIST <span class="citation" data-cites="arjovsky2020invariant">(<a href="#ref-arjovsky2020invariant" role="doc-biblioref">Arjovsky et al. 2020</a>)</span>, a benchmark for evaluating classifier robustness using adversarial color shifts between training and test data.</p>
<p>The Colored MNIST dataset starts with the standard grayscale handwritten digits in the MNIST data, and it applies background coloring differently according to the dataset: training images have coloring applied according to their classes, and test images are colored randomly. The two transformations applied to the training set are shown in <a href="#fig-mnist-train">Figure&nbsp;<span>6.5</span></a></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-mnist-train" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ColorMNISTExampleTraining0.png" class="img-fluid figure-img" width="716"></p>
<figcaption class="figure-caption">Figure&nbsp;6.5: An image in the MNIST training set with two color shifts applied</figcaption>
</figure>
</div>
</div>
</div>
<p>We can see that one variation has the digit in white on a yellow background, while the other has the digit in yellow on a black background. These are the same image, and both variations are present in the training set. Another example of this coloring scheme can be seen in <a href="#fig-mnist-train-test">Figure&nbsp;<span>6.6</span></a>, where there are two training examples labeled as the digit <code>7</code>, each representing a different coloring for sevens. Also present in <a href="#fig-mnist-train-test">Figure&nbsp;<span>6.6</span></a> is a test image with the label of <code>7</code>, and we can see that its coloring matches neither of the training sevens.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-mnist-train-test" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ColorMNISTTrainVsTest.png" class="img-fluid figure-img" width="716"></p>
<figcaption class="figure-caption">Figure&nbsp;6.6: Three images labeled “seven” in the Colored MNIST dataset</figcaption>
</figure>
</div>
</div>
</div>
<p>The causal transportability methods rely on the use of an image encoder; we follow the original implementation and use a variational autoencoder. However, there is nothing special about the autoencoder, and other image encoders (e.g.&nbsp;a ResNet) can be used. <a href="#fig-mnist-vae">Figure&nbsp;<span>6.7</span></a> shows an image from the Colored MNIST training set along with its reconstructed representation produced by the VAE.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-mnist-vae" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ColorMNISTReconstruction.png" class="img-fluid figure-img" width="716"></p>
<figcaption class="figure-caption">Figure&nbsp;6.7: An image from the training set (left) with its reconstructed autoencoder output (right)</figcaption>
</figure>
</div>
</div>
</div>
<p>The autoencoder is used to encode the input images. The causal classifier then uses fragments of similar training images with the same label to force the model to learn the relevant features from the input batch. Unfortunately, this does not seem to outperform a non-causal benchmark in our experiment: our non-causal model uses an architecture which is similar to the one used by the causal classifier. <a href="#fig-mnist-perf">Figure&nbsp;<span>6.8</span></a> shows the training and test set performance of each of these classifiers on the Colored MNIST dataset.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-mnist-perf" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ColorMNISTPerformance.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;6.8: Model performance on Colored MNIST</figcaption>
</figure>
</div>
</div>
</div>
<p>As we can see, the causal classifier has worse performance on the test set than the non-causal benchmark. This gap in performance could be due to a number of reasons, including the difference in baseline used in the original implementation, the modifications applied to the original Colored MNIST dataset, or the design of the encoder. For example, perhaps a CNN- or attention-based encoder would perform better.</p>
<p>Despite these classification results, causality in computer vision is still an exciting area of ongoing work, presenting challenges and opportunities which could lead to meaningful gains in robustness in applied CV. Importantly, as we can see in the case study code, causal methods in computer vision can be implemented in ways that look largely similar to other standard deep learning code – for instance, the model classes and training loop for the causal classifier looks very similar to the PyTorch code used to train the non-causal classifier. Making such methods accessible for use in other applications will foster additional experimentation with these methods.</p>
</section>
<section id="conclusions" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="conclusions"><span class="header-section-number">6.4</span> Conclusions</h2>
<p>In this chapter, we have introduced the reasons why practitioners in computer vision would want to use causality in their work. Causal methods have produced promising initial results across several areas of computer vision, with much work focusing on image classification and vision-language tasks.</p>
<p>We also demonstrated one of these methods in a case study. This case study introduces the Colored MNIST dataset and compares a causal transportability approach against a standard image classifier. While the causal method does not provide a performance boost over the non-causal method, it provides a data point and streamlined PyTorch implementation for future experimentation.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-ahn2018learning" class="csl-entry" role="listitem">
Ahn, Jiwoon, and Suha Kwak. 2018. <span>“Learning Pixel-Level Semantic Affinity with Image-Level Supervision for Weakly Supervised Semantic Segmentation.”</span> <a href="https://arxiv.org/abs/1803.10464">https://arxiv.org/abs/1803.10464</a>.
</div>
<div id="ref-arjovsky2020invariant" class="csl-entry" role="listitem">
Arjovsky, Martin, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. 2020. <span>“Invariant Risk Minimization.”</span> <a href="https://arxiv.org/abs/1907.02893">https://arxiv.org/abs/1907.02893</a>.
</div>
<div id="ref-https://doi.org/10.48550/arxiv.2207.12939" class="csl-entry" role="listitem">
Cakir, Senay, Marcel Gauß, Kai Häppeler, Yassine Ounajjar, Fabian Heinle, and Reiner Marchthaler. 2022. <span>“Semantic Segmentation for Autonomous Driving: Model Evaluation, Dataset Generation, Perspective Comparison, and Real-Time Capability.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2207.12939">https://doi.org/10.48550/ARXIV.2207.12939</a>.
</div>
<div id="ref-Castro_2020" class="csl-entry" role="listitem">
Castro, Daniel C., Ian Walker, and Ben Glocker. 2020. <span>“Causality Matters in Medical Imaging.”</span> <em>Nature Communications</em> 11 (1). <a href="https://doi.org/10.1038/s41467-020-17478-w">https://doi.org/10.1038/s41467-020-17478-w</a>.
</div>
<div id="ref-Chen_2022_CVPR" class="csl-entry" role="listitem">
Chen, Zhang, Zhiqiang Tian, Jihua Zhu, Ce Li, and Shaoyi Du. 2022. <span>“C-CAM: Causal CAM for Weakly Supervised Semantic Segmentation on Medical Image.”</span> In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 11676–85.
</div>
<div id="ref-5206848" class="csl-entry" role="listitem">
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. <span>“ImageNet: A Large-Scale Hierarchical Image Database.”</span> In <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248–55. <a href="https://doi.org/10.1109/CVPR.2009.5206848">https://doi.org/10.1109/CVPR.2009.5206848</a>.
</div>
<div id="ref-https://doi.org/10.48550/arxiv.2010.11929" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. <span>“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2010.11929">https://doi.org/10.48550/ARXIV.2010.11929</a>.
</div>
<div id="ref-https://doi.org/10.48550/arxiv.1512.03385" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. <span>“Deep Residual Learning for Image Recognition.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.1512.03385">https://doi.org/10.48550/ARXIV.1512.03385</a>.
</div>
<div id="ref-howard2020deep" class="csl-entry" role="listitem">
Howard, J., and S. Gugger. 2020. <em>Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD</em>. O’Reilly Media, Incorporated. <a href="https://books.google.no/books?id=xd6LxgEACAAJ">https://books.google.no/books?id=xd6LxgEACAAJ</a>.
</div>
<div id="ref-NIPS2012_c399862d" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. <span>“ImageNet Classification with Deep Convolutional Neural Networks.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Vol. 25. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a>.
</div>
<div id="ref-726791" class="csl-entry" role="listitem">
Lecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998. <span>“Gradient-Based Learning Applied to Document Recognition.”</span> <em>Proceedings of the IEEE</em> 86 (11): 2278–2324. <a href="https://doi.org/10.1109/5.726791">https://doi.org/10.1109/5.726791</a>.
</div>
<div id="ref-mao2022causal" class="csl-entry" role="listitem">
Mao, Chengzhi, Kevin Xia, James Wang, Hao Wang, Junfeng Yang, Elias Bareinboim, and Carl Vondrick. 2022. <span>“Causal Transportability for Visual Recognition.”</span> <a href="https://arxiv.org/abs/2204.12363">https://arxiv.org/abs/2204.12363</a>.
</div>
<div id="ref-Milletar2016VNetFC" class="csl-entry" role="listitem">
Milletarì, Fausto, Nassir Navab, and Seyed-Ahmad Ahmadi. 2016. <span>“V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation.”</span> <em>2016 Fourth International Conference on 3D Vision (3DV)</em>, 565–71.
</div>
<div id="ref-niu2021counterfactual" class="csl-entry" role="listitem">
Niu, Yulei, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong Wen. 2021. <span>“Counterfactual VQA: A Cause-Effect Look at Language Bias.”</span> <a href="https://arxiv.org/abs/2006.04315">https://arxiv.org/abs/2006.04315</a>.
</div>
<div id="ref-6137426" class="csl-entry" role="listitem">
Pearl, Judea, and Elias Bareinboim. 2011. <span>“Transportability of Causal and Statistical Relations: A Formal Approach.”</span> In <em>2011 IEEE 11th International Conference on Data Mining Workshops</em>, 540–47. <a href="https://doi.org/10.1109/ICDMW.2011.169">https://doi.org/10.1109/ICDMW.2011.169</a>.
</div>
<div id="ref-ramesh2021zeroshot" class="csl-entry" role="listitem">
Ramesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. <span>“Zero-Shot Text-to-Image Generation.”</span> <a href="https://arxiv.org/abs/2102.12092">https://arxiv.org/abs/2102.12092</a>.
</div>
<div id="ref-https://doi.org/10.48550/arxiv.1506.02640" class="csl-entry" role="listitem">
Redmon, Joseph, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2015. <span>“You Only Look Once: Unified, Real-Time Object Detection.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.1506.02640">https://doi.org/10.48550/ARXIV.1506.02640</a>.
</div>
<div id="ref-sauer2021counterfactual" class="csl-entry" role="listitem">
Sauer, Axel, and Andreas Geiger. 2021. <span>“Counterfactual Generative Networks.”</span> <a href="https://arxiv.org/abs/2101.06046">https://arxiv.org/abs/2101.06046</a>.
</div>
<div id="ref-Wang_2020_CVPR" class="csl-entry" role="listitem">
Wang, Tan, Jianqiang Huang, Hanwang Zhang, and Qianru Sun. 2020. <span>“Visual Commonsense r-CNN.”</span> In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
</div>
<div id="ref-https://doi.org/10.48550/arxiv.2108.08782" class="csl-entry" role="listitem">
Wang, Tan, Chang Zhou, Qianru Sun, and Hanwang Zhang. 2021. <span>“Causal Attention for Unbiased Visual Recognition.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2108.08782">https://doi.org/10.48550/ARXIV.2108.08782</a>.
</div>
<div id="ref-wang2022image" class="csl-entry" role="listitem">
Wang, Wenhui, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, et al. 2022. <span>“Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks.”</span> <a href="https://arxiv.org/abs/2208.10442">https://arxiv.org/abs/2208.10442</a>.
</div>
<div id="ref-Yang_2021_CVPR" class="csl-entry" role="listitem">
Yang, Xu, Hanwang Zhang, Guojun Qi, and Jianfei Cai. 2021. <span>“Causal Attention for Vision-Language Tasks.”</span> In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 9847–57.
</div>
<div id="ref-NEURIPS2020_1cc8a8ea" class="csl-entry" role="listitem">
Yue, Zhongqi, Hanwang Zhang, Qianru Sun, and Xian-Sheng Hua. 2020. <span>“Interventional Few-Shot Learning.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, 33:2734–46. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2020/file/1cc8a8ea51cd0adddf5dab504a285915-Paper.pdf">https://proceedings.neurips.cc/paper/2020/file/1cc8a8ea51cd0adddf5dab504a285915-Paper.pdf</a>.
</div>
<div id="ref-NEURIPS2020_07211688" class="csl-entry" role="listitem">
Zhang, Dong, Hanwang Zhang, Jinhui Tang, Xian-Sheng Hua, and Qianru Sun. 2020. <span>“Causal Intervention for Weakly-Supervised Semantic Segmentation.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, 33:655–66. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2020/file/07211688a0869d995947a8fb11b215d6-Paper.pdf">https://proceedings.neurips.cc/paper/2020/file/07211688a0869d995947a8fb11b215d6-Paper.pdf</a>.
</div>
<div id="ref-Zhou2021ComputerVT" class="csl-entry" role="listitem">
Zhou, Longfei, Lin Zhang, and N. Konz. 2021. <span>“Computer Vision Techniques in Manufacturing.”</span> <em>IEEE Transactions on Systems, Man, and Cybernetics: Systems</em> 53: 105–17.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>See chapters 4 and 5: https://github.com/fastai/fastbook<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Available at https://www.coursera.org/learn/convolutional-neural-networks?specialization=deep-learning<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./06-nlp.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">NLP</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./08-time-dependent-causal-inference.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Time-dependent Causal Inference</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>