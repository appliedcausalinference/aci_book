<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="This is a book which covers applications of causality, ranging from a practical overview of causal inference to cutting-edge applications of causality in machine learning domains.">

<title>Applied Causal Inference - 9&nbsp; Reinforcement Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./12-references.html" rel="next">
<link href="./10-fairness.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./11-reinforcement-learning.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Applied Causal Inference</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro-to-causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to Causality</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-potential-outcomes-framework.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Causal Inference: Theory and Basic Concepts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-causal-estimation-process.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Causal Inference: A Practical Approach</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-causal-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Causal Discovery</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-part-2-break.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part 2: Causality in ML Domains</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">NLP</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-computer-vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Computer Vision</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-time-dependent-causal-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Time-dependent Causal Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-part-3-break.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part 3: Advanced Topics in Causality</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Fairness</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-reinforcement-learning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#reinforcement-learning-a-brief-introduction" id="toc-reinforcement-learning-a-brief-introduction" class="nav-link active" data-scroll-target="#reinforcement-learning-a-brief-introduction"><span class="header-section-number">9.1</span> Reinforcement Learning: A Brief Introduction</a></li>
  <li><a href="#adding-causality-to-rl" id="toc-adding-causality-to-rl" class="nav-link" data-scroll-target="#adding-causality-to-rl"><span class="header-section-number">9.2</span> Adding Causality to RL</a>
  <ul class="collapse">
  <li><a href="#causality-for-smarter-agents" id="toc-causality-for-smarter-agents" class="nav-link" data-scroll-target="#causality-for-smarter-agents"><span class="header-section-number">9.2.1</span> Causality for Smarter Agents</a></li>
  <li><a href="#causality-for-transfer-learning-and-imitation-learning" id="toc-causality-for-transfer-learning-and-imitation-learning" class="nav-link" data-scroll-target="#causality-for-transfer-learning-and-imitation-learning"><span class="header-section-number">9.2.2</span> Causality for Transfer Learning and Imitation Learning</a></li>
  <li><a href="#causality-for-agent-explainability-and-fairness" id="toc-causality-for-agent-explainability-and-fairness" class="nav-link" data-scroll-target="#causality-for-agent-explainability-and-fairness"><span class="header-section-number">9.2.3</span> Causality for Agent Explainability and Fairness</a></li>
  </ul></li>
  <li><a href="#conclusions-and-open-problems" id="toc-conclusions-and-open-problems" class="nav-link" data-scroll-target="#conclusions-and-open-problems"><span class="header-section-number">9.3</span> Conclusions and Open Problems</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-rl" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>The application of causal methods within reinforcement learning is an area of active research and early adoption. This chapter will illustrate where causality fits into RL settings, what causal RL techniques exist, and the challenges that come with combining causal methods and reinforcement learning.</p>
<p>As we will see, there are many ways that causal inference can be integrated across many different contexts within RL — because of this, this chapter will be somewhat less consistently applied in nature than other chapters in this book. Here, we will outline several facets of the current state of causal RL, introduce the concepts, and provide direction for readers interested in going deeper.</p>
<section id="reinforcement-learning-a-brief-introduction" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="reinforcement-learning-a-brief-introduction"><span class="header-section-number">9.1</span> Reinforcement Learning: A Brief Introduction</h2>
<p><em>Reinforcement learning</em> (RL) is a subfield of machine learning in which an agent learns to interact within an environment in such a way that it maximizes its expected reward. RL has enjoyed profound success in the world of game-playing, with models like AlphaGo Zero achieving superhuman performance using only self-play <span class="citation" data-cites="silver2017zero">(<a href="#ref-silver2017zero" role="doc-biblioref">Silver et al. 2017</a>)</span>. More recent breakthroughs in game-playing have come in the games of Stratego, where DeepMind’s <em>DeepNash</em> agent achieves performance levels competitive with professional human players <span class="citation" data-cites="deepnash2022">(<a href="#ref-deepnash2022" role="doc-biblioref">Perolat et al. 2022</a>)</span>; and Diplomacy, where Meta’s <em>Cicero</em> agent also achieves human-level performance using RL and techniques from language modeling <span class="citation" data-cites="cicero2022">(<a href="#ref-cicero2022" role="doc-biblioref">Bakhtin et al. 2022</a>)</span>. Reinforcement learning applications have also appeared in various industry settings, including quantitative finance <span class="citation" data-cites="liu2018drlstock">(<a href="#ref-liu2018drlstock" role="doc-biblioref">Liu et al. 2018</a>)</span>, self-driving car technology <span class="citation" data-cites="kiran2020selfdriving">(<a href="#ref-kiran2020selfdriving" role="doc-biblioref">Kiran et al. 2020</a>)</span>, and computer hardware design <span class="citation" data-cites="google2020chips">(<a href="#ref-google2020chips" role="doc-biblioref">Mirhoseini et al. 2020</a>)</span>. RL has even found applications within other ML domains, with OpenAI’s ChatGPT model utilizing a PPO model with human feedback for controlling generation quality <span class="citation" data-cites="chatgpt2022">(<a href="#ref-chatgpt2022" role="doc-biblioref"><span>“ChatGPT: Optimizing Language Models for Dialogue”</span> 2022</a>)</span>.</p>
<p>Two primary approaches exist for framing reinforcement learning problems: in <em>model-based</em> RL, the agent attempts to learn a representation of how its world operates – specifically, it tries to model the transition between environment states separately from learning how it should act within the environment. Alternatively, the agents in <em>model-free</em> settings attempt to learn how to act without explicitly modeling the dynamics of the environment.</p>
<p>In <em>online</em> RL, models are updated incrementally through a process of trial and error as the agents interact with the environment and observe the consequences of their action. Conversely, <em>offline</em> RL leverages external datasets to approximate these functions without interacting with the environment <span class="citation" data-cites="levine2020offline">(<a href="#ref-levine2020offline" role="doc-biblioref">Levine et al. 2020</a>)</span>.</p>
<p>One final distinction between RL approaches is between <em>value-based</em> and <em>policy-based</em> reinforcement learning. In value-based RL, the modeling task seeks to approximate the discounted future rewards from taking a particular action at a given state — the agent would then take the action which maximizes expected future rewards. On the other hand, policy-based RL attempts to model a probability distribution over all possible actions at a given state, essentially modeling the policy directly.</p>
<p>This is a <em>very</em> simplified overview of reinforcement learning, and other resources exist for readers seeking a deeper introduction. In addition to the previously covered concepts of causality, this chapter assumes some familiarity with the following:</p>
<ul>
<li>POMDPs: environments, observations, states, actions, and rewards</li>
<li>Value-based RL: the Bellman equation and Q-learning</li>
<li>Policy-based RL</li>
<li>Multi-armed bandits</li>
<li>Visual RL environments</li>
</ul>
<p>Introductory material for these concepts can be found in an online lecture series from DeepMind’s David Silver <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> as well as in Sutton and Barto’s textbook <span class="citation" data-cites="sutton2018reinforcement">(<a href="#ref-sutton2018reinforcement" role="doc-biblioref">Sutton and Barto 2018</a>)</span> <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. Additionally, a free hands-on course in deep RL is available from Hugging Face <span class="citation" data-cites="deep-rl-class">(<a href="#ref-deep-rl-class" role="doc-biblioref">Simonini and Sanseviero 2022</a>)</span> <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
</section>
<section id="adding-causality-to-rl" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="adding-causality-to-rl"><span class="header-section-number">9.2</span> Adding Causality to RL</h2>
<p>As previously discussed, reinforcement learning is an area which has enjoyed success and generated excitement over the last several years. It is also an area which stands to benefit from adopting causal methods: online RL inherently contains a somewhat causal structure, in that the environment produces certain observations and outcomes based on the agent’s actions, and the agent seeks to learn the effects of its actions. The core idea of causal reinforcement learning is the introduction of causal information – via a causal graph and causal model – into an otherwise typical reinforcement learning setting.</p>
<p>One major benefit of causal inference is the ability to incorporate domain knowledge – this makes causality appealing for reinforcement learning, in which agent performance can be improved by incorporating outside knowledge of the world. For instance, RL applications within healthcare might benefit from knowledge about drug interactions which would be readily known by medical professionals — causality provides a method of encoding this prior knowledge.</p>
<p>Additionally, some of the most significant challenges in reinforcement learning at large include sample efficiency (how many episodes it takes for an agent to reach acceptable performance) and the ability to elegantly utilize offline data (for example, collected through observation of another agent in a similar system) with an agent performing in an online setting. As we will discuss in later sections, researchers in causal RL are using causal methods to address both of these problems.</p>
<p>Elias Bareinboim presented a tutorial on Causal Reinforcement Learning at NeurIPS 2020. Much of the progress in causal RL through late 2020 is captured in his tutorial, and many of the concepts included in this chapter are introduced there in some detail. Readers interested in going deeper on causal RL are encouraged to explore Bareinboim’s tutorial and other work further – his slides, notes, and videos from the CRL tutorial can be found at <a href="https://crl.causalai.net/">this link</a>.</p>
<section id="causality-for-smarter-agents" class="level3" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="causality-for-smarter-agents"><span class="header-section-number">9.2.1</span> Causality for Smarter Agents</h3>
<p>One of the primary areas of application in causal RL might be summarized by the notion of using causality to make agents “smarter” by…</p>
<ul>
<li>Helping agents understand their environment via a causal world model</li>
<li>Adding causal bounds on regret expectations</li>
<li>Improving action selection with causal knowledge</li>
<li>Making agents more robust against observational interference or interruptions</li>
</ul>
<p>While these approaches differ in their application within RL settings, they all contribute to improving sample efficiency, optimality, and agent reliability. We will go into greater depth on each of these in the following sections, starting with the inclusion of causal knowledge in an agent’s world model.</p>
<section id="better-world-models-in-mbrl" class="level4" data-number="9.2.1.1">
<h4 data-number="9.2.1.1" class="anchored" data-anchor-id="better-world-models-in-mbrl"><span class="header-section-number">9.2.1.1</span> Better World Models in MBRL</h4>
<p>In model-based reinforcement learning (MBRL), the models that seek to learn environment dynamics and state transitions are often called <em>world models</em>. These world models represent a significant area of application of causal RL because they provide a natural opportunity for embedding external knowledge (e.g., of a domain) into an agent.</p>
<p>In <span class="citation" data-cites="https://doi.org/10.48550/arxiv.2012.14228">(<a href="#ref-https://doi.org/10.48550/arxiv.2012.14228" role="doc-biblioref">Li et al. 2020</a>)</span> the authors introduce the concept of a counterfactual “dream world” in which the agent can imagine the results of hypothetical actions via <em>do</em>-interventions. Their methods are illustrated in physical environments (e.g.&nbsp;object placement) and show benefits in sample efficiency to reach optimal performance. Additionally, <span class="citation" data-cites="https://doi.org/10.48550/arxiv.2002.02836">(<a href="#ref-https://doi.org/10.48550/arxiv.2002.02836" role="doc-biblioref">Rezende et al. 2020</a>)</span> provides a general framework for creating <em>causally correct</em> partial models of future observations, which is especially appealing in cases where jointly modeling future observations fully would be intractable.</p>
<p>In a similar vein, <span class="citation" data-cites="pmlr-v177-lu22a">(<a href="#ref-pmlr-v177-lu22a" role="doc-biblioref">Lu, Meisami, and Tewari 2022</a>)</span> introduces the concept of <em>Causal Markov Decision Processes</em> (C-MDPs) which impose a causal framework over both the state transition and reward function within factored MDPs. The authors provide a regret bound given under the C-MDP, and they also introduce factorized methods for applications with high-dimensional action spaces and state spaces.</p>
<p>As we have seen, causal world models can be used in certain contexts where the variables underlying treatment, effect, and any other factors are explicitly given. However, this is not usually the case in visual RL environments where observations typically consist of pixels, and causal phenomena must be inferred based on changes in the pixel observation. In such settings, there is an additional need to express the observation in terms of high-level causal factors while also representing the relationships between them, a problem called <em>causal induction</em>. Fortunately, causal discovery methods can be used to learn the causal graph during online RL training; a survey of these methods can be found in <span class="citation" data-cites="discovery2021">(<a href="#ref-discovery2021" role="doc-biblioref">Ke et al. 2021</a>)</span>. In this paper, the authors explore the current state of causal discovery within visual environments and provide new benchmarks to systematically evaluate causal induction methods. At a high level, the modeling approach consists of training an encoder for the visual observation (either an autoencoder or variational autoencoder) along with a “transition model” for the structure of the causal graph, which consists of either a graph neural network or a modular approach with directed edges. The authors first train the encoder and transition model based on trajectories produced by random actions, followed by use in a downstream RL agent. Their work shows that the modular inductive bias most closely models the explicit causal structure of the environment. However, it is unclear whether this translates to consistent improvement over non-causal techniques in visual RL, since the proposed benchmarks largely involve effects that immediately follow the agent’s action.</p>
</section>
<section id="dynamic-treatment-regimes" class="level4" data-number="9.2.1.2">
<h4 data-number="9.2.1.2" class="anchored" data-anchor-id="dynamic-treatment-regimes"><span class="header-section-number">9.2.1.2</span> Dynamic Treatment Regimes</h4>
<p>One application related to causal world models comes in healthcare, where RL can be used to develop <em>dynamic treatment regimes</em> (DTRs). DTRs are plans of care that change as a disease or condition progresses along with other relevant markers of the patient’s status. Specifically, DTRs describe how a healthcare provider should respond to an evolving picture of a patient’s health, making them useful for managing chronic diseases as well as an intriguing application for reinforcement learning <span class="citation" data-cites="doi:10.1146/annurev-statistics-022513-115553">(<a href="#ref-doi:10.1146/annurev-statistics-022513-115553" role="doc-biblioref">Chakraborty and Murphy 2014</a>)</span>. When developing DTRs, incorporating prior knowledge of drug interactions or effects can reduce regret compared to standard exploration techniques, an ideal setting for causal modeling.</p>
<p>Junzhe Zhang and Elias Bareinboim have done much of the work on causal DTRs. <span class="citation" data-cites="zhang2019dtr">(<a href="#ref-zhang2019dtr" role="doc-biblioref">Zhang and Bareinboim 2019</a>)</span> first introduces an online RL approach which achieves near-optimality in learning DTRs when no observational data is available. This method, called <em>UC-DTR</em>, achieves near-optimal bounds on total regret without using causality – resulting in sub-linear total regret over the number of episodes. The authors then introduce <em>Causal UC-DTR</em>, which extends the UC-DTR algorithm by imposing causal bounds over state transition probabilities. In experiments with two-stage DTRs, both techniques vastly outperform random treatment selection, while Causal UC-DTR also produces lower cumulative regret than the non-causal technique.</p>
<p>Zhang and Bareinboim extend upon their work in causal DTRs in <span class="citation" data-cites="pmlr-v119-zhang20a">(<a href="#ref-pmlr-v119-zhang20a" role="doc-biblioref">Zhang 2020</a>)</span>, in which they show that adding a causal diagram of the DTR environment, along with a corresponding structural causal model, can provide “regret that is exponentially smaller” than non-causal RL methods for finding the optimal DTR. They introduce two online algorithms:</p>
<ul>
<li><strong>OFU-DTR</strong> uses “optimism in the face of uncertainty,” an optimistic planning approach which emphasizes unexplored or promising options during action selection, and adds causal knowledge of the environment to achieve tighter regret bounds</li>
<li><strong>PS-DTR</strong> combines the Bayesian method of posterior sampling with structural causal models (SCMs) in order to develop policies which maximize expected value based on the causal information in the SCMs</li>
</ul>
<p>In addition to these online learning methods, Zhang and Bareinboim propose a process of learning optimal DTRs from observational data – effectively giving the causal models a “warm start” – prior to online experimentation. The learning process consists of using the causal graph <span class="math inline">\(\mathcal{G}\)</span> to define the effect of treatment in the “offline” observational data. However, causal effects may not be identifiable from the observational data, so the authors use the technique of partial identification to produce causal bounds on the effects. Zhang and Bareinboim show that the introduction of informative causal bounds prior to online learning provides a dramatic improvement in both sample efficiency and the ultimate level of average regret.</p>
<p>Additional work on combining online and offline data can be found in <a href="#sec-rl-transfer-learning"><span>Section&nbsp;9.2.2.1</span></a>.</p>
</section>
<section id="action-selection-and-exploration" class="level4" data-number="9.2.1.3">
<h4 data-number="9.2.1.3" class="anchored" data-anchor-id="action-selection-and-exploration"><span class="header-section-number">9.2.1.3</span> Action Selection and Exploration</h4>
<p>Another area where causality can aid in the reinforcement learning process is within action selection. This is an interesting area of opportunity because it is applicable within both model-based and model-free RL settings.</p>
<p><span class="citation" data-cites="Seitzer2021CID">(<a href="#ref-Seitzer2021CID" role="doc-biblioref">Seitzer, Schölkopf, and Martius 2021</a>)</span> introduces the concept of <em>causal influence detection</em>, a technique that uses causal inference to better understand when an agent actually has the ability to impact its surroundings – as well as how that relationship changes over time and in different contexts. For example, consider an environment in which the agent is a robotic arm whose objective is to pick up and move an item; the agent has no influence over the position of the item unless its hand is close to the item’s location. The authors address this problem by creating a measure of <em>causal action influence</em> (CAI) which is based on an approximation of conditional mutual information within the state transition dynamics. The authors show that in robotic control environments (e.g.&nbsp;<code>FetchPickAndPlace</code> from OpenAI’s Gym), their methods provide a significant efficiency gain in terms of sample efficiency over standard <span class="math inline">\(\epsilon\)</span>-greedy action sampling. However, this method requires full observability as well as the ability to factorize the state into causal variables.</p>
<p>Another paper <span class="citation" data-cites="sun2022toward">(<a href="#ref-sun2022toward" role="doc-biblioref">Sun and Wang 2022</a>)</span> uses causal methods to prune redundant actions within continuous control environments, with the goal of improving exploration efficiency. The authors modify the temporal difference (TD) loss, which is commonly used to train the critic component of an Actor-Critic model, by using structural causal models (SCMs) to identify causally relevant actions. The experiments in this paper center around the injection of redundant or irrelevant actions into standard continuous control problems (e.g.&nbsp;<code>LunarLander</code> in the OpenAI Gym), and they illustrate that the causal methods (and the dynamic method Dyn-SWAR in particular) consistently outperform standard TD agents when redundant actions are present.</p>
</section>
<section id="robustness" class="level4" data-number="9.2.1.4">
<h4 data-number="9.2.1.4" class="anchored" data-anchor-id="robustness"><span class="header-section-number">9.2.1.4</span> Robustness</h4>
<p>Causality can also be used to improve RL agents’ resilience against interruptions or problems in their environments. In <span class="citation" data-cites="yang2021resilience">(<a href="#ref-yang2021resilience" role="doc-biblioref">Yang et al. 2021</a>)</span>, the authors use causal inference for observational interference to create an extension of deep Q-networks (DQNs) that is more robust to issues such as a blackout or lag in a visual environment. Their <em>Causal Inference Q-Network</em> (CIQ) is trained alongside a classifier of observational interference so that it can estimate during evaluation/inference whether each new observation is affected by an interference. CIQ makes uses causal inference to construct a more informative version of the interfered state, and it uses the interference label (during training) or classifier prediction (during inference) to route the modified state information to the appropriate neural network for <span class="math inline">\(Q\)</span> estimation. CIQ’s causal graph illustrates the relationship between interference and observation with the following components:</p>
<ul>
<li><span class="math inline">\(State\)</span>: the unobserved source of observations, rewards, and observational interference (e.g.&nbsp;via hardware overheating or system lag)</li>
<li><span class="math inline">\(Obs\)</span>: the actual observation received by the agent (always observed)</li>
<li><span class="math inline">\(Int\)</span>: indicator showing whether the observation has been affected by an interference (only observed/known during training)</li>
<li><span class="math inline">\(Q\)</span>: the reward value at the current timestep (always observed)</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/InferenceDAG.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
<p>During their experiments, the authors compare their method against a variety of standard DQNs as well as a DQN with a non-causal interference classifier. CIQ outperforms all baselines, with standard DQNs failing to reach acceptable performance when observational interference is introduced. Other methods with non-causal interference detection perform somewhat better than the standard baselines, but they do not reliably reach target performance. CIQ reaches target performance relatively quickly across environments with vector and pixel observations at an interference level of 20%, and its performance remains comparable to non-perturbed input until roughly 40% interference (based on the <code>Cartpole</code> environment).</p>
</section>
</section>
<section id="causality-for-transfer-learning-and-imitation-learning" class="level3" data-number="9.2.2">
<h3 data-number="9.2.2" class="anchored" data-anchor-id="causality-for-transfer-learning-and-imitation-learning"><span class="header-section-number">9.2.2</span> Causality for Transfer Learning and Imitation Learning</h3>
<p>Another area of opportunity for causality within reinforcement learning has to do with the combination of observed experience and interactive experience – that is, offline data and online interaction. This section focuses on two related but different concepts within causal RL: transfer learning (combining online RL with observational data) and imitation learning (observing and attempting to replicate another agent’s behavior). Applications in these areas involve challenging unknown factors, such as the reward model of the teaching agent in imitation learning, which sets the stage for causality to provide a benefit.</p>
<section id="sec-rl-transfer-learning" class="level4" data-number="9.2.2.1">
<h4 data-number="9.2.2.1" class="anchored" data-anchor-id="sec-rl-transfer-learning"><span class="header-section-number">9.2.2.1</span> Transfer Learning: Combining Online and Offline Information</h4>
<p>One of the biggest challenges in practical reinforcement learning is that online experimentation is often expensive, impractical, or impossible to do in meaningful applied settings. For example, it would be difficult and expensive to spend millions of hours experimenting “behind the wheel” of a self-driving car; similarly, a clinical decision support agent in a healthcare setting would raise ethical questions if it needed to test entirely random actions on actual patients. With increasing amounts of data available for use in the development of machine learning, a compelling idea arises: <em>can we use some of these datasets to help our RL agents act more efficiently, optimally, or robustly?</em> This is the root of the concept of <em>transfer learning</em> in reinforcement learning – the use of observational (or offline) data to aid in the performance of an agent in an experimental (or online) setting. However, two major challenges are present when trying to combine online and offline information; the first has to do with whether the offline data is informative to the online setting, and the second is whether differences in the data generating process can be handled by causal adjustments. The methods discussed in this section separate these challenges and show how causal techniques can assist.</p>
<p>One of the first works on using causality to combine observational and experimental data came in <span class="citation" data-cites="pmlr-v70-forney17a">(<a href="#ref-pmlr-v70-forney17a" role="doc-biblioref">Forney, Pearl, and Bareinboim 2017</a>)</span>, which proposes a counterfactual approach to the “fusion” of the two types of data within a multi-armed bandit setting. In this paper, the authors approach the problem of unobserved confounders when relating the two data sources by using a counterfactual quantity called the <em>effect of the treatment on the treated</em> (ETT) to estimate the causal effect of the action upon the reward, which they combine with empirically observed results from experimentation. Their work shows that the baseline Thompson Sampling (TS) agent which uses only experimental data performs the worst, while their proposed method (combining observational, experimental, and counterfactual data) performs the best in terms of both cumulative regret and the rate of optimal action selection.</p>
<p>Bareinboim’s work in causal transfer learning continues in <span class="citation" data-cites="ijcai2017p186">(<a href="#ref-ijcai2017p186" role="doc-biblioref">Zhang and Bareinboim 2017</a>)</span>, which applies transfer learning to multi-armed bandit settings in which causal effects cannot be identified with <em>do</em>-calculus. Zhang and Bareinboim provide an alternative approach for these settings where non-identifiability is an issue: use causal inference to derive bounds on the distribution of expected reward over the arms, and then use these bounds to identify the most promising actions. Their experiments show that when causal bounds derived from observational data are informative to the online system, causal transfer learning provides very significant efficiency gains over standard variants of Thompson Sampling and UCB. When the causal bounds are noninformative, the causal learners revert to the performance of the standard, non-causal methods (TS and UCB).</p>
<p>Moving beyond the setting of multi-armed bandits, <span class="citation" data-cites="https://doi.org/10.48550/arxiv.2106.14421">(<a href="#ref-https://doi.org/10.48550/arxiv.2106.14421" role="doc-biblioref">Gasse et al. 2021</a>)</span> follows a similar approach to Zhang and Bareinboim by creating causal bounds from a combination of observational and interventional data. However, their approach expands this idea to the world models of agents in MBRL settings, which aims to give agents a better understanding of the environment dynamics in arbitrary POMDPs.</p>
<p><span class="citation" data-cites="NEURIPS2021_b0b79da5">(<a href="#ref-NEURIPS2021_b0b79da5" role="doc-biblioref">Wang, Yang, and Wang 2021</a>)</span> introduces a value iteration approach to improve sample efficiency by combining online and offline data in settings with confounded observational data. They propose a method called deconfounded optimistic value iteration (DOVI), which uses causal inference to adjust for confounded behavior observed in offline data. Similarly to <span class="citation" data-cites="ijcai2017p186">(<a href="#ref-ijcai2017p186" role="doc-biblioref">Zhang and Bareinboim 2017</a>)</span>, the authors here show that informative observational data provably improves efficiency in the online setting.</p>
</section>
<section id="imitation-learning" class="level4" data-number="9.2.2.2">
<h4 data-number="9.2.2.2" class="anchored" data-anchor-id="imitation-learning"><span class="header-section-number">9.2.2.2</span> Imitation Learning</h4>
<p>Imitation learning differs from transfer learning in RL by focusing on learning to imitate another agent’s behavior rather than learning an original policy. This is related to transfer learning in that the high-level objective is to incorporate data produced by an external actor into the learning process, but the application is different due to the goal of behaving like another agent. Similarly to what we saw in the previous section, causality can provide a substantial benefit in dealing with tricky issues like unobservable confounders.</p>
<p>In <span class="citation" data-cites="10.5555/3495724.3496752">(<a href="#ref-10.5555/3495724.3496752" role="doc-biblioref">Zhang, Kumor, and Bareinboim 2020</a>)</span>, the authors provide a framework which approaches the imitation learning problem from a causal inference perspective. The authors begin by using a causal graph to determine whether causal imitation is feasible in a given environment: specifically, they define <em>imitability</em> as the condition when a partially observable structural causal model (POSCM) can be used to uniquely compute the desired policy. Additionally, they provide an alternative approach which can be used when strict imitability does not hold; this method uses quantitative information derived from the observed trajectories in addition to the causal graph of the environment to overcome lack of strict imitability. <span class="citation" data-cites="NEURIPS2021_7b670d55">(<a href="#ref-NEURIPS2021_7b670d55" role="doc-biblioref">Kumor, Zhang, and Bareinboim 2021</a>)</span> build upon this work by extending causal imitation learning to sequential decision-making processes. In a similar manner, the authors provide a graphical framework for identifying whether imitation is possible in a given environment.</p>
</section>
</section>
<section id="causality-for-agent-explainability-and-fairness" class="level3" data-number="9.2.3">
<h3 data-number="9.2.3" class="anchored" data-anchor-id="causality-for-agent-explainability-and-fairness"><span class="header-section-number">9.2.3</span> Causality for Agent Explainability and Fairness</h3>
<p>We close this chapter on the combination of causality and reinforcement learning with a slightly different focus: instead of using causality to guide an agent’s learning or decision making, we will explore ways that causality can provide insight into why an agent behaves the way it does. Specifically, we will outline methods for understanding agent incentives, gaining explainability in RL, and how this contributes to the principle of model fairness within an RL context.</p>
<p>Model explainability is an area of active research and significant practical interest within the machine learning community; explainable models can be easier to justify in decision-making settings, and some industry applications (for example, in financial or healthcare settings) might require transparency. Explainability in RL focuses on gaining an understanding for what motivates an agent to select a specific action at a specific point in time. <span class="citation" data-cites="Madumal_Miller_Sonenberg_Vetere_2020">(<a href="#ref-Madumal_Miller_Sonenberg_Vetere_2020" role="doc-biblioref">Madumal et al. 2020</a>)</span> introduces an approach to RL explainability which generates explanations for agents’ decisions using counterfactuals. Their method is designed for model-free RL settings and is based on an SCM which captures the relationships between state variables and the action space. In an experiment within an environment based on the video game <em>Starcraft</em>, the authors perform a human study which shows that their counterfactual explanation method is perceived as being more trustworthy than other techniques.</p>
<p>Additional work in this area focuses on <em>agent incentives</em>, or the factors which influence the agent’s decision. This differs from the broader concept of explainability by formalizing the notion that an agent can be incentivized in different ways by different things, such as self-preservation or exploiting an opponent’s weaknesses. <span class="citation" data-cites="https://doi.org/10.48550/arxiv.2102.01685">(<a href="#ref-https://doi.org/10.48550/arxiv.2102.01685" role="doc-biblioref">Everitt et al. 2021</a>)</span> presents a framework for agent incentive analysis based on structural causal influence models (SCIMs), combinations of SCMs and causal influence diagrams, which map the possible influences that exist for a given agent’s decision making process. SCIMs provide a causal view of how observations translate to decisions, which in turn produce the “utility” for the agent. The authors introduce several concepts of agent incentives modeled by the graphical relationships within the SCIM:</p>
<ul>
<li><strong>Materiality</strong>: whether the observation provides important information regarding utility nodes within the causal influence diagram</li>
<li><strong>Value of Information</strong>: whether the agent would benefit from seeing a given node prior to making the decision – this is a broader view of materiality which includes non-observable nodes</li>
<li><strong>Response Incentives</strong>: whether a node actually influences the optimal decision made by the agent
<ul>
<li>The authors show that Response Incentives are closely related to the notion of counterfactual fairness, in that a model is counterfactually unfair if a sensitive variable (e.g.&nbsp;gender, race) unduly influences the model’s output</li>
<li>Counterfactual fairness is covered in greater detail in <a href="10-fairness.html"><span>Chapter&nbsp;8</span></a></li>
</ul></li>
<li><strong>Value of Control</strong>: whether an agent benefits by actively controlling a particular node</li>
<li><strong>Instrumental Control Incentives</strong>: whether an agent manipulates a particular node <em>in order to</em> achieve a utility
<ul>
<li>This is the case if there exists a single directed path from the decision node <span class="math inline">\(D\)</span> to the utility node <span class="math inline">\(U\)</span> which only goes through the variable being manipulated by the agent <span class="math inline">\(X\)</span>: <span class="math inline">\(D \rightarrow X \rightarrow U\)</span></li>
</ul></li>
</ul>
<p><code>PyCID</code> <span class="citation" data-cites="james_fox-proc-scipy-2021">(<a href="#ref-james_fox-proc-scipy-2021" role="doc-biblioref">Fox et al. 2021</a>)</span> is a Python library which implements causal influence diagrams; working examples of the SCIM framework described in <span class="citation" data-cites="https://doi.org/10.48550/arxiv.2102.01685">(<a href="#ref-https://doi.org/10.48550/arxiv.2102.01685" role="doc-biblioref">Everitt et al. 2021</a>)</span> are available in the Jupyter notebooks on the <code>PyCID</code> <a href="https://github.com/causalincentives/pycid">GitHub</a>.</p>
<p>Explainability for an RL agent allows researchers to understand the reasons behind the agent’s actions. As discussed previously, this has significant appeal within many practical settings, but it also provides a way to assess the fairness of a given agent. Fairness in ML has many facets, and as reinforcement learning continues to find application in impactful areas, it becomes increasingly important to have tools available for understanding whether an agent is unfairly motivated. These causal tools are critical steps toward understanding agent incentives and ensuring fairness and safety in RL settings.</p>
</section>
</section>
<section id="conclusions-and-open-problems" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="conclusions-and-open-problems"><span class="header-section-number">9.3</span> Conclusions and Open Problems</h2>
<p>This chapter has given a broad view of the various intersections between causal inference and reinforcement learning. As we have seen, causal information can help agents learn more quickly, aid in transfer between offline and online settings, and provide insight into incentives. However, challenges exist which prevent the large-scale adoption of causal reinforcement learning, such as computational challenges, lack of causal identifiability in some environments, and intractability of causal methods in very high dimensional settings. Despite the challenges, the methods outlined in this chapter illustrate that causality can be a very helpful tool for improving agent performance in many RL settings.</p>
<div style="page-break-after: always;"></div>


<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-cicero2022" class="csl-entry" role="listitem">
Bakhtin, Anton, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, et al. 2022. <span>“Human-Level Play in the Game of Diplomacy by Combining Language Models with Strategic Reasoning.”</span> <em>Science</em> 0 (0): eade9097. <a href="https://doi.org/10.1126/science.ade9097">https://doi.org/10.1126/science.ade9097</a>.
</div>
<div id="ref-doi:10.1146/annurev-statistics-022513-115553" class="csl-entry" role="listitem">
Chakraborty, Bibhas, and Susan A. Murphy. 2014. <span>“Dynamic Treatment Regimes.”</span> <em>Annual Review of Statistics and Its Application</em> 1 (1): 447–64. <a href="https://doi.org/10.1146/annurev-statistics-022513-115553">https://doi.org/10.1146/annurev-statistics-022513-115553</a>.
</div>
<div id="ref-chatgpt2022" class="csl-entry" role="listitem">
<span>“ChatGPT: Optimizing Language Models for Dialogue.”</span> 2022. OpenAI. <a href="https://openai.com/blog/chatgpt/">https://openai.com/blog/chatgpt/</a>.
</div>
<div id="ref-https://doi.org/10.48550/arxiv.2102.01685" class="csl-entry" role="listitem">
Everitt, Tom, Ryan Carey, Eric Langlois, Pedro A Ortega, and Shane Legg. 2021. <span>“Agent Incentives: A Causal Perspective.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2102.01685">https://doi.org/10.48550/ARXIV.2102.01685</a>.
</div>
<div id="ref-pmlr-v70-forney17a" class="csl-entry" role="listitem">
Forney, Andrew, Judea Pearl, and Elias Bareinboim. 2017. <span>“Counterfactual Data-Fusion for Online Reinforcement Learners.”</span> In <em>Proceedings of the 34th International Conference on Machine Learning</em>, edited by Doina Precup and Yee Whye Teh, 70:1156–64. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v70/forney17a.html">https://proceedings.mlr.press/v70/forney17a.html</a>.
</div>
<div id="ref-james_fox-proc-scipy-2021" class="csl-entry" role="listitem">
Fox, James, Tom Everitt, Ryan Carey, Eric Langlois, Alessandro Abate, and Michael Wooldridge. 2021. <span>“<span>P</span>y<span>C</span><span>I</span><span>D</span>: <span>A</span> <span>P</span>ython <span>L</span>ibrary for <span>C</span>ausal <span>I</span>nfluence <span>D</span>iagrams.”</span> In <em><span>P</span>roceedings of the 20th <span>P</span>ython in <span>S</span>cience <span>C</span>onference</em>, edited by Meghann Agarwal, Chris Calloway, Dillon Niederhut, and David Shupe, 43–51. <a href="https://doi.org/10.25080/majora-1b6fd038-008">https://doi.org/10.25080/majora-1b6fd038-008</a>.
</div>
<div id="ref-https://doi.org/10.48550/arxiv.2106.14421" class="csl-entry" role="listitem">
Gasse, Maxime, Damien Grasset, Guillaume Gaudron, and Pierre-Yves Oudeyer. 2021. <span>“Causal Reinforcement Learning Using Observational and Interventional Data.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2106.14421">https://doi.org/10.48550/ARXIV.2106.14421</a>.
</div>
<div id="ref-discovery2021" class="csl-entry" role="listitem">
Ke, Nan Rosemary, Aniket Didolkar, Sarthak Mittal, Anirudh Goyal, Guillaume Lajoie, Stefan Bauer, Danilo Rezende, Yoshua Bengio, Michael Mozer, and Christopher Pal. 2021. <span>“Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2107.00848">https://doi.org/10.48550/ARXIV.2107.00848</a>.
</div>
<div id="ref-kiran2020selfdriving" class="csl-entry" role="listitem">
Kiran, B Ravi, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A. Al Sallab, Senthil Yogamani, and Patrick Pérez. 2020. <span>“Deep Reinforcement Learning for Autonomous Driving: A Survey.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2002.00444">https://doi.org/10.48550/ARXIV.2002.00444</a>.
</div>
<div id="ref-NEURIPS2021_7b670d55" class="csl-entry" role="listitem">
Kumor, Daniel, Junzhe Zhang, and Elias Bareinboim. 2021. <span>“Sequential Causal Imitation Learning with Unobserved Confounders.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. Wortman Vaughan, 34:14669–80. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2021/file/7b670d553471ad0fd7491c75bad587ff-Paper.pdf">https://proceedings.neurips.cc/paper/2021/file/7b670d553471ad0fd7491c75bad587ff-Paper.pdf</a>.
</div>
<div id="ref-levine2020offline" class="csl-entry" role="listitem">
Levine, Sergey, Aviral Kumar, George Tucker, and Justin Fu. 2020. <span>“Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2005.01643">https://doi.org/10.48550/ARXIV.2005.01643</a>.
</div>
<div id="ref-https://doi.org/10.48550/arxiv.2012.14228" class="csl-entry" role="listitem">
Li, Minne, Mengyue Yang, Furui Liu, Xu Chen, Zhitang Chen, and Jun Wang. 2020. <span>“Causal World Models by Unsupervised Deconfounding of Physical Dynamics.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2012.14228">https://doi.org/10.48550/ARXIV.2012.14228</a>.
</div>
<div id="ref-liu2018drlstock" class="csl-entry" role="listitem">
Liu, Xiao-Yang, Zhuoran Xiong, Shan Zhong, Hongyang Yang, and Anwar Walid. 2018. <span>“Practical Deep Reinforcement Learning Approach for Stock Trading.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.1811.07522">https://doi.org/10.48550/ARXIV.1811.07522</a>.
</div>
<div id="ref-pmlr-v177-lu22a" class="csl-entry" role="listitem">
Lu, Yangyi, Amirhossein Meisami, and Ambuj Tewari. 2022. <span>“Efficient Reinforcement Learning with Prior Causal Knowledge.”</span> In <em>Proceedings of the First Conference on Causal Learning and Reasoning</em>, edited by Bernhard Schölkopf, Caroline Uhler, and Kun Zhang, 177:526–41. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v177/lu22a.html">https://proceedings.mlr.press/v177/lu22a.html</a>.
</div>
<div id="ref-Madumal_Miller_Sonenberg_Vetere_2020" class="csl-entry" role="listitem">
Madumal, Prashan, Tim Miller, Liz Sonenberg, and Frank Vetere. 2020. <span>“Explainable Reinforcement Learning Through a Causal Lens.”</span> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> 34 (03): 2493–2500. <a href="https://doi.org/10.1609/aaai.v34i03.5631">https://doi.org/10.1609/aaai.v34i03.5631</a>.
</div>
<div id="ref-google2020chips" class="csl-entry" role="listitem">
Mirhoseini, Azalia, Anna Goldie, Mustafa Yazgan, Joe Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, et al. 2020. <span>“Chip Placement with Deep Reinforcement Learning.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2004.10746">https://doi.org/10.48550/ARXIV.2004.10746</a>.
</div>
<div id="ref-deepnash2022" class="csl-entry" role="listitem">
Perolat, Julien, Bart De Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul Muller, et al. 2022. <span>“Mastering the Game of Stratego with Model-Free Multiagent Reinforcement Learning.”</span> <em>Science</em> 378 (6623): 990–96. <a href="https://doi.org/10.1126/science.add4679">https://doi.org/10.1126/science.add4679</a>.
</div>
<div id="ref-https://doi.org/10.48550/arxiv.2002.02836" class="csl-entry" role="listitem">
Rezende, Danilo J., Ivo Danihelka, George Papamakarios, Nan Rosemary Ke, Ray Jiang, Theophane Weber, Karol Gregor, et al. 2020. <span>“Causally Correct Partial Models for Reinforcement Learning.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2002.02836">https://doi.org/10.48550/ARXIV.2002.02836</a>.
</div>
<div id="ref-Seitzer2021CID" class="csl-entry" role="listitem">
Seitzer, Maximilian, Bernhard Schölkopf, and Georg Martius. 2021. <span>“Causal Influence Detection for Improving Efficiency in Reinforcement Learning.”</span> In <em>Advances in Neural Information Processing Systems (NeurIPS 2021)</em>. <a href="https://arxiv.org/abs/2106.03443">https://arxiv.org/abs/2106.03443</a>.
</div>
<div id="ref-silver2017zero" class="csl-entry" role="listitem">
Silver, David, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, et al. 2017. <span>“Mastering the Game of Go Without Human Knowledge.”</span> <em>Nature</em> 550 (7676): 354–59. <a href="https://doi.org/10.1038/nature24270">https://doi.org/10.1038/nature24270</a>.
</div>
<div id="ref-deep-rl-class" class="csl-entry" role="listitem">
Simonini, Thomas, and Omar Sanseviero. 2022. <span>“The Hugging Face Deep Reinforcement Learning Class.”</span> <em>GitHub Repository</em>. <a href="https://github.com/huggingface/deep-rl-class" class="uri">https://github.com/huggingface/deep-rl-class</a>; GitHub.
</div>
<div id="ref-sun2022toward" class="csl-entry" role="listitem">
Sun, Hao, and Taiyi Wang. 2022. <span>“Toward Causal-Aware <span>RL</span>: State-Wise Action-Refined Temporal Difference.”</span> In <em>Deep Reinforcement Learning Workshop NeurIPS 2022</em>. <a href="https://openreview.net/forum?id=waLncuzMofp">https://openreview.net/forum?id=waLncuzMofp</a>.
</div>
<div id="ref-sutton2018reinforcement" class="csl-entry" role="listitem">
Sutton, R. S., and A. G. Barto. 2018. <em>Reinforcement Learning, Second Edition: An Introduction</em>. Adaptive Computation and Machine Learning Series. MIT Press. <a href="https://books.google.com/books?id=5s-MEAAAQBAJ">https://books.google.com/books?id=5s-MEAAAQBAJ</a>.
</div>
<div id="ref-NEURIPS2021_b0b79da5" class="csl-entry" role="listitem">
Wang, Lingxiao, Zhuoran Yang, and Zhaoran Wang. 2021. <span>“Provably Efficient Causal Reinforcement Learning with Confounded Observational Data.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. Wortman Vaughan, 34:21164–75. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2021/file/b0b79da57b95837f14be95aaa4d54cf8-Paper.pdf">https://proceedings.neurips.cc/paper/2021/file/b0b79da57b95837f14be95aaa4d54cf8-Paper.pdf</a>.
</div>
<div id="ref-yang2021resilience" class="csl-entry" role="listitem">
Yang, Chao-Han Huck, I-Te Danny Hung, Yi Ouyang, and Pin-Yu Chen. 2021. <span>“Training a Resilient q-Network Against Observational Interference.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2102.09677">https://doi.org/10.48550/ARXIV.2102.09677</a>.
</div>
<div id="ref-pmlr-v119-zhang20a" class="csl-entry" role="listitem">
Zhang, Junzhe. 2020. <span>“Designing Optimal Dynamic Treatment Regimes: A Causal Reinforcement Learning Approach.”</span> In <em>Proceedings of the 37th International Conference on Machine Learning</em>, edited by Hal Daumé III and Aarti Singh, 119:11012–22. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v119/zhang20a.html">https://proceedings.mlr.press/v119/zhang20a.html</a>.
</div>
<div id="ref-ijcai2017p186" class="csl-entry" role="listitem">
Zhang, Junzhe, and Elias Bareinboim. 2017. <span>“Transfer Learning in Multi-Armed Bandits: A Causal Approach.”</span> In <em>Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, <span>IJCAI-17</span></em>, 1340–46. <a href="https://doi.org/10.24963/ijcai.2017/186">https://doi.org/10.24963/ijcai.2017/186</a>.
</div>
<div id="ref-zhang2019dtr" class="csl-entry" role="listitem">
———. 2019. <span>“Near-Optimal Reinforcement Learning in Dynamic Treatment Regimes.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett. Vol. 32. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2019/file/8252831b9fce7a49421e622c14ce0f65-Paper.pdf">https://proceedings.neurips.cc/paper/2019/file/8252831b9fce7a49421e622c14ce0f65-Paper.pdf</a>.
</div>
<div id="ref-10.5555/3495724.3496752" class="csl-entry" role="listitem">
Zhang, Junzhe, Daniel Kumor, and Elias Bareinboim. 2020. <span>“Causal Imitation Learning with Unobserved Confounders.”</span> In <em>Proceedings of the 34th International Conference on Neural Information Processing Systems</em>. NIPS’20. Red Hook, NY, USA: Curran Associates Inc.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Available on the <a href="https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver">DeepMind website</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Full text available at <a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">this URL</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>See the course’s <a href="https://github.com/huggingface/deep-rl-class">official GitHub repo</a> for more information<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./10-fairness.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Fairness</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./12-references.html" class="pagination-link">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>