<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="This is a book which covers applications of causality, ranging from a practical overview of causal inference to cutting-edge applications of causality in machine learning domains.">

<title>Applied Causal Inference - 8&nbsp; Model Fairness</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./11-reinforcement-learning.html" rel="next">
<link href="./09-part-3-break.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-fairness.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Fairness</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Applied Causal Inference</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro-to-causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to Causality</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-potential-outcomes-framework.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Causal Inference: Theory and Basic Concepts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-causal-estimation-process.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Causal Inference: A Practical Approach</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-causal-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Causal Discovery</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-part-2-break.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part 2: Causality in ML Domains</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">NLP</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-computer-vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Computer Vision</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-time-dependent-causal-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Time-dependent Causal Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-part-3-break.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part 3: Advanced Topics in Causality</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-fairness.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Fairness</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-reinforcement-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction-to-model-fairness" id="toc-introduction-to-model-fairness" class="nav-link active" data-scroll-target="#introduction-to-model-fairness"><span class="header-section-number">8.1</span> Introduction to Model Fairness</a>
  <ul class="collapse">
  <li><a href="#prerequisite-knowledge" id="toc-prerequisite-knowledge" class="nav-link" data-scroll-target="#prerequisite-knowledge"><span class="header-section-number">8.1.1</span> Prerequisite Knowledge</a></li>
  <li><a href="#what-is-model-fairness" id="toc-what-is-model-fairness" class="nav-link" data-scroll-target="#what-is-model-fairness"><span class="header-section-number">8.1.2</span> What is Model Fairness?</a></li>
  </ul></li>
  <li><a href="#how-is-fairness-measured" id="toc-how-is-fairness-measured" class="nav-link" data-scroll-target="#how-is-fairness-measured"><span class="header-section-number">8.2</span> How is Fairness Measured?</a></li>
  <li><a href="#why-use-causality" id="toc-why-use-causality" class="nav-link" data-scroll-target="#why-use-causality"><span class="header-section-number">8.3</span> Why Use Causality?</a></li>
  <li><a href="#causal-fairness-measures" id="toc-causal-fairness-measures" class="nav-link" data-scroll-target="#causal-fairness-measures"><span class="header-section-number">8.4</span> Causal Fairness Measures</a>
  <ul class="collapse">
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background"><span class="header-section-number">8.4.1</span> Background</a></li>
  <li><a href="#the-standard-fairness-model" id="toc-the-standard-fairness-model" class="nav-link" data-scroll-target="#the-standard-fairness-model"><span class="header-section-number">8.4.2</span> The Standard Fairness Model</a></li>
  <li><a href="#sec-cookbook" id="toc-sec-cookbook" class="nav-link" data-scroll-target="#sec-cookbook"><span class="header-section-number">8.4.3</span> A Framework for Causal Fairness Analysis</a></li>
  </ul></li>
  <li><a href="#fairness-case-study-identifying-bias-in-the-compas-recidivism-model" id="toc-fairness-case-study-identifying-bias-in-the-compas-recidivism-model" class="nav-link" data-scroll-target="#fairness-case-study-identifying-bias-in-the-compas-recidivism-model"><span class="header-section-number">8.5</span> Fairness Case Study: Identifying Bias in the COMPAS Recidivism Model</a>
  <ul class="collapse">
  <li><a href="#framing-the-problem-non-causal-estimates" id="toc-framing-the-problem-non-causal-estimates" class="nav-link" data-scroll-target="#framing-the-problem-non-causal-estimates"><span class="header-section-number">8.5.1</span> Framing the Problem + Non-Causal Estimates</a></li>
  <li><a href="#causal-measures" id="toc-causal-measures" class="nav-link" data-scroll-target="#causal-measures"><span class="header-section-number">8.5.2</span> Causal Measures</a></li>
  </ul></li>
  <li><a href="#additional-topics" id="toc-additional-topics" class="nav-link" data-scroll-target="#additional-topics"><span class="header-section-number">8.6</span> Additional Topics</a>
  <ul class="collapse">
  <li><a href="#enforcing-fairness-during-model-training" id="toc-enforcing-fairness-during-model-training" class="nav-link" data-scroll-target="#enforcing-fairness-during-model-training"><span class="header-section-number">8.6.1</span> Enforcing Fairness During Model Training</a></li>
  <li><a href="#fairness-in-reinforcement-learning" id="toc-fairness-in-reinforcement-learning" class="nav-link" data-scroll-target="#fairness-in-reinforcement-learning"><span class="header-section-number">8.6.2</span> Fairness in Reinforcement Learning</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-model-fairness" class="quarto-section-identifier"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Fairness</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>The ethical implications of widespread ML adoption in practice could easily fill this entire book. Models have an increasingly significant impact on human lives — from the content a user sees on social media to credit approval and health outcomes. As such, it is paramount that practitioners approach model creation and deployment in an intentionally ethical manner.</p>
<p>With this in mind, much has already been written about ML ethics more broadly; for the sake of maintaining a focus on practical causal techniques, we will not rehash the high-level concepts around ethical concerns of machine learning applications in full. We will instead assume the reader has some familiarity with these ethical issues, introduce necessary definitions, and focus on how causality can be used to measure and mitigate algorithmic bias in practice.</p>
<p>Specifically, this section covers causal techniques that aid in the assessment of model fairness as well as the creation of fair models. We will discuss the need for causal methods instead of standard statistical tools when evaluating potentially discriminatory effects.</p>
<section id="introduction-to-model-fairness" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="introduction-to-model-fairness"><span class="header-section-number">8.1</span> Introduction to Model Fairness</h2>
<section id="prerequisite-knowledge" class="level3" data-number="8.1.1">
<h3 data-number="8.1.1" class="anchored" data-anchor-id="prerequisite-knowledge"><span class="header-section-number">8.1.1</span> Prerequisite Knowledge</h3>
<p>In this chapter, we will assume that the reader has a general understanding of issues surrounding algorithmic bias. If this is not the case for you, we recommend Chapter 3 of the <code>fast.ai</code> book <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span class="citation" data-cites="howard2020deep">(<a href="#ref-howard2020deep" role="doc-biblioref">Howard and Gugger 2020</a>)</span> as a starting point.</p>
<p>For a deeper view, <span class="citation" data-cites="mehrabi2019bias">(<a href="#ref-mehrabi2019bias" role="doc-biblioref">Mehrabi et al. 2019</a>)</span> provides a thorough survey of these topics from the machine learning perspective, including helpful distinctions between the types of bias which stem from datasets, the algorithms themselves, deployment techniques, and environmental or social factors (and how these concepts differ from the purely statistical notion of <em>bias</em>). We will rely on these definitions throughout this chapter, as described in the next section.</p>
<p>We also assume familiarity with causal inference techniques which can be gained in earlier chapters of this book; specifically, the causal estimation process outlined in <a href="03-causal-estimation-process.html"><span>Chapter&nbsp;3</span></a>.</p>
</section>
<section id="what-is-model-fairness" class="level3" data-number="8.1.2">
<h3 data-number="8.1.2" class="anchored" data-anchor-id="what-is-model-fairness"><span class="header-section-number">8.1.2</span> What is Model Fairness?</h3>
<p>The term <em>algorithmic bias</em> has become somewhat of an umbrella term for general concepts around unethical applications of technology: for example, a recent blog post by Liberties EU <span class="citation" data-cites="liberties_eu_2021">(<a href="#ref-liberties_eu_2021" role="doc-biblioref"><span>“Algorithmic Bias: Why and How Do Computers Make Unfair Decisions?”</span> 2021</a>)</span> provides a definition of algorithmic bias that ranges from the infamous COMPAS recidivism algorithm to airbag systems being better adapted for male bodies than female bodies. Such a definition helps draw attention to disparities, but it is not precise enough to distinguish issues which are specifically relevant to machine learning</p>
<p>In this chapter, we use <em>model fairness</em> to focus on the concepts of algorithmic bias directly related to the development, training, and deployment of machine learning models: specifically that a ML model or system may produce outputs which perpetuate, reinforce, or amplify discriminatory behavior; and that the model’s quality can vary across groups. The definitions in section 4 of <span class="citation" data-cites="mehrabi2019bias">(<a href="#ref-mehrabi2019bias" role="doc-biblioref">Mehrabi et al. 2019</a>)</span> provide a nuanced view of the aspects of fairness in machine learning, including the categorization of fairness at the individual (similar individuals should receive similar outcomes) and group (different groups should be treated equally) levels.</p>
</section>
</section>
<section id="how-is-fairness-measured" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="how-is-fairness-measured"><span class="header-section-number">8.2</span> How is Fairness Measured?</h2>
<p>Both <span class="citation" data-cites="mehrabi2019bias">(<a href="#ref-mehrabi2019bias" role="doc-biblioref">Mehrabi et al. 2019</a>)</span> and <span class="citation" data-cites="caton2020fairness">(<a href="#ref-caton2020fairness" role="doc-biblioref">Caton and Haas 2020</a>)</span> categorize the wide variety of existing fairness quantification methods. Each method provides a slightly different approach to the quantification of model fairness, and each comes with its own assumptions about the user’s responsibility for assessing fairness: as an extreme example, one approach named “Fairness Through Unawareness” – described in <span class="citation" data-cites="kusner2017counterfactual">(<a href="#ref-kusner2017counterfactual" role="doc-biblioref">Kusner et al. 2017</a>)</span> – asserts that a model is fair as long as it does not explicitly use protected attributes as features. Most techniques, however, recognize that reasonably complex models have the ability to proxy protected attributes through more subtle interactions. Examples of such definitions are “Demographic Parity” and “Equality of Opportunity,” which are also defined in <span class="citation" data-cites="kusner2017counterfactual">(<a href="#ref-kusner2017counterfactual" role="doc-biblioref">Kusner et al. 2017</a>)</span>.</p>
<p>Many of these methods focus on the joint distribution of model errors with protected categories (and, in some cases, other relevant covariates). One example is <em>conditional statistical parity</em>, originally <span class="citation" data-cites="corbett2017algorithmic">(<a href="#ref-corbett2017algorithmic" role="doc-biblioref">Corbett-Davies et al. 2017</a>)</span>, which states that, “For a set of legitimate factors <span class="math inline">\(L\)</span>, predictor <span class="math inline">\(\hat{Y}\)</span> satisfies conditional statistical parity if <span class="math inline">\(P(\hat{Y} |L=1,A = 0) = P(\hat{Y}|L=1,A = 1)\)</span>” <span class="citation" data-cites="mehrabi2019bias">(<a href="#ref-mehrabi2019bias" role="doc-biblioref">Mehrabi et al. 2019</a>)</span>. Essentially, conditional statistical parity attempts to measure whether individuals in different groups have similar outcomes (e.g.&nbsp;likelihood of being correctly classified) when controlling for relevant covariates.</p>
<p>In addition to these aggregate methods, some fairness measures focus on the similarity of predictions among similar individuals, while others measure the quality of the model’s calibration among individuals in differing groups.</p>
<p>Unfortunately, practical adoption of fairness measurements is not yet widespread at this time, and metric selection is inconsistent from one use case to the next. In other words, relatively few practitioners are using fairness methods at all, and when they do, they use different techniques with different assumptions. Before we dive into the current state of causal fairness metrics, we will take a moment to discuss why practitioners and researchers should choose causal methods over other techniques.</p>
</section>
<section id="why-use-causality" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="why-use-causality"><span class="header-section-number">8.3</span> Why Use Causality?</h2>
<p>With such a wide variety of available fairness evaluation methods, one could reasonably question whether causality should be preferred over “standard” non-causal techniques.</p>
<p>First, the statistical notion of omitted variable bias means that algorithmic fairness/bias estimates may be flawed if the user does not control for the necessary covariates. Simpson’s Paradox describes reversals in estimated effects when changing the conditioning variables <span class="citation" data-cites="sep-paradox-simpson">(<a href="#ref-sep-paradox-simpson" role="doc-biblioref">Sprenger and Weinberger 2021</a>)</span>, and such reversals can be disastrous when assessing possible discriminatory treatment by ML systems. For example, if one wishes to use conditional statistical parity to evaluate discriminatory behavior, the definition of “legitimate factors” may be up for debate, and the decision to include or exclude certain factors can significantly impact results. The practice of creating a graphical model for causal inference enumerates all assumptions about how the world works with respect to the analysis at hand <span class="citation" data-cites="pearl2016causal">(<a href="#ref-pearl2016causal" role="doc-biblioref">J. Pearl, Glymour, and Jewell 2016</a>)</span>. The causal graph creates a reference point for the foundational assumptions that underlie a fairness analysis, and it can be debated and refined as needed by subject matter experts.</p>
<p>The other major benefit of framing model fairness questions as causal inference tasks is that we gain understanding for the actual mechanisms of bias. A correlational approach can describe whether an observed discrepancy exists, but cannot tell you why it exists or the degree to which such a discrepancy is actually causal in nature. This makes the use of causal language extremely important when describing unethical treatment: it allows us to articulate the mechanisms through which an algorithm might propagate biased outcomes.</p>
<p>To summarize, the answer to the question of <em>“why should I use causality?”</em> is ultimately the same for applications in model fairness as it is for any other use of causal inference:</p>
<ol type="1">
<li>The creation of a causal graph formalizes assumptions about how the world works (in this case, how bias might be perpetuated by a machine learning model or system)</li>
<li>Framing fairness analysis as a causal inference task allows us to understand and directly quantify mechanisms of bias instead of simply recognizing that a discrepancy exists</li>
</ol>
</section>
<section id="causal-fairness-measures" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="causal-fairness-measures"><span class="header-section-number">8.4</span> Causal Fairness Measures</h2>
<section id="background" class="level3" data-number="8.4.1">
<h3 data-number="8.4.1" class="anchored" data-anchor-id="background"><span class="header-section-number">8.4.1</span> Background</h3>
<p>The first concerted efforts to estimate model fairness with causal methods involved framing the fairness problem in the same way as any other causal inference task and using existing metrics. <span class="citation" data-cites="kusner2017counterfactual">(<a href="#ref-kusner2017counterfactual" role="doc-biblioref">Kusner et al. 2017</a>)</span> suggests using Effect of Treatment on the Treated (ETT) <span class="citation" data-cites="pearl2009overview">(<a href="#ref-pearl2009overview" role="doc-biblioref">Judea Pearl 2009</a>)</span> to measure causal effect. However, ETT does not distinguish the effect that is caused specifically by the treatment as opposed to backdoor channels, so later work – as described in <span class="citation" data-cites="10.5555/3504035.3504283">(<a href="#ref-10.5555/3504035.3504283" role="doc-biblioref">Zhang and Bareinboim 2018</a>)</span> – used specific measures to highlight the mechanisms of discriminatory effect. This work used three causal measures to account for this distinction:</p>
<ul>
<li>Controlled direct effect (CDE) describes the effect of the treatment <span class="math inline">\(X\)</span> on the outcome <span class="math inline">\(Y\)</span> while holding everything else constant (hence the <em>controlled</em> aspect). As the name suggests, this “direct” measure intends to quantify the disparity that is due explicitly to <span class="math inline">\(X\)</span> which, in the fairness setting, would be a protected category such as gender, race, or religion.</li>
<li>Natural direct effect (NDE) is slightly different from CDE in that it describes the direct effect of <span class="math inline">\(X\rightarrow Y\)</span> when any mediators <span class="math inline">\(W\)</span> in <span class="math inline">\(X\rightarrow W \rightarrow Y\)</span> are set to the values they would naturally obtain when intervening on <span class="math inline">\(X\)</span></li>
<li>Natural indirect effect (NIE) measures the change in the outcome <span class="math inline">\(Y\)</span> that is due to shifts in the mediators <span class="math inline">\(W\)</span> while the treatment <span class="math inline">\(X\)</span> remains constant.</li>
</ul>
<p>These three metrics can be used to describe the amount and the nature of unfair treatment in many fairness settings. However, the underlying assumptions only hold when <span class="math inline">\(X\)</span> does not have another parent node in the causal graph; if this is not the case, other sources of discrimination may exist which would not be captured by CDE, NDE, and NIE.</p>
</section>
<section id="the-standard-fairness-model" class="level3" data-number="8.4.2">
<h3 data-number="8.4.2" class="anchored" data-anchor-id="the-standard-fairness-model"><span class="header-section-number">8.4.2</span> The Standard Fairness Model</h3>
<p>It is often the case that researchers interested in studying potentially discriminatory behavior of a model may not have access to the inner workings of the model (e.g.&nbsp;proprietary data or system dynamics), which poses challenges for robust causal inference using standard tools. In <span class="citation" data-cites="10.5555/3504035.3504283">(<a href="#ref-10.5555/3504035.3504283" role="doc-biblioref">Zhang and Bareinboim 2018</a>)</span>, the authors introduce the <em>Standard Fairness Model</em>, a causal model to be used generally for questions concerning ML model fairness – including when information about the model’s decision-making process is limited.</p>
<p>The causal graph for the Standard Fairness Model is given in <a href="#fig-sfm-generic">Figure&nbsp;<span>8.1</span></a>, where <span class="math inline">\(X\)</span> is the protected category, <span class="math inline">\(Y\)</span> is the outcome, <span class="math inline">\(W\)</span> is a possible set of mediators between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and <span class="math inline">\(Z\)</span> is a possible set of confounders between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-sfm-generic" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/SFMGeneric.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;8.1: Causal graph for the Standard Fairness Model</figcaption>
</figure>
</div>
</div>
</div>
<p>Along with this causal model, Zhang and Bareinboim introduce a suite of counterfactual measures that account for the specific types of discriminatory effect. These quantities are similar in nature to CDE/NDE/NIE in that they quantify both direct and indirect effect, but Zhang and Bareinboim address the limitations of CDE/NDE/NIE by introducing a dedicated measure for the spurious effect that arises when confounders are present (i.e.&nbsp;<span class="math inline">\(X \leftarrow Z \rightarrow Y\)</span>):</p>
<ul>
<li>Ctf-DE: the counterfactual <em>direct</em> effect of the treatment <span class="math inline">\(X\)</span> on the outcome <span class="math inline">\(Y\)</span>. This measures how <span class="math inline">\(Y\)</span> changes when <span class="math inline">\(X\)</span> goes from <span class="math inline">\(x_0\rightarrow x_1\)</span> while mediators <span class="math inline">\(W\)</span> retain the values they would have naturally attained at <span class="math inline">\(X=x_0\)</span>. This is similar in nature to NDE, but it simply tries to capture “the existence of any disparate treatment” such that when <span class="math inline">\(\text{Ctf-DE}\neq0\)</span>, then there is evidence to indicate that some amount of disparity exists which is due directly to the protected category <span class="math inline">\(X\)</span></li>
<li>Ctf-IE: the counterfactual <em>indirect</em> effect of the treatment <span class="math inline">\(X\)</span> on the outcome <span class="math inline">\(Y\)</span>. This is essentially a flipped version of Ctf-DE, meaning that instead of shifting <span class="math inline">\(X\)</span> from <span class="math inline">\(x_0\rightarrow x_1\)</span>, the mediators <span class="math inline">\(W\)</span> shift to what they would have naturally attained at <span class="math inline">\(X=x_1\)</span> while <span class="math inline">\(X\)</span> remains at <span class="math inline">\(x_0\)</span>. The counterfactual indirect effect seeks to measure discrimination through backdoor channels.</li>
<li>Ctf-SE: the counterfactual <em>spurious</em> effect measures the residual or leftover effect which is due to confounders, or common ancestors, of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. This is the missing component of the CDE/NDE/NIE framework, since it allows us to quantify differences due to spurious relationships between the protected factor and the outcome. Cases where no discrimination exists will have DE and IE equal to 0, with the entirety of the observed variation allocated to spurious effect.</li>
</ul>
<p>The formulas for these measures are given below:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Ctf-DE}_{x_0, x_1}(y|x) &amp;= P(y_{x_1, W_{x_0}}|x) - P(y_{x_0}|x) \\
\text{Ctf-IE}_{x_0, x_1} (y|x) &amp;= P(y_{x_0, W_{x_1}}|x) - P(y_{x_0}|x)\\
\text{Ctf-SE}_{x_0, x_1} (y) &amp;= P(y_{x_0}|x_1) - P(y|x_0)
\end{aligned}
\]</span></p>
<p>If certain assumptions hold – namely those outlined in the relations of the variables as outlined in the Standard Fairness Model – these measures are directly estimable from observational data. The identification formulas for these metrics are given here:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Ctf-DE}_{x_0, x_1}(y|x) &amp;= \sum_{z, w}\big[P(y|x_1, z, w) - P(y|x_0, z, w) \big] P(w|x_0, z)P(z | x) \\
\text{Ctf-IE}_{x_0, x_1} (y|x) &amp;= \sum_{z, w}P(y|x_0, z, w)\big[P(w|x_1, z) - P(w|x_0, z) \big]  P(z|x) \\
\text{Ctf-SE}_{x_0, x_1} (y) &amp;= \sum_z P(y|x_0, z) \big[P(z|x_0) - P(z|x_1) \big]
\end{aligned}
\]</span></p>
<p>These identification formulas provide a way to calculate discriminatory effect using observational data, which greatly simplifies the causal estimation in cases where the Standard Fairness Model applies.</p>
<p>One attractive quality of this set of measures is that they provide a decomposition for the total observed disparity. In many cases, this high-level, non-causal difference in outcomes between protected groups – <span class="math inline">\(TV=P(y|x_1) - P(y|x_0)\)</span> – is the first indicator that something may be amiss. These counterfactual measures break the total variation down into distinct components which quantify both the amount and the nature of any discriminatory effect that exists in the dataset. The decomposition of <span class="math inline">\(TV\)</span> is provided here:</p>
<p><span class="math display">\[
TV_{x_0, x_1}(y) = DE_{x_0, x_1}(y|x_0) - SE_{x_1, x_0}(y) - IE_{x_1, x_0}(y|x_0)
\]</span></p>
</section>
<section id="sec-cookbook" class="level3" data-number="8.4.3">
<h3 data-number="8.4.3" class="anchored" data-anchor-id="sec-cookbook"><span class="header-section-number">8.4.3</span> A Framework for Causal Fairness Analysis</h3>
<p>In their latest work, Plečko and Bareinboim build upon the Standard Fairness Model to propose a standardized process for assessing model fairness with causality. We will provide a practical introduction to their tools, but we suggest that readers interested in going deeper with causal fairness techniques refer to their paper <span class="citation" data-cites="pleckobareinboim2022">(<a href="#ref-pleckobareinboim2022" role="doc-biblioref">Plečko and Bareinboim 2022</a>)</span> and <a href="https://fairness.causalai.net/">website</a> for further details as well as continued updates to the extended version of the <em>Fairness Cookbook</em>.</p>
<p>Plečko and Bareinboim begin by formalizing what they call the <em>Fundamental Problem of Causal Fairness Analysis</em>, which is essentially the need to decompose the total variation in the observed outcomes into more meaningful causal measures. They provide a toolkit for approaching this problem by organizing existing causal fairness metrics (including ETT, CDE, NDE, NIE, and Ctf-DE/IE/SE) into a hierarchy ranging in granularity on the population level (from the total population to an individual unit) as well as in the mechanism of the disparity (including causal, spurious, direct, and indirect). Additionally, the Fairness Map shows how metrics relate to and build upon each other, as well as the types of quantifications that are not possible. The Fairness Map in <span class="citation" data-cites="pleckobareinboim2022">(<a href="#ref-pleckobareinboim2022" role="doc-biblioref">Plečko and Bareinboim 2022</a>)</span> is provided below:</p>
<p><img src="img/fairness-map.png" class="img-fluid"></p>
<section id="the-fairness-cookbook" class="level4" data-number="8.4.3.1">
<h4 data-number="8.4.3.1" class="anchored" data-anchor-id="the-fairness-cookbook"><span class="header-section-number">8.4.3.1</span> The Fairness Cookbook</h4>
<p>Plečko and Bareinboim continue by introducing the <em>Fairness Cookbook</em>, which is a step-by-step process designed to be used by analysts seeking to answer questions of fairness with causal tools. A condensed version of the Fairness Cookbook is provided here, but readers are encouraged to explore the specific examples available in <span class="citation" data-cites="pleckobareinboim2022">(<a href="#ref-pleckobareinboim2022" role="doc-biblioref">Plečko and Bareinboim 2022</a>)</span>:</p>
<ol type="1">
<li>Obtain the dataset</li>
<li>Determine the Standard Fairness Model projection
<ul>
<li>Define which variables in your dataset map to variable sets <span class="math inline">\(W\)</span> and <span class="math inline">\(Z\)</span> in addition to the treatment <span class="math inline">\(X\)</span> and outcome <span class="math inline">\(Y\)</span></li>
<li>Also assess whether bidirected edges exist between these variable groups – if so, more work will be required to estimate causal impacts</li>
</ul></li>
<li>Assess disparate treatment
<ul>
<li>Use one of the <span class="math inline">\(-DE\)</span> methods to quantify the direct effect</li>
</ul></li>
<li>Assess disparate impact
<ul>
<li>Use one of the <span class="math inline">\(-IE\)</span> methods to quantify indirect effect</li>
</ul></li>
</ol>
</section>
</section>
</section>
<section id="fairness-case-study-identifying-bias-in-the-compas-recidivism-model" class="level2" data-number="8.5">
<h2 data-number="8.5" class="anchored" data-anchor-id="fairness-case-study-identifying-bias-in-the-compas-recidivism-model"><span class="header-section-number">8.5</span> Fairness Case Study: Identifying Bias in the COMPAS Recidivism Model</h2>
<p>Our case study for evaluating model fairness uses the COMPAS dataset. COMPAS is an infamous model which was the subject of an <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">analysis by ProPublica</a> revealing its discriminatory bias against black defendants, resulting in longer sentencing than their white colleagues. The full Python code used in this case study is available in <a href="https://colab.research.google.com/drive/1bviQfw1BWtgKw4O-XXPAqXb-t7vtMRls?usp=sharing">this Colab notebook</a>.</p>
<section id="framing-the-problem-non-causal-estimates" class="level3" data-number="8.5.1">
<h3 data-number="8.5.1" class="anchored" data-anchor-id="framing-the-problem-non-causal-estimates"><span class="header-section-number">8.5.1</span> Framing the Problem + Non-Causal Estimates</h3>
<p>The ProPublica analysis uses traditional statistical tools to investigate the disparity in model output by the defendant’s race, and they showed that the algorithm produces higher predicted likelihood of recidivism for black defendants than white defendants. <a href="#fig-compas-pred">Figure&nbsp;<span>8.2</span></a> illustrates the high-level difference in the percentage of defendants predicted as “Medium” or “High” recidivism risk.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-compas-pred" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/CompasPredictionsByRace.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;8.2: Difference in COMPAS Recidivism Predictions by Race</figcaption>
</figure>
</div>
</div>
</div>
<p>As we can see, there is a large difference in model output between the two groups: black defendants are predicted as Medium or High risk at a much higher rate than their white counterparts. However, it is possible that there are underlying factors contributing to this trend, such as the nature of the charge. The published analysis uses a generalized linear model to control for certain covariates when estimating the discriminatory effect of race on model predictions: specifically, the authors control for the defendant’s sex, age, charge degree (felony or misdemeanor), and number of prior convictions, while interpreting the coefficient for race as the discriminatory effect. This method estimates that black defendants are 56.6% more likely to be predicted as Medium or High risk than similar white defendants. Since model output is used directly in sentencing, this would be an alarming finding.</p>
<p>Another way to assess model fairness is by directly modeling some relevant performance metric to determine whether the model performs suboptimally on certain groups. We can see in <a href="#fig-compas-acc">Figure&nbsp;<span>8.3</span></a> that there is, again, a large difference in outcomes by race.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-compas-acc" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/CompasAccuracyByRace.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;8.3: Difference in COMPAS Model Accuracy by Race</figcaption>
</figure>
</div>
</div>
</div>
<p>A similar non-causal approach can be taken here, with the indicator of model correctness (i.e.&nbsp;whether the model accurately predicted a defendant as Low or Medium/High risk) serving as the dependent variable instead of the binarized model score. Controlling for the same factors, this model estimates that the model is correct 10.5% less often for black defendants than white defendants. This also would be a concerning result, indicating that the model does not perform as well when presented with data from black defendants.</p>
</section>
<section id="causal-measures" class="level3" data-number="8.5.2">
<h3 data-number="8.5.2" class="anchored" data-anchor-id="causal-measures"><span class="header-section-number">8.5.2</span> Causal Measures</h3>
<p>The previous section illustrates two great ways of framing the problem of model fairness, by targeting one or both of the following quantities:</p>
<ol type="1">
<li>Model predictions</li>
<li>Performance metrics (e.g.&nbsp;accuracy, precision, recall)</li>
</ol>
<p>These two pieces of information represent the primary aspects of a model-driven decision process. The problem, as we have seen throughout this book, is that we cannot be sure of the causal interpretation of the estimates produced by the linear models in the previous section. The only way we can have this confidence is with causal inference, starting with the construction of a causal graph. We use the COMPAS graph described in <span class="citation" data-cites="pleckobareinboim2022">(<a href="#ref-pleckobareinboim2022" role="doc-biblioref">Plečko and Bareinboim 2022</a>)</span>, which is shown in <a href="#fig-compas-graph">Figure&nbsp;<span>8.4</span></a> from the <code>DoWhy</code> causal model visualization:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-compas-graph" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/CompasGraph.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;8.4: Causal Graph for COMPAS Predictions</figcaption>
</figure>
</div>
</div>
</div>
<p>After the graph is constructed, we care mostly about the edge between <code>race</code> and <code>predicted_recid</code>, which estimates the direct discriminatory effect based on the defendant’s race. As you might expect, using <code>DoWhy</code> to repeat the analysis with causal estimates produces different results compared to the non-causal estimates. This table compares the estimates produced by the causal and non-causal estimates.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Outcome</th>
<th style="text-align: center;">Causal Effect</th>
<th style="text-align: center;">Non-Causal Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Predicted recidivism</td>
<td style="text-align: center;">+21.8%</td>
<td style="text-align: center;">+56.6%</td>
</tr>
<tr class="even">
<td style="text-align: center;">Model accuracy</td>
<td style="text-align: center;">-11.6%</td>
<td style="text-align: center;">-10.5%</td>
</tr>
</tbody>
</table>
<p>Both approaches agree that there is some degree of quantifiable racial bias in the COMPAS algorithm, but there is a difference in the amount that is due to race. We don’t see a major reversal in trends like in the Simpson’s Paradox examples, but taking the time to frame the analysis in terms of causal relationships gives us more confidence in quantifying true causal effects.</p>
</section>
</section>
<section id="additional-topics" class="level2" data-number="8.6">
<h2 data-number="8.6" class="anchored" data-anchor-id="additional-topics"><span class="header-section-number">8.6</span> Additional Topics</h2>
<section id="enforcing-fairness-during-model-training" class="level3" data-number="8.6.1">
<h3 data-number="8.6.1" class="anchored" data-anchor-id="enforcing-fairness-during-model-training"><span class="header-section-number">8.6.1</span> Enforcing Fairness During Model Training</h3>
<p>In addition to assessing whether an existing model or ML system is behaving unfairly, recent work has incorporated fairness directly into the model-building process. <span class="citation" data-cites="distefano2020">(<a href="#ref-distefano2020" role="doc-biblioref">Di Stefano, Hickey, and Vasileiou 2020</a>)</span> introduces a loss penalty based upon the controlled direct effect (CDE) measuring the disparity one wishes to minimize. In <a href="#fig-mfcde-dag">Figure&nbsp;<span>8.5</span></a> we show the causal diagram that is assumed to hold for the process which generated the data used in the ML task. The authors use <span class="math inline">\(Z\)</span> as the protected category, <span class="math inline">\(Y\)</span> as the task label (e.g.&nbsp;0/1 in binary classification), and <span class="math inline">\(\mathbf{X}\)</span> as covariates in the model.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-mfcde-dag" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/Mfcde.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;8.5: Causal graph for MFCDE</figcaption>
</figure>
</div>
</div>
</div>
<p>One important assumption encoded into this causal model is that no confounder exists which would open a backdoor path between the protected category <span class="math inline">\(Z\)</span> and the label <span class="math inline">\(Y\)</span>. This is likely the case for many contexts of fairness — for instance, there is no common cause between a person’s race and whether the person clicks on an ad, gets into a traffic collision, or defaults on a loan — but this assumption should be validated for each use.</p>
<p>The authors proceed by framing the problem of training a fair model as the process of building a model that does not learn the CDE between <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span>. A significant challenge with such an approach is that many popular machine learning algorithms require differentiable loss functions; the authors address this by leveraging propensity scoring from a surrogate model to minimize the CDE via mean-field approximation (MFCDE) along with the task’s original loss (e.g.&nbsp;binary cross-entropy). This is formally defined as the combination of a regularization weight <span class="math inline">\(\lambda\)</span> with the original loss <span class="math inline">\(\mathcal{L_o}\)</span> and a differentiable fairness penalty <span class="math inline">\(\mathcal{R_f}\)</span>:</p>
<p><span class="math display">\[ \mathcal{L}_f =  (1-\lambda)\mathcal{L_o} + \lambda\mathcal{R_f}\]</span></p>
<p>The fairness penalty is derived using propensity matching to calculate the MFCDE iteratively as the model updates. This happens via a surrogate model that uses the propensity scores and the protected attribute as inputs to predict the model’s output at that iteration – this connection to the model being trained provides the differentiable component required to use the fair loss as a gradient-based optimization target.</p>
<p>The drawback of such an approach is that the surrogate model must be updated at each iteration, which adds computational overhead: both from surrogate model training and the need to provide updated predictions to be used as the surrogate model’s training set. However, the authors show that a model trained to minimize MFCDE does show practical reduction in the CDE measuring the disparity of interest. The authors also note that, for higher values of <span class="math inline">\(\lambda\)</span>, it may be beneficial to “pre-train” the classifier using smaller values of <span class="math inline">\(\lambda\)</span> before increasing incrementally to the desired level of regularization.</p>
</section>
<section id="fairness-in-reinforcement-learning" class="level3" data-number="8.6.2">
<h3 data-number="8.6.2" class="anchored" data-anchor-id="fairness-in-reinforcement-learning"><span class="header-section-number">8.6.2</span> Fairness in Reinforcement Learning</h3>
<p>Measuring fair outcomes in a reinforcement learning setting could be done using the techniques outlined in the Fairness Cookbook (<a href="#sec-cookbook"><span>Section&nbsp;8.4.3</span></a>) given that the model is already trained and one simply wants to assess whether it is behaving fairly. However, since assumptions and behaviors when training RL agents differ significantly from supervised learning, additional topics on fairness in reinforcement learning will be covered in <a href="11-reinforcement-learning.html"><span>Chapter&nbsp;9</span></a>.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-liberties_eu_2021" class="csl-entry" role="listitem">
<span>“Algorithmic Bias: Why and How Do Computers Make Unfair Decisions?”</span> 2021. <em>Liberties.eu</em>. Liberties EU. <a href="https://www.liberties.eu/en/stories/algorithmic-bias-17052021/43528">https://www.liberties.eu/en/stories/algorithmic-bias-17052021/43528</a>.
</div>
<div id="ref-caton2020fairness" class="csl-entry" role="listitem">
Caton, Simon, and Christian Haas. 2020. <span>“Fairness in Machine Learning: A Survey.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2010.04053">https://doi.org/10.48550/ARXIV.2010.04053</a>.
</div>
<div id="ref-corbett2017algorithmic" class="csl-entry" role="listitem">
Corbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017. <span>“Algorithmic Decision Making and the Cost of Fairness.”</span> In <em>Proceedings of the 23rd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining</em>, 797–806.
</div>
<div id="ref-distefano2020" class="csl-entry" role="listitem">
Di Stefano, Pietro G., James M. Hickey, and Vlasios Vasileiou. 2020. <span>“Counterfactual Fairness: Removing Direct Effects Through Regularization.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2002.10774">https://doi.org/10.48550/ARXIV.2002.10774</a>.
</div>
<div id="ref-howard2020deep" class="csl-entry" role="listitem">
Howard, J., and S. Gugger. 2020. <em>Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD</em>. O’Reilly Media, Incorporated. <a href="https://books.google.no/books?id=xd6LxgEACAAJ">https://books.google.no/books?id=xd6LxgEACAAJ</a>.
</div>
<div id="ref-kusner2017counterfactual" class="csl-entry" role="listitem">
Kusner, Matt J, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. <span>“Counterfactual Fairness.”</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div>
<div id="ref-mehrabi2019bias" class="csl-entry" role="listitem">
Mehrabi, Ninareh, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2019. <span>“A Survey on Bias and Fairness in Machine Learning.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.1908.09635">https://doi.org/10.48550/ARXIV.1908.09635</a>.
</div>
<div id="ref-pearl2016causal" class="csl-entry" role="listitem">
Pearl, J., M. Glymour, and N. P. Jewell. 2016. <em>Causal Inference in Statistics: A Primer</em>. Wiley. <a href="https://books.google.com/books?id=L3G-CgAAQBAJ">https://books.google.com/books?id=L3G-CgAAQBAJ</a>.
</div>
<div id="ref-pearl2009overview" class="csl-entry" role="listitem">
Pearl, Judea. 2009. <span>“Causal Inference in Statistics: An Overview.”</span> <em>Statistics Surveys</em> 3 (January): 96–146. <a href="https://doi.org/10.1214/09-SS057">https://doi.org/10.1214/09-SS057</a>.
</div>
<div id="ref-pleckobareinboim2022" class="csl-entry" role="listitem">
Plečko, Drago, and Elias Bareinboim. 2022. <span>“Causal Fairness Analysis.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2207.11385">https://doi.org/10.48550/ARXIV.2207.11385</a>.
</div>
<div id="ref-sep-paradox-simpson" class="csl-entry" role="listitem">
Sprenger, Jan, and Naftali Weinberger. 2021. <span>“<span class="nocase">Simpson’s Paradox</span>.”</span> In <em>The <span>Stanford</span> Encyclopedia of Philosophy</em>, edited by Edward N. Zalta, <span>S</span>ummer 2021. <a href="https://plato.stanford.edu/archives/sum2021/entries/paradox-simpson/" class="uri">https://plato.stanford.edu/archives/sum2021/entries/paradox-simpson/</a>; Metaphysics Research Lab, Stanford University.
</div>
<div id="ref-10.5555/3504035.3504283" class="csl-entry" role="listitem">
Zhang, Junzhe, and Elias Bareinboim. 2018. <span>“Fairness in Decision-Making — the Causal Explanation Formula.”</span> In. AAAI’18/IAAI’18/EAAI’18. New Orleans, Louisiana, USA: AAAI Press.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>This chapter, along with the rest of the fast.ai book, is also available in Jupyter Notebook format on their <a href="https://github.com/fastai/fastbook/blob/master/03_ethics.ipynb">GitHub</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./09-part-3-break.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Part 3: Advanced Topics in Causality</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./11-reinforcement-learning.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>